#!/usr/bin/perl -CSAL

#  This program, wfind, is Copyright (c) 2008-2014 Volker Schatz.
#
#  wfind is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 3 of the License, or
#  (at your option) any later version.
#
#  wfind is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with wfind; if not, see <http://www.gnu.org/licenses/gpl.html>.


# export PERL_UNICODE="SA" in shell if -C in #! line not allowed


# Table of contents for code sections:
#
# Global data                             sectionglobal
# Auxiliary Functions                     sectionaux
# Initialisation                          sectioninit
# Command line and expression parsing     sectioncmdline
# Script parsing                          sectionscript
# Non-content tests                       sectionnonctest
# Document parsing and content tests      sectiondoctest
# Retrieval and high-level test functions sectionget
# Forking and URL scheduling              sectionsched
# Output                                  sectionout
# Main program                            sectionmain


use 5.012;
use strict;
use warnings;

BEGIN {         # only require, no imports

# general stuff:
use Socket;
require Storable;
require Cwd;
require File::Spec;
require File::Glob;
require File::Copy;
require Encode;
require Unicode::Normalize;
require Time::Local;
require Safe;
# use Data::Dumper;
# use Carp;

# retrieval stuff:
require LWP::UserAgent;
require HTTP::Response;         # returned by User Agent requests

# decode stuff:
require URI;
require URI::file;
require URI::Split;
require URI::Escape;
require HTML::PullParser;
require HTML::Tagset;
# require HTML::Entities;

}


#sectionglobal################################################################
##############################################################################
####                    Global data
##############################################################################
##############################################################################


my $wfversion= "unknown (not properly installed)";
my $wfverdate= "";


# Global settings and capabilities:
my %globals = (
  debug => 0,                 # enable (a little) debug output
  wfinddir => ".wfind",       # per-user config + cache directory
  fork => 1,                  # fork slaves by default
  keepalive => 50,            # number of cached connections per slave
  dispatchwait => 0.51,       # wait interval checking for slave results
  norobots => 0, norobwgt => 0,
  modtolerance => 180,        # tolerance for modified time in seconds
  fillcolumn => 79,           # fill column for terminal messages
  charoverlap => 20,          # overlap between successive text chunks
  intexttestinter => 1000,    # interval for running in-text tests, in chars
  evalinterval => 2500,       # evaluation interval for test expression tree
  defholdoff => 10,           # default crawl delay
  defgdepth => 3, defpdepth => 2, # default global/pipe section depth
  plain8bitfrac => 0.05,      # max. # of chars >127 allowed for plain text
  asplain8bitfrac => 0.05,    # number of chars >127 allowed for -asplain
  defcharset => "iso-8859-1"  # default charset
);

# Elements of %globals (and %locals) to be accumulated from all slaves:
my @cumulatives= qw(nonex nonexwgt nohttps nohttpswgt blacklisted blacklistwgt nowhilewgt);

# Global options and their defaults:
my %globalopts = 
( slaves => 20, verbose => 0, silent => 0,
  useragent => "wfind", timeout => undef,
  cookiejar => undef, redirects => undef,
  holdoff => "-10" );

# Protocols which wfind can handle
my $PROTOCOLS= qr/file|https?|ftps?/i;

# The following two hashes are the defaults for the 
# -follow and -linksto command-line switches.  The format is "tag" for tags
# which have only one attribute containing a link URL and "tag.attr" for
# specifying an attribute where ambiguous.  These hashes are stored in every
# pipe section's options hash.  Their elements will be converted before use by
# the linktaghash() function.

# Follow these links by default
my %defaultfollow= map { $_ => 1; } qw(meta.content a.href area.href
           blockquote.cite del.cite frame.longdesc frame.src iframe.longdesc
           iframe.src link.href);
# Attributes of img tag frequently used to obfuscate images:
my @moreimgattrs;         # initialised in BEGIN block below
# Take these into account for -linksto (all link tags)
my %defaultlinks= ( "meta.content" => 1,  map { $_ => 1; }  (
        map("img.$_", @moreimgattrs),
        map { $a= $_; map $a.".".$_, @{$HTML::Tagset::linkElements{$a}} }
                keys %HTML::Tagset::linkElements ) );
# All link tags in hash/hashset form into which the link validity options are
# also transformed.  This is derived from %HTML::Tagset::linkElements, with
# meta.content added.
my %alllinktags;
BEGIN {
  @moreimgattrs= qw(data-url srcset dynsrc data-defer-src data-original data-delay_url);
  %alllinktags= ( meta => { content => 1 }, 
        map { my $a= $_; $a => +{ map { $_ => 1; } @{$HTML::Tagset::linkElements{$a}} }; }
            keys %HTML::Tagset::linkElements );
  @{$alllinktags{img}}{@moreimgattrs}= (1) x @moreimgattrs;
}

# Maximum number of link targets from a HTTP directory listing not pointing to
# its files and subdirectories (heuristic for identifying dir listings):
my $httpdirmaxother= 1;
# HTTP dir listings end in "/" except for possible fragment and parameters
# (such as sorting)
my $HTTPDIRRE= qr|/(?:[?#].*)?$|;

# Regard these links as part of the current document.  This is not
# user-specifiable.  It is merely a filter for links to be followed which
# affects the search depth bookkeeping and crawl delay behaviour.
my %fwlinktags= map { $_ => 1; }
                  qw(frame.src iframe.src layer.src ilayer.src meta.content);

# Array of "pipe sections", sequentially applied sets of tests
my @pipesecs;
my $secdefault= { utests => [], htests => [], ctests => [],
options => { follow => { %defaultfollow }, inline => 1, httpdirs => 1,
            linktags => { %defaultlinks }, print => { furl => 1 } }  };

# Initial test expression, an "and" operator without operands:
my $emptyand= { type => "and", class => "op", options => {}, subexprs => [] };
# Default test expression, "true":
my $trueexpr= { type => "true", class => "utests" };

# Array of jobs for URLs given on the command line or standard input.
my @jobs;

# Minimum section for new jobs.  >0 only when a maximum result number has been
# exceeded for some section.
my $minsection= 0;

# Array of completed URL jobs
my @attic;

# Partial regex of word characters, to be enclosed in character class
my $WORDCHARS= '\\p{L}\\p{N}';


#sectionaux###################################################################
##############################################################################
####                    Auxiliary functions
##############################################################################
##############################################################################

# Minimum of two numbers
# -> Two numerical values to be compared
# <- The smaller number
sub min2
{
  my ($x1, $x2)= @_;

  return $x1 < $x2? $x1 : $x2;
}


# Maximum of two numbers
# -> Two numerical values to be compared
# <- The larger number
sub max2
{
  my ($x1, $x2)= @_;

  return $x1 > $x2? $x1 : $x2;
}


# Sum up a list.
sub sum
{
  my $s= 0;

  for (@_) { $s += $_; }
  return $s;
}


# Return all elements of an array or a hash set.
# -> Reference to array or hash set
# <- All array elements or hash keys
sub set
{
  my ($ref)= @_;

  if( ref($ref) eq "ARRAY" ) {
    return @$ref;
  }
  elsif( ref($ref) eq "HASH" ) {
    return keys %$ref;
  }
  return ();
}


# Duplicate a nested data structure of hash and array references.
# -> List of scalars, possibly array or hash references
# <- List of deep copies of arguments.  References that are not hash or array
#    refs are copied as-is.
sub dup
{
  my @result;

  for (@_) {
    my $reftype= ref $_;
    if( $reftype eq "ARRAY" ) {
      push @result, [ dup(@$_) ];
    }
    elsif( $reftype eq "HASH" ) {
      my %h;
      @h{keys %$_}= dup(values %$_);
      push @result, \%h;
    }
    else {
      push @result, $_;
    }
  }
  return @_ == 1 ? $result[0] : @result;
}


# Apply a two-place operation pairwaise to two arrays.  Return the array of
# results.  The length of the result is the length of the shorter of the
# operand arrays.
# -> Subroutine reference to function accepting two scalar arguments and
#    returning a scalar
#    Reference to first operand array
#    Reference to second operand array
# <- Reference to result array
sub zipwith
{
  my ($op, $val1, $val2)= @_;
  my @result;

  my $maxind= $#$val1 > $#$val2 ? $#$val2 : $#$val1;
  for my $ind (0..$maxind) {
    push @result, &$op($$val1[$ind], $$val2[$ind]);
  }
  return \@result;
}


# Short-circuited grep in scalar context, which returns 1 if an element of the
# passed list is evaluated to true by the anonymous subroutine, without
# continuing after one has been found.
# -> Anonymous test subroutine operating on $_
#    List to test
# <- 1 if an element matched, 0 otherwise
sub shortgrep(&@)
{
  my $test= shift @_;
  for (@_) {
    return 1 if &$test;
  }
  return 0;
}


# Split an array into sub-arrays.
# -> Code reference which accepts an array element as its first argument and
#    returns true if it is a separator.
#    Array to split
# <- List of sub-array references
sub arysplit(&@)
{
  my ($is_sep, @ary)= @_;
  my @result= ( [] );

  for my $elem (@ary) {
    if( &$is_sep($elem) ) {
      push @result, [];
      next;
    }
    push @{$result[-1]}, $elem;
  }
  return @result;
}


# Test if a hash set has a true value for one of several keys.
# -> Reference to hash (undef is tolerated and taken as an empty set)
#    List of keys
# <- 1 if one of the keys is in the hash set, 0 otherwise
sub hsetany
{
    my $hset= shift;

    return 0 unless $hset;
    for (@_) {
        return 1 if $$hset{$_};
    }
    return 0;
}


# Escape a string to be passed in a URL query parameter.
# -> String to escape
# <- Escaped string
sub query_escape
{
  my $str= URI::Escape::uri_escape(shift);
  $str =~ s/%20/+/g;
  return $str;
}


# TODO: can one keep relative file: URLs relative? or at least print relative paths?

# Canonicalise an URI for our purposes
# -> URI
#    Base URI if first argument is relative
# <- canonical URI
sub canonuri
{
  my ($uri, $base)= @_;

  # substitute hashbang to get non-javascript page
  $uri =~ s/(\?.*?)\#(?:!|%21)/${1}\&_escaped_fragment_=/;
  $uri =~ s/\#(?:!|%21)/\?_escaped_fragment_=/;
  $uri =~ s/\#.*$//;    # otherwise, discard fragment
  my $obj= $base? URI->new_abs($uri, $base) : URI->new($uri);
  return $obj->canonical()->as_string();
}


# Regex to extract the base domain name.  Slightly complicated because we want
# to treat sub-country top-level domains such as .ac.uk and .com.tw as TLDs.
my $SERVDOMAIN= qr/(?:[^:\/?#.]\.)*([^:\/?#.]+\.(?:\w{2,3}\.\w{2}|\w+))(?::\d+)?$/;

# Extract the domain name from a URL, discarding subdomain prefixes and port
# numbers.
# -> URL
# <- domain (or empty string if it could not be extracted)
sub getdomain
{
  my ($url)= @_;
  my $server= ${[URI::Split::uri_split($url)]}[1];
  return "" unless defined($server);
  my ($domain)= $server =~ $SERVDOMAIN;
  return defined($domain)? $domain : "";
}


# Form a case insensitive regular expression matching URLs on one of a list of
# domains.
# -> List of domains
# <- Regular expression
sub domainre
{
  return "(?:^\\w+://(?:[^:/\\?#]+\.)?(?i:" . join("|", map(quotemeta($_), @_)) . ")(?:/|:|\$))";
}


# Form a case insensitive regular expression matching strings starting with
# certain prefixes.
# -> List of prefixes
# <- Regular expression
sub prefixre
{
  return "(?:^(?i:" . join("|", map(quotemeta($_), @_)) . "))";
}


# Convert a file:// URI to a file name.
# -> URI
# <- File name, or undef if not a file URI
sub uri2file
{
  my ($uri)= @_;

  my $obj= URI->new($uri);
  return $obj->scheme() eq "file"? $obj->file() : undef;
}


# Recursively follow a symbolic link.  Detects cycles.
# -> File name of symbolic link
# <- Absolute file name of ultimate link target, or undef if link is dead or
#    points to a cycle or if an error occurred.
sub tracesymlink
{
  my ($fname)= @_;
  my @trace;

  while( -l $fname ) {
    push @trace, $fname;
    my ($vol, $path,)= File::Spec->splitpath($fname);
    my $dirpart= File::Spec->catpath($vol, $path, "");
    return undef unless defined($fname= readlink($fname));
    $fname= File::Spec->rel2abs($fname, $dirpart);
    return undef if shortgrep { $_ eq $fname } @trace;
  }
  return -e $fname? $fname : undef;
}


# Print at most the first three elements of an array.  If there are any more,
# they are replaced by "...".
# -> String to join the strings in the array with
#    List of array elements
sub print3
{
  my ($joinstr, @ary)= @_;

  if( @ary > 3 ) {
    print join($joinstr, @ary[0..2]), $joinstr, "...";
  }
  else {
    print join($joinstr, @ary);
  }
}


# Concatenate a word list, separated by a comma and a space, but with the last
# two words separated by a given string instead.
# -> String between last two words, including surrounding spaces if applicable
#    List of words
sub cswordlist
{
  my ($conj, @words)= @_;

  if( @words > 1 ) {
    return join(", ", @words[0..$#words-1]) . $conj . $words[-1];
  }
  elsif( @words ) {
    return $words[0];
  }
}


# Guess if a chunk of data is probably text, based on the length of "words" and
# gaps between them.  Used by setparsetype() when a file contains some but not
# too many 8-bit characters.
# -> Data fragment
# <- 1 if it looks like text, 0 otherwise
sub looksliketext
{
  my ($data)= @_;
  my ($oldpos, $nwords, $wordsize, $gapsize)= (0,0,0,0);

  pos($data)= 0;
  while( 13 )
  {
    last unless $data =~ /\b[a-z0-9\x80-\xFF]/ig;
    my $wdpos= pos($data);
    last unless $data =~ /\b[^a-z0-9\x80-\xFF]/ig;
    ++$nwords;
    $wordsize += pos($data) - $wdpos;
    $gapsize += $wdpos - $oldpos;
    $oldpos= pos($data);
  }
  $wordsize /= $nwords;
  $gapsize /= $nwords;
  return $gapsize < 8 && $wordsize > 2 && $wordsize < 15;
}


# Print strings, dividing words into lines of length $globals{fillcolumn}.
# -> (optional) reference to IO handle to print to
#    String(s)
sub fmt
{
  my ($handle, @strings)= @_;

  unless( ref($handle) ) {
    unshift @strings, $handle;
    $handle= *STDOUT{IO};
  }
  my $output= join "", @strings;
  my @lines= split /\n/, $output, -1;
line:
  while( @lines ) {
    my $line= shift @lines;
    my @words= split /(?<=\S)\s/, $line;
    while( @words ) {
      my $col= length($words[0]);
      do {
        print $handle shift(@words), " ";
        next line unless @words;
        $col += length($words[0])+1;
      } while( $col< $globals{fillcolumn} );
      print $handle "\n";
      $words[0] =~ s/^\s+//;
    }
  }
  continue { print $handle "\n" if @lines; }
}


# Print an error message when opening a file through an input pipe has failed.
# -> Name of the input pipe program
#    File error message, if any ("$!")
#    Return value of the pipe command ($?)
#    Original file name, or empty string for URL jobs
sub printpipeerr
{
  my ($prog, $fileerr, $retcode, $fname)= @_;

  print STDERR "wfind: Error: $fileerr\n"
              if $fileerr && $globalopts{verbose};
  print STDERR "wfind: $prog returned exit status $retcode\n"
              if $retcode && $globalopts{verbose};
  if( $fname ) {
    # Mention the file name if a local file is being parsed.  This is not
    # strictly, but conceptually correct if the file has been preprocessed
    # (decompression or PS->PDF).
    print STDERR "wfind: Error: Could not open `$fname' with `$prog'.\n"
      unless $globalopts{silent};
  }
  else {
    print STDERR "wfind: Error: Could not open downloaded file with `$prog'.\n"
      unless $globalopts{silent};
  }
}


# Print an error message when opening a cache file or target file failed.
# -> File name
#    Reference to URL job hash
sub printopenerr
{
  my ($fname, $job)= @_;

  return if $globalopts{silent};
  if( $job->{type} eq "file" && $job->{path} eq $job->{cachefile} ) {
    print STDERR "wfind: Error opening $fname: $!\n"
            if $! && $globalopts{verbose};
    if( $fname eq $job->{path} ) {
      print STDERR "wfind: Error: Could not open `$fname'.\n"
    }
    else {
      print STDERR "wfind: Error: Could not open cached file for `$job->{path}'.\n"
    }
  }
  else {
    print STDERR "wfind: Error opening cache file: $!\n"
            if $! && $globalopts{verbose};
    print STDERR "wfind: Error: Could not open cache file for `$job->{url}'.\n";
  }
  return;
}


# Another auxiliary function which exits the program with return value 1 after
# optionally printing a message to STDERR.  Akin to die but without the silly
# line number output which users don't have any use for.  To be used for errors
# which are the user's fault, rather than the programmer's.
# -> Error message
sub croak
{
  fmt(*STDERR{IO}, @_, "\n") if @_;
  exit 1;
}


# Convenience wrapper around Encode::decode().  Catches Encode::decode() dying.
# First tries to decode using the given encoding, then the default encoding,
# then removes all non-7-bit characters and forces the utf8 flag.
# -> Character set of the source text
#    Source text
# <- utf8 flagged equivalent Perl string
sub dodecode
{
  my ($charset, $text)= @_;
  my $result= eval { Encode::decode($charset, $text, Encode::FB_CROAK); };
  if( $@ ) {
    # try default charset as a fallback
    $result= eval { Encode::decode($globals{defcharset}, $text, Encode::FB_CROAK); };
  }
  if( $@ ) {
    # last resort: grep out 7-bit characters and force utf8 flag
    $result= $text;
    $result =~ s/[\200-\377]+//g;
    Encode::_utf8_on($result);
  }
  return $result;
}


# Open a file so that its charset is decoded.  Tries to open using the given
# encoding first, then the default encoding, then without giving an encoding.
# -> File name
#    Encoding
# <- Reference to file handle or undef on failure
sub opendecoding
{
  my ($fname, $charset)= @_;
  my $fh;
  my $encobj;

  ($charset && (($encobj= Encode::find_encoding($charset)) && $encobj->perlio_ok()) && open($fh, "<:encoding($charset)", $fname)) ||
  open($fh, "<:encoding($globals{defcharset})", $fname) ||
  open($fh, "<", $fname) || return undef;
  return $fh;
}


# Reference to function which maps unicode characters to their base character,
# which I define as the latin character with all diacritics, strokes, hooks and
# similar removed.  Furthermore, the scandinavian OE etc. are mapped to their
# multi-letter ASCII equivalent and some punctuation characters also mapped to
# ASCII.  The actual subroutine is generated by makeuni2base() below.
my $uni2base;

{
# Map for normalising some international characters and punctuation to several
# ASCII characters
my %ligmap = ( "\x{00C6}" => "AE", "\x{00DF}" => "ss", "\x{00E6}" => "ae",
# remove zero-width joiner and discretionary hyphen
"\x{200D}" => "", "\x{00AD}" => "",
"\x{0152}" => "OE", "\x{0153}" => "oe", "\x{0195}" => "hv", "\x{01F6}" => "Hv",
"\x{0238}" => "db", "\x{0239}" => "qp",
"\x{2025}" => "..", "\x{2026}" => "...", "\x{2033}" => "''",
"\x{2034}" => "'''", "\x{2036}" => "``", "\x{2037}" => "```",
"\x{2057}" => "''''", "\x{301E}" => "''", "\x{301D}" => "``",
"\x{2016}" => "||", "\x{203C}" => "!!",
"\x{2047}" => "??", "\x{2048}" => "?!", "\x{2049}" => "!?" );

# Collection of transliterations from unicode characters to their base
# characters.  Listed (vaguely) by code chart to help clarity and
# maintainability.
my %unimap = (
  "\x{00A1}\x{00BF}\x{00D0}\x{00D8}\x{00F8}" => "!?DOo",
  "\x{0110}\x{0111}\x{0126}\x{0127}\x{0131}\x{0138}" => "DdHhiq",
  "\x{0141}\x{0142}\x{014A}\x{014B}\x{0166}\x{0167}" => "LlNnTt",
  "\x{0180}\x{0243}\x{0181}\x{0253}\x{0182}\x{0183}" => "bBBbBb",
  "\x{0187}\x{0188}\x{0189}\x{0256}\x{018A}\x{0257}\x{018B}\x{018C}" =>
                "CcDdDdDd",
  "\x{0191}\x{0192}\x{0193}\x{0260}\x{0197}\x{0268}" => "FfGgIi",
  "\x{0197}\x{0199}\x{019A}\x{023D}\x{019D}\x{019E}" => "KklLNn",
  "\x{019F}\x{0275}\x{01A4}\x{01A5}\x{01AB}\x{01AC}\x{01AD}\x{01AE}\x{0288}"
                => "OoPptTtTt",
  "\x{01B3}\x{01B4}\x{01B5}\x{01B6}\x{01C3}\x{01E4}\x{01E5}\x{0220}" =>
                "YyZz!GgN",
  "\x{0224}\x{0225}\x{0237}\x{023A}\x{023B}\x{023C}\x{023E}" => "ZzjACcT",
  "\x{023F}\x{0240}\x{0244}\x{0246}\x{0247}\x{0248}\x{0249}" => "szUEeJj",
  "\x{024C}\x{024D}\x{024E}\x{024F}" => "RrYy",
  "\x{2018}\x{2019}\x{201A}\x{201B}\x{2032}\x{2035}" => "'''''`",
  "\x{201C}\x{201D}\x{201E}\x{201F}" => '""""',
  "\x{2044}\x{204E}\x{2024}\x{2052}\x{2053}" => "/*.%~",
  "\x{2010}\x{2011}\x{2012}\x{2013}\x{2014}\x{2015}" => "\\-\\-\\-\\-\\-\\-"
);

# This function generates the uni2base subroutine from the transliterations
# given in %ligmap and %unimap (and some other code).  This works around having
# a single transliteration too large to manage and still achieves the same
# runtime efficiency.
sub makeuni2base
{
  my $trfrom= join "", keys(%unimap);
  my $trto= join "", values(%unimap);
  my $ligfrom= join "", keys(%ligmap);

  my $unesc_trto= $trto;
  $unesc_trto =~ s|\\(.)|$1|g;
  die "Error generating uni2base subroutine: transliteration length mismatch"
        if length($trfrom) != length($unesc_trto);

  my $evalcode= <<EOF;
sub {
  my (\$word)= \@_;
  \$word= Unicode::Normalize::NFKD(\$word);
  \$word =~ s/[\\p{InSpacingModifierLetters}\\p{M}]+//g;
  \$word =~ tr|$trfrom|$trto|;
  \$word =~ s/([$ligfrom])/\$ligmap{\$1}/eg;
  \$word =~ s/([\x{1D400}-\x{1D6A3}])/chr(0x41 + (ord(\$1) - 0x1D400)%26)/eg;
#  \$word =~ s/([\x{1D6A8}-\x{1D7C9}])/(ord(\$1) - 0x1D7CE)%58 .../eg;
# greek - much more difficult due to different alignment from canonical
  \$word =~ s/([\x{1D7CE}-\x{1D7FF}])/chr(0x30 + (ord(\$1) - 0x1D7CE)%26)/eg;
  return \$word;
}
EOF
  $uni2base= eval($evalcode);
  die "Error generating uni2base subroutine: $@" if $@;
}

}


# Split text into words and convert words to their unicode base character
# equivalent.  Words are considered to be uninterrupted sequences of letters
# and digits.  Words and their start positions in terms of unconverted
# characters are appended to arrays.  This is done in a way that words split
# between successive calls of this function will be put together again.  The
# calling context should be aware that the last word in the array may not be
# complete unless the text is exhausted, and may even be empty.  The word
# position array will receive an additional element, the position of the first
# character in the following call of this function.
# -> Reference to word array
#    Reference to word position array (or undef if not wanted by caller)
#    Text
sub splitwords
{
  my ($wordlist, $poslist, $text)= @_;
  my $pos0= $poslist && @$poslist? pop(@$poslist) : 0;
  my $lastincomplete= $poslist && @$poslist &&
                        $pos0 == $$poslist[-1]+length($$wordlist[-1]);

  pos($text)= 0;
  while( pos($text) < length($text) ) {
    $text =~ /\G[^$WORDCHARS]*/go;
    my $offset= pos($text);
    last unless $text =~ /\G([$WORDCHARS]+)/go;
    # print STDERR "  word at offset $offset, up to ", pos($text), "\n";
    if( !$offset && $lastincomplete ) {
      $$wordlist[-1] .= &$uni2base($1);
    }
    else {
      push @$wordlist, &$uni2base($1);
      push @$poslist, $pos0 + $offset if $poslist;
    }
  }
  push @$poslist, $pos0+length($text) if $poslist;
}



#sectioninit##################################################################
##############################################################################
####                    Initialisation
##############################################################################
##############################################################################

# Generate a subroutine calling the system() command in a way which allows
# spaces in the arguments of the executed program.
# -> Command line to put in the system command
# <- Reference to subroutine which returns true if system() succeeds
sub makesyssub
{
  my ($prog, $commandline)= @_;
  my @cmdline= split /\s+/, $commandline;
  # Put variable arguments in quotes to allow spaces in file names:
  map s/^(\$.*)$/'$1'/, @cmdline;
  $commandline= join " ", @cmdline;
  $commandline= $globals{$prog} . " " . $commandline;

  my $code= <<EOF;
sub {
  return !system("$commandline");
}
EOF
#  print $code;
  return eval($code);
}


# Generate a subroutine using a IO::Uncompress::... module to decode a file and
# writing the result to another file.
# -> (ignored)
#    Name of the module (has to start with "IO::Uncompress::")
# <- Subroutine reference; undef if the module is not one of IO::Uncompress::..
sub makeiodecsub
{
  my (undef, $mod)= @_;

  return undef unless $mod =~ /^IO::Uncompress::(\w+)$/;
  my $func= lc $1;
  return eval <<EOF;
sub
{
  return ${mod}::$func(\$_[0] => \$_[1], BinModeOut => 1, Transparent => 0);
}
EOF
}


# Generate a subroutine using a MIME::... module to decode a file and
# writing the result to another file.
# -> (ignored)
#    Name of the module (has to start with "MIME::")
# <- Subroutine reference; undef if the module is not one of MIME::..
sub makemimedecsub
{
  my (undef, $mod)= @_;

  return undef unless $mod =~ /^MIME::/;
  my $code= <<EOF;
sub {
  my (\$from, \$to)= @_;
  open IN, "<\$from" or return 0;
  open OUT, ">\$to" or return 0;
  while( <IN> ) {
    my \$dec= ${mod}::decode(\$_) or return 0;
    print OUT \$dec;
  }
  return 0 if \$!;
  return 1;
}
EOF
  return eval $code;
}


# Manufactures subroutines which call readplain() with an input pipe as the
# file name.
# -> Command name of the external program
#    Command line, with $_ representing the input file name
# <- Subroutine which opens an input pipe with this command line and calls
#    readplain() 
sub makefilteredread
{
  my ($prog, $commandline)= @_;
  my $binmode= "";
  if( $commandline =~ /(8859-\d+)/ ) {
    $binmode= "\n  binmode PIPE, \":encoding(iso-$1)\";";
  }
  elsif( $commandline =~ /utf-8/i ) {
    $binmode= "\n  binmode PIPE, \":encoding(UTF-8)\";";
  }
  my @cmdline= split /\s+/, $commandline;
  # Put variable arguments in quotes to allow spaces in file names:
  map s/^(\$.*)$/'$1'/, @cmdline;
  $commandline= join " ", @cmdline;
  $commandline= $globals{$prog} . " " . $commandline;

  my $code= <<EOF;
sub {
  \$_= shift \@_;
  my (\$job, \$pd)= \@_;
  if( !open(PIPE, "-|", "$commandline") ) {
    printpipeerr("$prog", \$!, \$?, \$job->{path} || "");
    setnocontent(\$job);
    return;
  }$binmode
  readplain(*PIPE{IO}, \@_);
}
EOF
#  print $code;
  return eval($code);
}


{

# The following arrays contain an entry for each capability related to
# decompression, file format decoding, or specific tests.  Each element is a
# reference to an array describing the capability.  Its first entries are the
# %globals key containing a decoding function (generated by getcaps()), a noun
# or substantival expression saying what it does, and a flag indicating whether
# the description is (linguistical) plural.  The latter two entries serve to
# automatically generate messages reporting whether a feature is available,
# which is done by printonecap().  The trailing elements of each capability
# array represent the alternative ways of achieving capabilities (Perl modules
# or external programs) and are themselves array references.  Their first two
# entries are a %globals key and the full name for Perl modules, or the name of
# the program (which doubles as a %globals key) and its command line arguments.
# The third element is a reference to a subroutine which manufactures the
# decoding / parsing function from the first two.  This allows the decoding
# functions to be automatically generated, which is done by getcaps().  They
# are ordered according to preference.

my @filecaps= (
[ "dec_gzip", "decompression of gzipped files", 0,
        [ "ioany", "IO::Uncompress::AnyUncompress", \&makeiodecsub ],
        [ "ioanyi", "IO::Uncompress::AnyInflate", \&makeiodecsub ],
        [ "iogzip", "IO::Uncompress::Gunzip", \&makeiodecsub ],
        [ "gzip", "-c -d \$_[0] > \$_[1] 2>/dev/null", \&makesyssub ] ],
[ "dec_deflate", "decompression of zip files", 0,
        [ "ioany", "IO::Uncompress::AnyUncompress", \&makeiodecsub ],
        [ "ioanyi", "IO::Uncompress::AnyInflate", \&makeiodecsub ],
        [ "iodeflate", "IO::Uncompress::Inflate", \&makeiodecsub ],
        [ "gzip", "-c -d \$_[0] > \$_[1] 2>/dev/null", \&makesyssub ] ],
[ "dec_bzip2", "decompression of bzip2 compressed files", 0,
        [ "ioany", "IO::Uncompress::AnyUncompress", \&makeiodecsub ],
        [ "ioanyi", "IO::Uncompress::AnyInflate", \&makeiodecsub ],
        [ "iobzip2", "IO::Uncompress::Bunzip2", \&makeiodecsub ],
        [ "bzip2", "-c -d \$_[0] > \$_[1] 2>/dev/null", \&makesyssub ] ],
[ "dec_compress", "decompression of UNIX compress'ed files", 0,
        [ "gzip", "-c -d \$_[0] > \$_[1] 2>/dev/null", \&makesyssub ],
        [ "uncompress", "-c \$_[0] > \$_[1] 2>/dev/null", \&makesyssub ] ],
[ "dec_base64", "decompression of Base64 encoded files", 0,
        [ "mimebase64", "MIME::Base64", \&makemimedecsub ],
        [ "base64", "-d \$_[0] > \$_[1] 2>/dev/null", \&makesyssub ] ],
#       [ "uudecode", "-o \$_[1] \$_[0] 2>/dev/null", \&makesyssub ] ],
# uudecode needs begin-base64 line, the others don't
[ "dec_uuencoded", "decompression of uuencoded files", 0,
        [ "uudecode", "-o \$_[1] \$_[0] 2>/dev/null", \&makesyssub ] ],
[ "dec_quoted-printable", "decompression of quoted-printable files", 0,
        [ "mimeqp", "MIME::QuotedPrint", \&makemimedecsub ] ],
[ "proc_pdf", "processing of PDF files", 0,
#       [ "pdf", "PDF", sub { \&readpdf; } ],
        [ "pdftotext", "-enc UTF-8 \$_ - 2>/dev/null", \&makefilteredread ] ],
# TODO: consider -htmlmeta (requires custom read function)
[ "proc_doc", "processing of MS Word files", 0,
#       [ "ole", "OLE::Storage_Lite", sub { \&readole; } ],
        [ "catdoc", "-d8859-1 \$_ 2>/dev/null", \&makefilteredread ] ],
# catdoc ships only with 8-bit charsets; extending to UTF-8 seems possible in
# principle but I can't find the corresponding translation table.  Also, it
# would be huge and nobody would have it installed.
[ "proc_xls", "processing of MS Excel files", 0,
#       [ "ole", "OLE::Storage_Lite", sub { \&readole; } ],
        [ "xls2csv", "-d8859-1 \$_ 2>/dev/null", \&makefilteredread ] ],
[ "proc_ppt", "processing of MS PPT files", 0,
#       [ "ole", "OLE::Storage_Lite", sub { \&readole; } ],
        [ "catppt", "-d8859-1 \$_ 2>/dev/null", \&makefilteredread ] ],
[ "proc_troff", "processing of UNIX manual page files", 0,
        [ "man", "-E UTF-8 --nh \$_ 2>/dev/null", \&makefilteredread ] ],
[ "proc_rtf", "processing of rich text files", 0,
        [ "rtf", "RTF::Tokenizer", sub { \&readrtf; } ],
        [ "unrtf", "", sub { \&readunrtf; } ] ],
);

# File capabilities which need special treatment.  PS decoding is performed as
# preprocessing to an auxiliary file, but is not an encoding to be advertised
# in the HTTP header; besides, it is dependent on the PDF decoding capability.
my @extrafilecaps= (
[ "dec_ps", "", 0, [ "ps2pdf", "\$_[0] \$_[1] 2>/dev/null", \&makesyssub ] ],
);

my @testcaps= (
[ "modified", "-modified tests", 1,
        [ "dateparse", "Date::Parse" ] ],
[ "rtype", "-rtype tests", 1,
        [ "mimeinfomagic", "File::MimeInfo::Magic" ],
        [ "file", "" ] ],
[ "~", "Levenshtein distance tests", 1,
        [ "levenshtein", "Text::LevenshteinXS" ] ],
[ "\$", "phonetic equivalence tests", 1,
        [ "doublemetaphone", "Text::DoubleMetaphone" ],
        [ "doublemetaphone_pp", "Text::DoubleMetaphone_PP" ],
        [ "metaphone", "Text::Metaphone" ],
        [ "soundex", "Text::Soundex" ] ],
[ "exif", "EXIF tests", 1,
        [ "exiftool", "Image::ExifTool" ] ],
);

my @protocaps= (
[ "proto_https", "HTTPS requests", 1,
        [ "lwphttps", "LWP::Protocol::https" ] ],
[ "proto_connect", "HTTPS requests via HTTP proxy", 1,
        [ "lwpconnect", "LWP::Protocol::connect" ] ],
);

# Regex to recognise Perl modules in the second element of capability
# alternatives:
my $PERLMODRE= qr/^[A-Z]\w+(?:::[A-Za-z]\w+)+$/;


# Change preference for a capability to a given perl module or external program.
# -> List of names od external programs or Perl modules.  If two arguments
#    affect the same capability, the order of preference is kept.
# <- List of arguments which could not be found in the capability alternatives
#    lists.
sub prefercap
{
  my @rejects;

  for my $pref (reverse @_) {
    my $ismod= $pref =~ /$PERLMODRE/ ? 1 : 0;
    my $found;
searchcappref:
    for my $cap (@filecaps, @extrafilecaps, @testcaps, @protocaps) {
      next unless @$cap > 4;    # preference pointless if only one option
      my $ind= 3;
      while( 13 ) {
        next searchcappref if $ind == @$cap;
        last if $cap->[$ind][$ismod] eq $pref;
        ++$ind;
      }
      next if ($cap->[$ind][1] =~ /$PERLMODRE/ ? 1 : 0) != $ismod;
      $found= 1;
      next if $ind == 0;    # nothing to do
      splice @$cap, 3, 0, splice(@$cap, $ind, 1);
    }
    push @rejects, $pref unless $found;
  }
  return reverse @rejects;
}


{
my $preferredphonetic;

# Determine phonetic encoding of words using the preferred available
# capability.
# -> List of words
# <- List of phonetic codes, possibly containing undef if a word had on
#    phonetic encoding
sub phoneticenc
{
  # Find first available phonetic encoding in capabilities array, the order of
  # which may have been changed by prefercap():
  unless( defined $preferredphonetic ) {
    my ($phoneticcap)= grep $$_[0] eq "\$", @testcaps;
    for (@$phoneticcap[3..$#$phoneticcap]) {
      next unless $globals{$$_[0]};
      $preferredphonetic= $$_[0];
      last;
    }
    die "No phonetic tests available - this should have been caught earlier"
        unless defined $preferredphonetic;
  }
  if( $preferredphonetic eq "doublemetaphone" ) {
    my @result;
    for (@_) {
      push @result, [ Text::DoubleMetaphone::double_metaphone($_) ];
      $result[-1]= undef unless @{$result[-1]};
    }
    return @result;
  }
  elsif( $preferredphonetic eq "doublemetaphone_pp" ) {
    my @result;
    for (@_) {
      push @result, [ Text::DoubleMetaphone_PP::double_metaphone_pp($_) ];
      $result[-1]= undef unless @{$result[-1]};
    }
    return @result;
  }
  elsif( $preferredphonetic eq "metaphone" ) {
    return map Text::Metaphone::Metaphone($_), @_;
  }
  elsif( $preferredphonetic eq "soundex" ) {
    return map Text::Soundex::soundex($_), @_;
  }
}


# Compare arrays of phonetically encoded words for equality.  Undefined
# elements in the first array serve as wildcards, in the second array they
# compare as false.  The caller has to verify that the first array is at most
# as long as the second.
# -> Reference to pattern array
#    Reference to word array to test
# <- 1 if the elements of the first array are equal to the initial elements of
#    the second (string comparison).
sub phoneticeq
{
  my ($pattern, $ary)= @_;

  if( $preferredphonetic =~ /^doublemetaphone/ ) {
doublemetaloop:
    for my $ind (0 .. $#$pattern) {
      next unless defined $$pattern[$ind];
      return 0 unless defined $$ary[$ind];
      for my $patword (@{$$pattern[$ind]}) {
        for my $aryword (@{$$ary[$ind]}) {
          next doublemetaloop if $patword eq $aryword;
        }
      }
      return 0;
    }
  }
  else {
    for my $ind (0 .. $#$pattern) {
      return 0 unless !defined($$pattern[$ind]) ||
                      (defined($$ary[$ind]) && $$pattern[$ind] eq $$ary[$ind]);
    }
  }
  return 1;
}

}


# Print which module or external program is used for a given feature, or which
# can be installed to enable it.  This function accepts a different data format
# from printgencapmsg and generates the message automatically.
# -> Reference to array containing the name of the capability, a flag
#    indicating whether it is plural, and a list of array references in
#    declining order of preference.  The latter references contain the key in
#    the %globals hash relating to a module or command providing the
#    capability, and a string describing the module or the empty string for
#    commands (where the hash key is the name of the command).
sub printonecap
{
  my ($cap)= @_;

  my ($found, $havesub);
  for my $alternative (@$cap[3..$#$cap]) {
    if( $globals{$alternative->[0]} ) {
      $found= $alternative->[1] =~ /$PERLMODRE/ ?
                $alternative->[1] : $globals{$alternative->[0]};
      $havesub= !!$alternative->[2];
      last;
    }
  }
  if( $found ) {
    if( $havesub && !$globals{$cap->[0]} ) {
      fmt ucfirst($cap->[1]), " seems to be broken even though $found is ",
                        "available.  Please let the maintainer know.\n\n";
    }
    elsif( @$cap > 3 || $found =~ /^\// ) {
      fmt "Using $found for ", $cap->[1], ".\n\n";
    }
    else {
      fmt ucfirst($cap->[1]), ($cap->[2] ? " are" : " is"),
                " available ($found is installed).\n\n";
    }
  }
  else {
    my @names= map $_->[1] =~ /::/? $_->[1] : "the $_->[0] program",
                @$cap[3..$#$cap];
    fmt "Cannot perform ", $cap->[1], ".  Install ",
        cswordlist(" or ", @names), " to enable ",
        ($cap->[2]? "them" : "it"), ".\n\n";
  }
}

# Find out our capabilities (availability of Perl modules and external
# programs) and write them to %globals.
sub getcaps
{
  eval {
    my $mask= "";
    socketpair(SLAVE, MASTER, AF_UNIX, SOCK_DGRAM, PF_UNSPEC);
    vec($mask, fileno(SLAVE), 1)= 1;
    select($mask, undef, undef, 0);
    close(SLAVE);
    close(MASTER);
  };
  $globals{fork}= $@ ? 0 : 1;
  print STDERR <<EOF if $@ && !$globalopts{silent};
wfind: Warning: Your version of Perl or your system lacks the low-level
functions socketpair() or select() which are required for concurrent operation.
wfind will still run, but will not be able to download several documents
simultaneously.
EOF
  $@= "";
  $globals{proc_html}= \&readhtml;
  $globals{proc_plain}= \&readplain;
  # Check for Perl modules and external programs for additional capabilities:
  my %subcache;
  for my $cap (@filecaps, @extrafilecaps, @testcaps, @protocaps) {
    for my $alternative (@$cap[3..$#$cap]) {
      if( $globals{$$alternative[0]} ) {        # have found this already
        $globals{$$cap[0]}= $subcache{$$alternative[0]};
      }
      elsif( $$alternative[1] =~ /$PERLMODRE/ ) {     # look for a Perl module
        $@= "";
        eval "require $$alternative[1];";
        $globals{$$alternative[0]}= !$@;
      }
      else {                                    # look for an external program
        $globals{$$alternative[0]}= `which $$alternative[0] 2> /dev/null`;
        chomp $globals{$$alternative[0]};
      }
      if( $globals{$$alternative[0]} ) {
        if( $$alternative[2] ) {
          $globals{$$cap[0]}= $subcache{$$alternative[0]}=
              &{$$alternative[2]}(@$alternative[0..1]);
        }
        last;
      }
    }
  }

  # Build list of supported content codings (compressions):
  $globals{"accept-encoding"}= join ", ", 
                grep $globals{$_} && s/^dec_//, map $_->[0], @filecaps;
  $globals{"accept-encoding"} .= ", x-bzip2" if $globals{dec_bzip2};
  $globals{"accept-encoding"} .= ", identity, *;q=0";

  # Generate list of supported character sets from Unicode Encode module:
  # This is longer than any I have seen a browser send, but I see no easy way
  # of narrowing it down in an unbiased way.
  $globals{"accept-charset"}= join(", ", 
                grep( +( s/utf8/UTF-8/, /^(ascii|cp|iso|ucs|utf-[^-]+$)/i ),
                        Encode->encodings(":all")));
}


# Check whether a given non-content test is available.
# -> Name of the test option (without "-")
#    Error handling function taking message as argument
sub testavailable
{
  my ($testopt, $error)= @_;

  my ($cap)= grep $_->[0] eq $testopt, @testcaps;
  # return OK if test is always available
  return 1 unless $cap;
  unless( grep $globals{$_->[0]}, @$cap[3..$#$cap] ) {
    my @names= map $_->[1] =~ /::/? $_->[1] : "the $_->[0] program",
                @$cap[3..$#$cap];
    &$error("Cannot perform $cap->[1]  (install " .  cswordlist(" or ", @names)
            . " to enable " .  ($cap->[2]? "them)" : "it)"));
  }
}


# The following array contains the messages which report the (un)availability
# of a general feature, currently only multithreading.
my @generalcapmsgs= (
[ "fork", "Concurrency is enabled.\n\n", <<EOF ],
Your version of Perl or your system lacks the low-level functions socketpair()
or select() which are required for concurrent operation.  This is bad because
wfind cannot download several documents at once, and your search may be stalled
forever by one slow server.  Local file system searches will be fine, though.

EOF
);

# Print message depending on whether some general feature is available.
# -> Reference to array containing the feature name (= %globals key), the
#    messages which apply if the feature is available or unavailable.  The
#    message strings may contain "$<key>", which will be replaced by
#    $globals{<key>}.
sub printgencapmsg
{
  my ($cap)= @_;

  my $output= $globals{$cap->[0]} ? $cap->[1] : $cap->[2];
  $output =~ s/\$(\w+)/$globals{$1} || "\$$1"/ge;
  fmt $output;
}


# Print out wfind's capabilities.  What is printed here is largely determined
# by the message arrays above.  Only some headlines are added.
sub printcapsverbose
{
  my $versstr= $wfverdate ? " version  $wfversion  from  $wfverdate" : "";
  print <<EOF;
Welcome to wfind$versstr.

The following features are available:

GENERAL FEATURES

EOF
  map { printgencapmsg $_; } @generalcapmsgs;
  map { printonecap $_; } @protocaps;
  print "Search engine names allowed with \"search:\" : ", allsearchengines(), "\n\n";
  print <<EOF;

FILE FORMATS

EOF
  map { printonecap $_; } @filecaps;
  if( $globals{ps2pdf} ) {
    if( $globals{proc_pdf} ) {
      fmt "Using $globals{ps2pdf} to preprocess PostScript files.\n\n";
    }
    else {
      fmt "Cannot process PostScript files because PDF processing is unavailable\n\n";
    }
  }
  elsif( $globals{proc_pdf} ) {
    fmt "Cannot process PostScript files.  Install the ps2pdf command to allow that.\n\n";
  }
  else {
    fmt "Cannot process PostScript files.  Install the ps2pdf command and make PDF processing (see above) available to allow that.\n\n";
  }
  print <<EOF;

TESTS

EOF
  map { printonecap $_; } @testcaps;
}


my %comprnames= ( deflate => "zip", compress => "compress'ed" );
my %formatnames= ( html => "HTML", rtf => "rich text",
  plain => "plain text", pdf => "PDF", ps => "PostScript",
  doc => "MS Word", xls => "MS Excel", ppt => "MS PPT",
  troff => "UNIX manual page" );

# Brief list of wfind's capabilities.
sub printcapsbrief
{
  print "wfind version  $wfversion  from  $wfverdate\n\n";
  print "Concurrency is ", ($globals{fork}? "en" : "dis"), "abled.\n\n";
  print "HTTPS is ", ($globals{lwphttps} ? "":"not "), "supported",
        ($globals{lwpconnect} ? ", also via proxies":""), ".\n\n";
  print "Search engine names allowed with \"search:\" : ", allsearchengines(), "\n\n";
  my @havecompr= map $comprnames{$_} || $_,
                grep $globals{$_} && s/^dec_//, map $_->[0], @filecaps;
  print "Can decompress ", cswordlist(" and ", @havecompr), " files.\n\n";
  my @formats= map $formatnames{$_} || $_,
                grep $globals{$_} && s/^proc_//, map $_->[0], @filecaps;
  push @formats, "PostScript" if $globals{dec_ps} && $globals{proc_pdf};
  print "Can process ", cswordlist(" and ", @formats), " documents.\n\n";
  my @tests;
  for my $t (@testcaps) {
    next unless grep $globals{$_->[0]}, @$t[3..$#$t];
    push @tests, $t->[1];
    $tests[-1] =~ s/\s*tests$//;
  }
  print cswordlist(" and ", @tests), " tests are available.\n\n"
        if @tests;
  print "Add -verbose for more detailed information.\n\n";
}

}



# Find home or data directory and create wfind's private direcory if possible
# (unless it exists).  The cache directory name is set but it is not created to
# avoid croaking on error when -version, -status or -echo is passed.  Also,
# find and parse URL group files and form data files.
sub getwfinddir
{
  my ($datadir, $wfinddir, $tmpdir);

  eval {
    require File::Save::Home;
    $datadir= File::Save::Home::get_home_directory();
  } or
  eval {
    require File::HomeDir;
    $datadir= File::HomeDir->my_data();
  } or
  $datadir= $ENV{HOME};
  if( $datadir &&
        ($wfinddir= File::Spec->catdir($datadir, $globals{wfinddir}))
        && (-d $wfinddir || mkdir $wfinddir) ) {
    my $rc= File::Spec->catfile($wfinddir, "wfindrc");
    parserc($rc) if $rc && -f $rc;
    for( File::Glob::bsd_glob(File::Spec->catdir($wfinddir, "urlgroups", "*")) ) {
      parseurlgroup($_);
    }
    my $bl= File::Spec->catfile($wfinddir, "blacklist");
    if( $bl && -f $bl ) {
      if( $globals{urlgroups}{blacklist} ) {
        fmt *STDERR{IO}, "Blacklist file found both inside and outside urlgroup folder.  Using the one within.\n"
            unless $globals{silent};
      }
      else {
        parseurlgroup($bl);
      }
    }
    checkurlgroupincludes();
    for( File::Glob::bsd_glob(File::Spec->catdir($wfinddir, "forms", "*")) ) {
      parseformdata($_);
    }
    $tmpdir= File::Spec->catdir($wfinddir || File::Spec->tmpdir(), "cache-$$");
  }
  else {
    $tmpdir= File::Spec->catdir($wfinddir || File::Spec->tmpdir(), "wfind-$$");
  }
  $globals{tmpdir}= $tmpdir;
  return if $globals{debug};
  # Remove stale cache directories of previous runs - tried to do this with
  # signal handler but wfind would just hang.
  my $tmptempl= $tmpdir;
  $tmptempl =~ s/\d+$/\*/;
  for my $oldtmp (File::Glob::bsd_glob($tmptempl)) {
    next unless $oldtmp =~ /-(\d+)$/;
    next if kill 0, $1;                 # other wfind still runs
    unlink File::Glob::bsd_glob(File::Spec->catdir($oldtmp, "*"));
    rmdir $oldtmp;
  }
}


# Parse URL group file.  Entries starting with a scheme are interpreted as URL
# prefixes, other entries as domains.  Regular expressions are built for both
# separately and stored in $globals{urlgroups}{<group name>}.  Lines
# starting with the word "include" or "exclude" are interpreted as include /
# exclude directives.  The URL groups they refer to are stored in a list and
# are used in the order in which they occur in the file.
# -> URL group file name
# <- 1 OK, 0 error
sub parseurlgroup
{
  my ($filename)= @_;
  my (%paths, %domains, %includes);

  my (undef, undef, $groupname)= File::Spec->splitpath($filename);
  if( !open(UG, "<$filename") ) {
    fmt *STDERR{IO}, "Could not read URL group \`$groupname'.  Continuing anyway.\n"
        unless $globalopts{silent};
    return 0;
  }
  print STDERR "Reading URL group \`$groupname'...\n" if $globalopts{verbose};
  while( <UG> ) {
    chomp;
    s/\*?\s*(?:\#.*)?$//;
    next unless $_;
    s/^\s+//;
    if( s/^include\s+//i ) {
      $includes{$_}= 1;
    }
    elsif( /^\w+:\/\// ) {
      $paths{$_}= 1;
    }
    else {
      if( /\// ) {
        fmt *STDERR{IO}, "URL group \`$groupname', line $.: Domain entries",
                " may not contain slashes; URL prefixes must start with a ",
                "scheme \`...://'.  Ignored.\n"
          unless $globalopts{silent};
        next;
      }
      $domains{$_}= 1;
    }
  }
  close UG;
  if( !keys(%paths) && !keys(%domains) && !keys(%includes) && !$globalopts{silent} ) {
    fmt *STDERR{IO}, "Warning: Empty URL group $groupname.\n";
  }
  $globals{urlgroups}{$groupname}{pathre}= prefixre(keys %paths) if keys %paths;
  $globals{urlgroups}{$groupname}{domainre}= domainre(keys %domains) if keys %domains;
  $globals{urlgroups}{$groupname}{includes}= [ keys(%includes) ];
  return 1;
}


# Check that URL groups named in include directives really exist and remove
# those that do not.  An error message is output unless -silent is active.
# (<->) $globals->{urlgroups}
sub checkurlgroupincludes
{
  my %unknownincs;

  return unless $globals{urlgroups} && keys %{$globals{urlgroups}};
  for my $grp (keys %{$globals{urlgroups}}) {
    my @okincs;
    for my $inc (@{$globals{urlgroups}{$grp}{includes}}) {
      if( $globals{urlgroups}{$inc} ) { push @okincs, $inc; }
      else                                { $unknownincs{$inc}{$grp}= 1; }
    }
    $globals{urlgroups}{$grp}{includes}= \@okincs;
  }
  if( keys(%unknownincs) && !$globalopts{silent} ) {
    print STDERR "Warning: Unknown URL groups in include directives:\n";
    for my $inc (keys %unknownincs) {
      print STDERR "\`$inc' included from ", cswordlist(" and ", keys %{$unknownincs{$inc}}), "\n";
    }
    print STDERR "Ignoring these includes.\n";
  }
}


# Check if a URL is in a URL group.  The URL groups already checked are kept
# track of with a hash, so cyclical includes do not lead to infinite recursion.
# -> URL (as string)
#    Name of the URL group
#    Optional reference to hash set of URL group names already checked, to
#    prevent infinite recursion
# <- 1 if included,  0 otherwise
sub inurlgroup
{
  my ($url, $group, $trace)= @_;

  return 0 unless $globals{urlgroups}{$group};
  return 0 if $trace && $trace->{$group};
  return 1 if $globals{urlgroups}{$group}{pathre} &&
              $url =~ $globals{urlgroups}{$group}{pathre};
  return 1 if $globals{urlgroups}{$group}{domainre} &&
              $url =~ $globals{urlgroups}{$group}{domainre};
  return 0 unless @{$globals{urlgroups}{$group}{includes}};
  $trace ||= {};
  $trace->{$group}= 1;
  for (@{$globals{urlgroups}{$group}{includes}}) {
    return 1 if inurlgroup($url, $_, $trace);
  }
  return 0;
}


# Parse a file containing data to be filled into an HTML web form.
# -> File name
# (<-) @{$globals{forms}}
sub parseformdata
{
  my ($formfile)= @_;
  my (@keys, @entries);

  my (undef, undef, $formname)= File::Spec->splitpath($formfile);
  if( !open(UG, "<$formfile") ) {
    fmt *STDERR{IO}, "Could not read form data file \`$formname'.  Continuing anyway.\n"
        unless $globalopts{silent};
    return 0;
  }
  print STDERR "Reading form data file \`$formname'...\n" if $globalopts{verbose};
  while( <UG> ) {
    s/^\s+(?:\#.*)?$//;
    next unless $_;
    s/\s+$//;
    if( s/^(id|name|type)\s+//i ) {
      push @entries, [ $1 ];
      @{$entries[-1]}[1,2]= /^("[^"]*"|\S+)(?:\s+(\S.*))?$/;
      pop @entries unless defined $entries[-1][1];
    }
    elsif( /^(\S+)\s+form(id|name)\s+(.*)$/ ) {
      push @keys, [ $1, $2, $3 ];
    }
    elsif( /\s/ ) {
      fmt *STDERR{IO}, "No space allowed in domain or URL (and no \`formid' ",
 "or \`formname' found) in form data file \`$formname' on line $..  Ignored.\n"
          unless $globalopts{silent};
    }
    elsif( /\// ) {
      fmt *STDERR{IO}, "Form data file \`$formname', line $.: Domain entries",
              " may not contain slashes; URL prefixes must start with a ",
              "scheme \`...://'.  Ignored.\n"
        unless $globalopts{silent};
    }
    else {
      push @keys, [ $_ ];
    }
  }
  unless( @keys ) {
    fmt *STDERR{IO}, "No domains or URL prefixes found in form data file \`$formname'.  Ignoring this file.\n"
            unless $globalopts{silent};
    return;
  }
  for my $key (@keys) {
    if( $key->[0] =~ /^\w+:\/\// ) {
      $key->[0]= prefixre($key->[0]);
    }
    else {
      $key->[0]= domainre($key->[0]);
    }
  }
  push @{$globals{forms}}, { urls => \@keys, entries => \@entries };
}


# Remove cache files and directory.
sub removecache
{
  return unless $globals{tmpdir} && -d $globals{tmpdir};
  unlink File::Glob::bsd_glob(File::Spec->catfile($globals{tmpdir}, "*"));
  if( !rmdir($globals{tmpdir}) ) {
    print STDERR "wfind: Warning: Could not remove cache directory ",
                    $globals{tmpdir}, ".\n" unless $globalopts{silent};
  }
}

END {
  removecache() unless $globals{debug} || $globals{isslave};
}


#sectioncmdline###############################################################
##############################################################################
####          Command line parsing and creation of expression tree
##############################################################################
##############################################################################

# Hash set of expression-scoped option hash keys, used for propagation from
# section and global options:
my %localoptkeys;

# Options accepting a full test expression subtree as their argument, needed
# when evaluating their expressions:
# (-while is hard-coded in parseopts() because it is really an action in disguise)
my %treeopts= map { $_ => 1; } qw(linktext linkafter linkbefore);

{

# List of all specifications of link tags and their attributes allowed with the
# -follow and -linksto command-line switches.  The format is "tag" for tags
# which have only one attribute containing a link URL and "tag.attr" for
# specifying an attribute where ambiguous.
my $ltags;

BEGIN {
  $ltags= [ map { $a= $_; map $a.".".$_, keys %{$alllinktags{$a}}; }
                keys %alllinktags ];
  # Allow  giving just the tag if only one attribute can contain a hyperlink
  unshift @$ltags, grep keys(%{$alllinktags{$_}}) == 1, keys %alllinktags;
  unshift @$ltags, "all";
#  print Dumper(\%alllinktags, $ltags);
}

# Command line switches: The arrays contain the switch (without "-"), the type
# of argument (? none (boolean option), # unsigned integer, + signed integer, b
# number of bytes with  optional k/M/G suffix, $ string, * string to be syntax
# checked, ! none (run special function), ARRAYREF cumulative list, @ test
# expression, * checked by checktestparam), the hash key to be set, and the
# hash in which to set it (G global options, P wfind pipe section, S section
# (wfind or scav), A scav action, L local scope, O local operator options).
# The optional fifth element tells if the option is only for the command line
# (""), only for scav script option lines ("o"), only in scav action ("a") or
# only for link-following scav actions ("f").
my @cmdswitches = (
[ "silent", "?", "silent", "G" ],
[ "verbose", "?", "verbose", "G" ],
[ "version", "!", "version", "G" ],
[ "help", "!", "help", "G" ],
[ "slaves", "#", "slaves", "G" ],
[ "echo", "!", "echo", "G" ],
[ "scavecho", "!", "scavecho", "G", "" ],
[ "status", "!", "status", "G" ],
[ "uagent", '$', "useragent", "G" ],
[ "timeout", "#", "timeout", "G" ],
[ "holdoff", "+", "holdoff", "G" ],
[ "userholdoff", "?", "userholdoff", "G" ],
[ "redirects", "#", "redirects", "G" ],
# [ "exec", '$', "exec", "G" ],         # TODO
# [ "cookies", '$', "cookiejar", "G" ], # TODO
[ "gdepth", "#", "maxdepth", "G" ],
[ "maxsize", "b", "maxsize", "G" ],
[ "nogargs", "?", "nogargs", "G", "o" ],
[ "nouargs", "?", "nouargs", "G", "o" ],
[ "asplain", "?", "asplain", "S" ],
[ "download", "?", "download", "P" ],
[ "downxform", '*', "downxform", "A" ],
[ "decompress", "?", "decompress", "A" ],
[ "depth", "#", "maxdepth", "S" ],
[ "inlineframes", "?", "inline", "S" ],
[ "httpdirs", "?", "httpdirs", "S" ],
[ "print", [ qw(all url furl type size time trace linkprops) ], "print", "A" ],   # TODO:  nmatches matchpos quote
[ "nostart", "?", "nostart", "A" ],
[ "max", "#", "maxresults", "A" ],
[ "follow", $ltags, "follow", "A" ],
[ "symlinks", "?", "symlinks", "A" ],
[ "unrestricted", "?", "unrestricted", "A" ],
[ "samedomain", "?", "samedomain", "A" ],
[ "plainurls", "?", "plainurls", "A" ],
[ "pessimistic", "?", "pessimistic", "A" ],
[ "transform", '*', "transform", "A" ],
[ "require", "#", "require", "O" ],     # generalisation of AND and OR
[ "wordnear", "#", "wordnear", "O" ],
[ "charnear", "#", "charnear", "O" ],
[ "linktags", $ltags, "linktags", "L" ],
);

# Negation of boolean switches:
my @nocmdswitches = map [ "no".$_->[0], "?", @{$_}[2..$#$_] ],
                        grep( $_->[1] eq "?", @cmdswitches );

# Options accepting a full test expression subtree as their argument:
push @cmdswitches, map [ $_, "%", $_, "A" ], keys %treeopts;

# Hash set of options not allowed in wfindrc:
my %norcopts= map { $_->[0] => 1; }
        grep $_->[3] eq "O" || $_->[1] eq "!", (@cmdswitches, @nocmdswitches);

# Hash set of local option hash keys, used for propagation in expression tree:
%localoptkeys= map { $_->[2] => 1; }
        grep $_->[3] eq "L", (@cmdswitches, @nocmdswitches);

# List of test options.  The first element of the array is the command-line
# switch without the "-", the second is the type of its argument (see above;
# "@" means a content-like test parsed by parsetest(), "*" something that will
# be checked after command-line parsing by checktestparam()), the third is the
# class of the test (utests to be applied to the document's URL, htests for the
# HTTP header, and ctests requiring the downloaded file).
my @testswitches = (
[ "true", "?", "utests" ],
[ "false", "?", "utests" ],
[ "url", "@", "utests" ],
[ "urlgroup", "*", "utests" ],
[ "name", "@", "utests" ],
[ "localfile", "*", "utests" ],
[ "size", "b", "htests" ],
[ "modified", "*", "htests" ],
[ "type", "@", "htests" ],
[ "httpheader", "*", "htests" ],
[ "rsize", "b", "ctests" ],
[ "rtype", "@", "ctests" ],
[ "linksto", "@", "ctests" ],
[ "header", "*", "ctests" ],
[ "exif", "*", "ctests" ]
);

# Regex for boolean operator options:
my $OPERATOROPT= qr/^(?:!|-not|-a|-and|-o|-or)$/i;


# Filter a hashset of option values by class (global, section, local etc.).
# -> String of allowed option classes denoted by capital letters
#    Reference to option hash set
# <- Reference to filtered option hash set
sub grepoptclass
{
  my ($class, $hset)= @_;
  my %filtered;

  for my $opt (@cmdswitches) {
    next unless $opt->[3] =~ /[$class]/;
    $filtered{$opt->[2]}= $$hset{$opt->[2]}
        if defined $$hset{$opt->[2]};
  }
  return \%filtered;
}


# Add section and argument number to error message and croak.  Used by
# parseopts() and its subroutines for errors in command-line options, not in
# scripts.
# -> Reference to pipe section array (for obtaining current section)
#    Reference to index of option argument on command line
#    Error message
#    Flag to just output a warning without exiting
#    Reference to pipe section hash
sub cmdlineoptmsg
{
  my ($secary, $argnr, $msg, $warn, $pipesec)= @_;

  return if $warn && $globalopts{silent};
  $msg .= " in pipe section $#$secary, test argument $$argnr.";
  $msg .= "  Aborting." unless $warn;
  fmt(*STDERR{IO}, "$msg\n");
  if( $pipesec ) {
    if( $globalopts{verbose} ) {
      print STDERR "Expression tree so far:\n";
      printexprtree($pipesec->{testexpr}, "");
      if( $pipesec->{while} && @{$pipesec->{while}->{subexprs}} ) {
        print STDERR "While expression tree so far:\n";
        printexprtree($pipesec->{while}{subexprs}[0], "");
      }
    }
    else {
      print STDERR "Use -verbose for more information.\n";
    }
  }
  exit 1 unless $warn;
}


# Add location in script to error message and die (to be caught by
# parsescript).  Used by parseopts() and its subroutines for errors in scripts.
# -> String describing script file name and line
#    Reference to index of option argument in action or option line
#    Error message
#    Flag to just output a warning without exiting
#    Reference to pipe section hash
sub scriptoptmsg
{
  my ($scriptloc, $argnr, $msg, $warn, $pipesec)= @_;

  return if $warn && $globalopts{silent};
  fmt(*STDERR{IO}, "$msg at test argument $$argnr $scriptloc.\n");
  if( $pipesec ) {
    if( $globalopts{verbose} ) {
      print STDERR "Expression tree so far:\n";
      printexprtree($pipesec->{testexpr}, "");
    }
    else {
      print STDERR "Use -verbose for more information.\n";
    }
  }
  exit 1 unless $warn;
}


# Parse test and option arguments.
# -> Reference to array containing command-line options (no URLs)
#    Reference to array of pipe section hashes; or reference to action hash
#    Single-char scalar indicating the scav script context (o = option line, f
#    = link-following action, a = other action), or undef for wfind
#    String describing location of option arguments in script; or undef for
#    command line (used for error messages)
# (<-) Test expression tree generated in "testexpr" entry of pipe section
#    hashes, options entered in %globalopts, in "options" entry of pipe section
#    hashes and local expressions.
sub parseopts
{
  my ($args, $secs, $scriptmode, $scriptloc)= @_;
  my $pipesec;
  my $intreeopt= "";
  my @exprstack;                # stack of subexpressions
  my @parenstack;       # stack of parenthesis levels = local option scopes
  my $argnr= 0;
  my $error= $scriptmode ? sub { scriptoptmsg($scriptloc, \$argnr, @_); } :
                           sub { cmdlineoptmsg($secs, \$argnr, @_); };

  $scriptmode ||= "";
  if( ref($secs) eq "ARRAY" ) {   # command-line or script section option line
    $pipesec= $$secs[-1];
  }
  elsif( ref($secs) eq "HASH" ) { # script action line
    $pipesec= $secs;
  }
  my $expr= $pipesec->{testexpr}= dup($emptyand);
  my $optscope= $scriptmode eq "o"? $pipesec : $expr;   # scope of local options
  shift @$args if @$args && $$args[0] eq "--";
  my $arg;
  my $lastarg= "";
  while( defined($arg= shift(@$args)) )
  {
    # Allow double "-" for options
    $arg =~ s/^--/-/;
    # End unary operator subexpressions.  Allow chaining unary operators.
    while( ($expr->{type} =~ /^(?:not|while)$/ || $treeopts{$expr->{type}})
        && @{$expr->{subexprs}} == 1 ) {
      $intreeopt= "" if $treeopts{$expr->{type}};
      $expr= pop @exprstack;            # scope ends after one test or () block
    }
    # Now look at new argument.  First, operators.
    if( $arg eq "(" ) {                 # start of parenthesised subexpression
      if( $scriptmode eq "o" ) {
        &$error("Test grouping parentheses not allowed (and pointless) in script option line");
      }
      my $newexpr= dup($emptyand);
      push @{$expr->{subexprs}}, $newexpr;
      push @exprstack, $expr;
      $expr= $newexpr;
      push @parenstack, $optscope;
      $optscope= $newexpr;
    }
    elsif( $arg eq ")" ) {              # end of parenthesised subexpression
      if( !@exprstack ) {
        &$error("Surplus `)' in test expression,", 0, $pipesec);
      }
      if( $lastarg eq "(" ) {
        &$error("Empty pair of parentheses in test expression,", 0, $pipesec);
      }
      if( $lastarg =~ /$OPERATOROPT/o ) {
        &$error("Missing operand", 0, $pipesec);
      }
      push @exprstack, $expr;     # include current expression in checks below
      $expr= $optscope= pop @parenstack;
      while( @exprstack && (my $e= pop(@exprstack)) != $expr ) {
        &$error("-$e->{type} option needs a subexpression argument")
            if ($e->{type} =~ /^(?:not|while)$/ || $treeopts{$e->{type}})
                && @{$e->{subexprs}} == 0;
        $intreeopt= "" if $treeopts{$e->{type}};
      }
    }
    elsif( $arg eq "|" ) {              # end of pipe section
      if( $scriptmode ) {
        &$error("Pipe symbol not allowed in scripts (use section labels and control flow commands)");
      }
      checkexprcomplete(\@parenstack, $expr, $pipesec, $error);
      propagatelocalopts($pipesec);
      push @$secs, { options => { %{$pipesec->{options}} },
                        utests => [], htests => [], ctests => [],
                        testexpr => dup($emptyand) };
      $pipesec= $$secs[-1];
      $expr= $pipesec->{testexpr};
      $optscope= $expr;
    }
    elsif( $arg eq "!" || lc($arg) eq "-not" ) {        # "not" operator
      if( $scriptmode eq "o" ) {
        &$error("Test negation not allowed (and pointless) in script option line");
      }
      my $newexpr= { type => "not", class => "op",
                        options => {}, subexprs => [] };
      push @{$expr->{subexprs}}, $newexpr;
      push @exprstack, $expr;
      $expr= $newexpr;
    }
    elsif( lc($arg) eq "-a" || lc($arg) eq "-and" ) {   # "and" is always implied
      if( $scriptmode eq "o" ) {
        &$error("Test operator not allowed (and pointless) in script option line");
      }
      if( !$argnr || $lastarg eq "(" ) {
        &$error("(Sub)Expression must not start with binary operator", 0, $pipesec);
      }
      if( $lastarg =~ /$OPERATOROPT/o ) {
        &$error("Operator following operator");
      }
      if( ! @{$expr->{subexprs}} ) {
        &$error("Missing argument to operator or subtree option before $arg", 0, $pipesec);
      }
    }
    elsif( lc($arg) eq "-o" || lc($arg) eq "-or" ) {    # "or" operator
      if( $scriptmode eq "o" ) {
        &$error("Test operators not allowed (and pointless) in script option line");
      }
      if( !$argnr || $lastarg eq "(" ) {
        &$error("(Sub)Expression must not start with binary operator", 0, $pipesec);
      }
      if( $lastarg =~ /$OPERATOROPT/o ) {
        &$error("Operator following operator", 0, $pipesec);
      }
      if( ! @{$expr->{subexprs}} ) {
        &$error("Missing argument to operator or subtree option before $arg", 0, $pipesec);
      }
      # Backtrack across operators with a higher precedence
      push @exprstack, $expr;
      while( @exprstack && $exprstack[-1] != $optscope &&
              $exprstack[-1]{type} =~ /^(?:not|and)$/ ) {
        pop @exprstack;
      }
      if( @exprstack && $exprstack[-1]{type} eq "or" ) {
        # Add more operands to existing OR
        $expr= pop @exprstack;
      }
      else {
        $expr= { type => "or", class => "op",
                 options => {}, subexprs => [] };
        if( @exprstack ) {
          # previous expression is first argument of OR
          my $arg= pop @exprstack;
          if( $arg == $optscope ) {
            # argument is previous top-level expression inside parentheses
            $expr->{options}= $arg->{options};
            $arg->{options}= {};
            $optscope= $expr;
          }
          push @{$expr->{subexprs}}, $arg;
          if( $pipesec->{testexpr} == $arg ) {
            $pipesec->{testexpr}= $expr;
          }
          else {
            $exprstack[-1]{subexprs}[-1]= $expr;
          }
        }
        else {    # first argument of OR is previous top-level expression
          push @{$expr->{subexprs}}, $pipesec->{testexpr};
          $pipesec->{testexpr}= $expr;
        }
      }
      # Add an "and" as new argument of OR to allow default AND without -a.
      my $newand= dup($emptyand);
      push @{$expr->{subexprs}}, $newand;
      push @exprstack, $expr;
      $expr= $newand;
    }
    elsif( lc($arg) eq "-while" ) {
      &$error("-while not allowed in scripts (use script control flow commands)")
          if $scriptmode;
      &$error("Only one -while option allowed per ", $scriptmode? "action," : "pipe section")
          if $pipesec->{while};
      checkexprcomplete(\@parenstack, $expr, $pipesec, $error, " before $arg");
      $pipesec->{while}= { type => "while", class => "flow",
                           options => {}, subexprs => [] };
      push @exprstack, $expr;
      $expr= $pipesec->{while};
    }
    # Now for options and non-content tests
    elsif( $arg =~ /^-(\w+)\b/i ) {
      my $optname= lc($1);
      my $optre= quotemeta($optname);
      my @opts= grep $_->[0] =~ /^$optre/i, @cmdswitches;
      my @nopts= grep $_->[0] =~ /^$optre/i, @nocmdswitches;
      my @topts= grep $_->[0] =~ /^$optre/i, @testswitches;
      my @invopts;  # options invalid due to context ($scriptmode); kept for error message
      if( $scriptmode eq "o" ) {
        push @invopts, @topts;
        @topts= ();
      }
      for my $ary (\@opts, \@nopts) {
        my @newary;
        for (@$ary) {
          if( defined $_->[4] && $_->[4] ne $scriptmode ||
              $scriptmode =~ /^[af]/ && $_->[3] =~ /^[GPS]/ ||
              $scriptmode eq "o" && $_->[3] =~ /^[OP]/ ) {
            push @invopts, $_;
          }
          else {
            push @newary, $_;
          }
          @$ary= @newary;
        }
      }
      my @allfound= (@opts, @nopts, @topts);
      if( @allfound > 1 ) {
        my @exact= grep $_->[0] eq $optname, @allfound;
        &$error("Ambiguous option `$arg'.  Could be " .
              cswordlist(" or ", map("-".$_->[0], @allfound)) . "\n -")
            unless @exact == 1;
        @allfound= @exact;
      }
      if( @allfound == 0 ) {
        if( @invopts ) {
          my @invnames= map { $_= "-$_->[0]"; } @invopts;
          my $where= { o => "in script option line", a => "in script action",
                       f => "in script action", "" => "on wfind command line" }
                     ->{$scriptmode};
          &$error("Option" . (@invopts>1?"s ":" ") . cswordlist(" and ",
                  @invnames) . " not allowed $where");
        }
        else {
          &$error("Unknown option `$arg'");
        }
      }
      if( !$scriptmode && @parenstack && ((@opts && $opts[0][3] !~ /^[OL]/) ||
                                     (@nopts && $nopts[0][3] !~ /^[OL]/)) ) {
        &$error("Warning: Option `$arg' in subexpression despite not being locally scoped", 1);
      }
      if( @topts && $intreeopt ) {
        &$error("Only primitive tests allowed in -$intreeopt.  Found test `$arg'");
      }
      my $opt= $opts[0] || $nopts[0] || $topts[0];
      my $val;
      if( $opt->[1] eq "?" || $opt->[1] eq "!" ) {
        $val= 1 - @nopts;
      }
      elsif( $opt->[1] ne "%" ) {                # get option arguments
        if( $arg =~ /=(.*)$/ ) {
          $val= $1;
        }
        else {
          $val= shift @$args;
          &$error("$arg has to be followed by an argument")
              if !defined($val) || $val =~ /^[()|!-]$/;
          ++$argnr;
        }
      }
      if( @topts ) {            # non-content test
        testavailable($opt->[0], $error);
        my $testexpr;
        if( $opt->[1] eq "@" ) {
          $testexpr= parsetest($val, $error);
          $testexpr->{type}= $opt->[0];
          $testexpr->{class}= $opt->[2];
        }
        elsif( $opt->[1] eq "#" || $opt->[1] eq "b" ) {
          $val =~ s/^([+-])//;
          my $cmp= $1 || "=";
          $val= parsenumarg($opt, $val, 1, "test argument $argnr", \&cmdopterr);
          $testexpr= { type => $opt->[0], class => $opt->[2],
                        arg => $val, subtype => $cmp };
        }
        else {
          $testexpr= { type => $opt->[0], class => $opt->[2], arg => $val };
        }
        checktestparam($testexpr, $error) if $opt->[1] eq "*";
        push @{$expr->{subexprs}}, $testexpr;
      }
      elsif( $opt->[1] eq "%" ) {         # subtree option
        checkexprcomplete(\@parenstack, $expr, $pipesec, $error, " before $arg");
        &$error("Cannot nest subtree options (-$opt->[0] within -$intreeopt)")
            if $intreeopt;
        &$error("Only one $opt->[2] option allowed per ", $scriptmode? "action" : "pipe section")
            if $pipesec->{$opt->[2]};
        $pipesec->{$opt->[2]}= { type => $opt->[2], class => "subtree",
                                 options => {}, subexprs => [] };
        push @exprstack, $expr;
        $expr= $pipesec->{$opt->[2]};
        $intreeopt= $opt->[2];
      }
      else {                    # other option
        checktestparam({ type => $opt->[0], arg => $val }, $error)
            if $opt->[1] eq "*";
        my $target;
        if( $opt->[3] eq "G" )          { $target= \%globalopts; }
        elsif( $opt->[3] =~ /^[OL]/ )   { $target= $optscope->{options}; }
        else {
          &$error("Option $opt->[0] only allowed in " . ($opt->[3] eq "P" ?
                      "wfind pipe sections" : "section option lines"))
            if ($opt->[3] eq "P" && $scriptmode) ||
               ($opt->[3] eq "S" && $scriptmode =~ /^[^o]/);
          $target= $pipesec->{options};
        }
        setopt($opt, $val, $target, "test argument $argnr", \&cmdopterr);
      }
    }
    else {                              # Test text content
      testavailable($1, $error) if $arg =~ /^(~|\$\$?)/;
      my $testexpr= parsetest($arg, $error);
      $testexpr->{type}= "intext";
      $testexpr->{class}= "ctests";
      push @{$expr->{subexprs}}, $testexpr;
    }
    $lastarg= $arg;
    ++$argnr;
  }
  checkexprcomplete(\@parenstack, $expr, $pipesec, $error);
  propagatelocalopts($pipesec);
}


# Abort with error if a parsed expression has parentheses that are not closed
# or operators without arguments.
# -> Reference to parenthesis stack array
#    Reference to hash of current operator expression
#    Reference to hash of current pipe section
#    Reference to error output function
#    String giving more details on where the error occurred (optional)
sub checkexprcomplete
{
  my ($parenstack, $currexpr, $pipesec, $error, $details)= @_;

  $details //= "";
  if( @$parenstack ) {
    &$error("Surplus `(' in test expression$details,", 0, $pipesec);
  }
  if( ! @{$currexpr->{subexprs}} && $currexpr != $pipesec->{testexpr} ) {
    # Complain about missing arguments but allow completely empty test expression
    &$error("Missing argument to operator or subtree option$details", 0, $pipesec);
  }
}


# Propgate local options from pipe section to its expressions (normal, while
# and other subtree expressions as applicable).  (Also works for actions.)
# -> Refernce to section hash
sub propagatelocalopts
{
  my ($section)= @_;

  my @lockeys= grep $localoptkeys{$_}, keys %{$section->{options}};
  my %locopts;
  @locopts{@lockeys}= @{$section->{options}}{@lockeys};
  my @exprs= ($section->{testexpr});
  push @exprs, $section->{while} if $section->{while};
  push @exprs, grep defined, @$section{keys %treeopts};
  propagateopts(\%locopts, @exprs);
}


# Recursively propagate options in an expression tree.  Options explicitly set
# in subexpressions override the propagated option values.
# -> Reference to option hash to propagate
#    List of expressions to propagate options to
sub propagateopts
{
  my $opts= shift;

  for my $expr (@_) {
    while( my ($opt, $val) = each %$opts ) {
      $expr->{options}{$opt}= $val
        unless exists $expr->{options}{$opt};
    }
    if( $expr->{subexprs} ) {
      propagateopts($expr->{options}, @{$expr->{subexprs}});
    }
  }
}


my %boolargto01= ( true => 1, on => 1, yes => 1, "1" => 1,
                false => 0, off => 0, no => 0, "0" => 0 );

# Parse wfindrc.  Error in option names or values cause that option to be
# ignored.
# -> RC file name
# <- 1 OK, 0 could not open file
sub parserc
{
  my ($rcname)= @_;
  my ($optname, $val, $optre, $opt, $target);
  my (@opts, @nopts, @allfound);

  if( !open(RC, "<$rcname") ) {
    fmt *STDERR{IO}, "Could not read wfindrc `$rcname'.  Continuing anyway.\n"
        unless $globalopts{silent};
    return 0;
  }
  print STDERR "Reading wfindrc `$rcname'...\n" if $globalopts{verbose};
  while( <RC> ) {
    chomp;
    s/\s*(?:\#.*)?$//;
    next unless $_;
    ($optname, $val)= /^\s*(\w+)(?:\s+|\s*=\s*)(.*)$/;
    $optname= lc($optname);
    $optre= quotemeta($optname);
    @opts= grep $_->[0] =~ /^$optre/i, (@cmdswitches, [ "prefer" ]);
    @nopts= grep $_->[0] =~ /^$optre/i, @nocmdswitches;
    @allfound= grep !$norcopts{$_}, (@opts, @nopts);
    if( @allfound > 1 ) {
      fmt *STDERR{IO}, "Ambiguous option `$optname' in wfindrc (line $.).  ",
        "Could be ", cswordlist(" or ", map $_->[0], @allfound), ".  Ignored.\n"
          unless $globalopts{silent};
      next;
    }
    if( @allfound == 0 ) {
      fmt *STDERR{IO}, "Option `$optname' unknown or not allowed in wfindrc (line $.).  Ignored.\n"
          unless $globalopts{silent};
      next;
    }
    $opt= $opts[0] || $nopts[0];
    if( $opt->[0] eq "prefer" ) {
      my @rejects= prefercap(split /[,\s]+/, $val);
      fmt *STDERR{IO}, "Preferred capability alternative", (@rejects>1?"s ":" "),
            cswordlist(" and ", @rejects), " not found (in wfindrc, line $.).  Ignored.\n"
            if @rejects;
      next;
    }
    if( $opt->[1] eq "?" ) {
      if( $val ) {
        my $normval= $boolargto01{lc($val)};
        unless( defined($normval) ) {
          fmt *STDERR{IO}, "Boolean option $optname cannot take the value $val",
                           " (in wfindrc, line $.).  Ignored.\n"
              unless $globalopts{silent};
          next;
        }
        $val= @nopts? !$normval : $normval;
      }
      else {
        $val= 1 - @nopts;
      }
    }
    elsif( !defined($val) ) {
      fmt *STDERR{IO}, "Option $optname has to be followed by an ",
                        "argument (in wfindrc, line $.).  Ignored.\n"
          unless $globalopts{silent};
      next;
    }
    if( $opt->[3] eq "G" )      { $target= \%globalopts; }
    else                        { $target= $pipesecs[0]{options}; }
    setopt( $opt, $val, $target, "in wfindrc, line $.", \&rcopterr );
  }
  close RC;
  return 1;
}


# Error handler for last argument of setopt() and parsenumarg() when called for
# command-line options.  Errors are reported with the option name prepended by
# a minus sign, and the program is terminated.
sub cmdopterr
{
  croak "-", @_, "Aborting.";
}

# Error handler for last argument of setopt() and parsenumarg() when called for
# wfindrc options.  Errors are reported with the option name prepended by
# "Option", but are otherwise ignored.
sub rcopterr
{
  fmt *STDERR{IO}, "Option ", @_, "Ignored.\n"
        unless $globalopts{silent};
  return undef;
}


# Determine the value of array and (using parsenumarg()) numerical options and
# assign them to an options hash.
# -> Reference to array representing option (containing name, type, key, scope)
#    String value of option
#    Reference to options hash to assign to
#    String describing where the option was encountered (for error messages)
#    Reference to error handler, which is passed a list of strings describing
#    the error
sub setopt
{
  my ($opt, $val, $target, $where, $err)= @_;

  if( ref($opt->[1]) eq "ARRAY" ) {
    $val =~ s/^([+-])//;
    my $aryop= $1 || "";
    my @vals= split /[,\s]+/, lc($val);
    my %valtest;
    @valtest{@vals}= ();                # set all given values to undef
    delete @valtest{@{$opt->[1]}};      # delete legal values
    if( keys %valtest ) {
      &$err($opt->[0], " cannot take these argument(s) ($where): ",
          join(", ", keys(%valtest)), 
          "\nIt may be followed by one or several of the ",
          "following arguments: \n", join(", ", @{$opt->[1]}), "\n");
    }
    if( grep $_ eq "all", @vals ) {
      if( $aryop eq "-" ) {
        $target->{$opt->[2]}= {};
      }
      else {   # set all values, including "all" for easy checking
        @{$target->{$opt->[2]}}{@{$opt->[1]}}= (1) x @{$opt->[1]};
      }
    }
    else {
      if( $aryop eq "+" ) {
        @{$target->{$opt->[2]}}{@vals}= (1) x @vals;
      }
      elsif( $aryop eq "-" ) {
        delete @{$target->{$opt->[2]}}{@vals};
      }
      else {
        $target->{$opt->[2]}= { map { $_ => 1; } @vals };
      }
    }
  }
  else {
    if( $opt->[1] =~ /^[b#+]/ ) {
      $val= parsenumarg($opt, $val, 0, $where, $err);
      return unless defined($val);
    }
    $target->{$opt->[2]}= $val;
  }
}


# Parse a numerical option parameter argument.
# -> Reference to option description hash
#    Parameter argument
#    Flag indicating a test option, which allows a sign prefix for numerical
#    options to indicate the test comparison
#    String describing where the option was encountered (for error messages)
#    Reference to error handler, which is passed a list of strings describing
#    the error
# <- The evaluated parameter argument, identical to the one passed except for
#    byte sizes which have their k/M/G suffix evaluated.  If an error occurs,
#    the result of the error handler function is returned, which is usually
#    undef unless it croaks right away.
sub parsenumarg
{
  my ($opt, $val, $intest, $where, $err)= @_;

  my $sign= $intest? ", optionally prefixed by a sign" : "";
  if( $opt->[1] eq "#" ) {
    return &$err($opt->[0], " has to be followed by a numerical ",
          "argument$sign ($where).  ")
      if $val !~ /^\d+$/;
  }
  elsif( $opt->[1] eq "+" ) {
    return &$err($opt->[0], " has to be followed by a (possibly signed)",
          " numerical argument$sign ($where).  ")
      if $val !~ /^[+-]?\d+$/;
  }
  elsif( $opt->[1] eq "b" ) {
    return &$err($opt->[0], " has to be followed by a size in bytes with ",
          "optional quantifier k/M/G$sign ($where).  ")
        if $val !~ /^\d+[kmg]?$/i;
    $val =~ s/([kmg]?)$//i;
    my $q= lc($1);
    $val *= { k => 1024, m => 1048576, g => 1073741824 }->{$q} if $q;
  }
  return $val;
}


# Print out an option value to stdout.  For echoing the explicit and implicit
# options back to the user.
# -> Reference to option hash
#    Prefix to prepend before each option output
#    Column in which to print value
sub printopthash
{
  my ($opthash, $prefix, $valcol)= @_;

  while( my ($key, $val) = each %$opthash ) {
    next unless defined($val);
    my ($opt)= grep $_->[2] eq $key, @cmdswitches;
    next unless defined($opt);
    my $keystr= "$prefix$key" . " " x ($valcol - length("$prefix$key"));
    if( ref($opt->[1]) eq "ARRAY" ) {
      unless( keys %$val ) {
        print "$prefix$key\n";
        next;
      }
      print $keystr;
      my @vallist;
      if( ref($$val{(keys %$val)[0]}) ) {
        if( $key eq "follow" && exists($$opthash{forward}) ) {
          # A kludge to regenerate the complete list of link attributes to
          # follow, which was divided up between "follow" and "forward"
          @vallist= sort
            ((map { $a= $_; map "$a.$_", set($val->{$a}); } keys %$val),
             (map { $a= $_; map "$a.$_", set($opthash->{forward}{$a}); }
                keys %{$opthash->{forward}}));
        }
        else {
          @vallist= map { $a= $_; map "$a.$_", set($val->{$a}); }
                      sort keys %$val;
        }
      }
      else {
        @vallist= keys(%$val);
      }
      print join(", ", @vallist);
    }
    elsif( $opt->[1] eq "?" || $opt->[1] eq "!" ) {
      print $keystr, $val? "True" : "False";
    }
    else {
      $val =~ tr/\n/ /;
      print $keystr, $val;
    }
    print "\n";
  }
}


# Preliminary parse of command-line arguments to extract -verbose and -silent
# for wfind, and to parse and remove the pre-script global options for scav.
# If options are added, the comparisons may have to be modified to avoid
# ambiguities.
# (->) @ARGV
# (<-) %globalopts
sub preparseopts
{
  if( $globals{scav} ) {
    my @preopts= ( qw(silent verbose nosilent noverbose), map $_->[0], grep $_->[1] eq "!", @cmdswitches );
    while( @ARGV && $ARGV[0] =~ /^-/ ) {
      my $arg= shift @ARGV;
      $arg =~ s/^--?//;
      my $argre= quotemeta($arg);
      my @matches= grep /^$arg/i, @preopts;
      if( !@matches ) {
        croak "Error: Unrecognised global option.  Only ",
                cswordlist(" and ", map "-$_", @preopts),
                " are allowed before the script file name.";
      }
      elsif( @matches > 1 ) {
        croak "Error: ambiguous global option -$arg.  Could be ",
              cswordlist(" or ", map "-$_", @preopts), ".";
      }
      $matches[0] =~ s/^(no)//;
      $globalopts{$matches[0]}= $1 ? 0 : 1;
    }
  }
  else {
    for (@ARGV) {
      my $arg= $_;
      next unless $arg =~ s/^--?//;
      my $argre= quotemeta($arg);
      my $arglen= length($arg);
      $globalopts{verbose}= 1 if $arglen >= 4 && "verbose" =~ /^$argre/i;
      $globalopts{verbose}= 0 if $arglen >= 6 && "noverbose" =~ /^$argre/i;
      $globalopts{silent}= 1 if $arglen >= 2 && "silent" =~ /^$argre/i;
      $globalopts{silent}= 0 if $arglen >= 4 && "nosilent" =~ /^$argre/i;
    }
  }
  $globalopts{silent}= 0 if $globalopts{verbose};
}


}       # end of scope for option name constants


# Parse a test argument.  The first character decides whether it is a glob
# pattern, regular expression, phonetic equivalence or Levenshtein similarity.
# -> Command-line argument
#    Error handling function taking message and warning flag as arguments
# <- Ref. to hash identifying test
sub parsetest
{
  my ($testarg, $error)= @_;
  my $teststr= $testarg;
  my $testexpr;

  if( $teststr =~ s/^\/// ) {           # regular expression
    $teststr =~ s/\/([^\/]*)$//;
    my $flags= $1 || "";
    if( $teststr =~ /\(\?\??\{/ ) {
      &$error("Sorry, no `(?{' eval code allowed in wfind regexes: `$testarg'\n -");
    }
    if( $flags =~ /[^Iv]/ ) {
      $flags =~ s/[Iv]//g;
      &$error("Unknown regular expression flag(s) `$flags' in test `$testarg'");
    }
    $teststr= &$uni2base($teststr) unless $flags =~ /v/;
    my $re= "(?s";
    $re .= "i" unless $flags =~ /I/;
    $re .= ":$teststr)";
    eval { "blabla" =~ /$re/; };
    &$error("Error in test evaluation of regular expression `$testarg':\n$@\n -")
        if $@;
# TODO: warn against uselessness of ^ $ \A \Z \z except for -linktext
    if( "" =~ /$re/ && $testarg !~ /[^\\]\^|[^\\]\$|\\[AzZbB]/ ) {
      &$error("Warning: Regular expression `$testarg' seems to match any string", 1);
    }
    $testexpr= { matchtype => "re", pattern => $re,
                                      nouni2base => scalar($flags =~ /v/) };
  }
  elsif( $teststr =~ s/^~// ) {         # Levenshtein similarity
    my ($flags)= $teststr =~ s/~([^~]*)$// || "";
    my @wordlist= split /[^$WORDCHARS]+/o, $teststr;
    $testexpr= { matchtype => "leven", pattern => \@wordlist };
    if( $flags ) {
      $flags =~ s/(I?)(?:(\d+\%?)|\%)(I?)$// or 
                &$error("Unknown flag(s) in test `$testarg'");
      my $dist= $2 || "20%";
      if( $1 || $3 ) {
        $testexpr->{case}= 1;
      }
      else {
        map { $_= lc($_); } @{$testexpr->{pattern}};
      }
      if( $dist =~ s/\%$// ) {
        $testexpr->{reldist}= $dist / 100.0 || 0.2;   # 20% if "%" or "0%"
        $testexpr->{plengths}= [ map length($_), @{$testexpr->{patterns}} ];
      }
      else {
        $testexpr->{distance}= $dist;
      }
    }
    else {      # default: 20% relative distance, case insensitive
      map { $_= lc($_); } @{$testexpr->{pattern}};
      $testexpr->{reldist}= 0.2;
      $testexpr->{plengths}= [ map length($_), @{$testexpr->{patterns}} ];
    }
  }
  elsif( $teststr =~ s/^\$// ) {        # phonetic equivalence
    $teststr =~ s/\$$//;
    my @scodes= split /[^$WORDCHARS*?]+/o, $teststr;
    while( $scodes[0] =~ /^$/ ) { shift @scodes; }
    for my $word (@scodes) {
      my ($scode)= $word eq "*" || $word eq "?" ? (undef) : phoneticenc($word);
      if( !defined($scode) && $word ne "*" && $word ne "?" ) {
        &$error( "Could not determine phonetic code for `$word'.  Give `*' or `?' for a wildcard.\n -");
      }
      $word= $scode;
    }
    $testexpr= { matchtype => "soundex", pattern => \@scodes };
  }
  else {                                # word(s) or glob pattern(s) (no [] {})
    my $flags= "";
    if( $teststr =~ s/^=// ) {
      $teststr =~ s/=([^=]*)$//;
      $flags= $1 || "";
      &$error("Unknown pattern flag(s) in test `$testarg'")
                if $flags =~ /[^Iv]/;
    }
    my $pat= "(?s";
    $pat .= "i" unless $flags =~ /I/;
    if( $flags =~ /v/ ) {
      $teststr =~ tr/?*/\001\002/;
      $teststr= &$uni2base($teststr);
      $teststr =~ s/([^$WORDCHARS\s\001\002])/\\s\*\\$1\\s\*/g;
# TODO: this treatment of ? and * disallows verbatim ? from GET method submissions
      $teststr =~ s/\001/\[$WORDCHARS\]/g;
      $teststr =~ s/\002/\[$WORDCHARS\]\*/g;
      $teststr =~ s/\\s\*\\s\*/\\s\*/g;
      $teststr =~ s/\\s\*\s+|\s+\\s\*/\\s\*/g;
      $teststr =~ s/^\s+([$WORDCHARS])/\\b$1/;
      $teststr =~ s/([$WORDCHARS])\s+$/$1\\b/;
      $teststr =~ s/\s+/\\s\+/;
      $pat .= ":$teststr)";
      eval { "blabla" =~ /$pat/; };
      if( $@ ) {
        &$error("wfind fouled up the generation of a regular expression for the ".
            "glob pattern argument `$testarg'.  Please send the offending ".
            "argument to the developer.\n -");
      }
      if( "" =~ /$pat/ ) {
        &$error("Regular expression derived from glob pattern argument `$testarg' matches the empty string");
      }
    }
    else {
      $pat .= ":";
      my @patterns;
      for my $word (split /[^$WORDCHARS?*]+/o, $teststr) {
        $word =~ s/\?/\[$WORDCHARS\]/go;
        $word =~ s/\*/\[$WORDCHARS\]\*/go;
        $word= &$uni2base($word);
        push @patterns, $pat . "^$word\$)";
        eval { "blabla" =~ /$patterns[-1]/; };
        if( $@ ) {
          &$error("wfind fouled up the generation of a regular expression for the ".
            "glob pattern argument `$testarg'.  Please send the offending ".
            "argument to the developer.\n -");
        }
      }
      if( !@patterns ) {
        my $vtestarg= "=$teststr=v$flags";
        &$error("The word search `$testarg' contains no word characters." .
                "  To search for non-word characters, use the =v option " .
                "(i.e. `$vtestarg') or a regex test,");
      }
      $pat= \@patterns;
    }
    $testexpr= { matchtype => "glob", pattern => $pat };
  }
  return $testexpr;
}


# Try to extract search terms from glob test patterns.  Used to auto-generate
# search engine terms.  Words from  multiple-word glob tests are quoted.  Words
# containing wildcards are discarded without replacement.  The result has
# spaces replaced by "+" and is otherwise URL escaped, so it can be fed
# directly into GET request URLs.
# -> Reference to scalar to write result string to, possibly empty string if
#    too much had to be discarded
#    References to glob test hashes
# <- 0 OK, 1 if wildcard words were discarded
sub globtest2searchstring
{
  my ($result, @tests)= @_;
  my $searchstr;
  my $havewildcard= 0;

  for my $test (@tests) {
    my @words= grep ! /\[/, @{$test->{pattern}};
    $havewildcard= 1 unless @words == @{$test->{pattern}};
    next unless @words;
    map { s/^\(\?\w*:\^?//; s/\$?\)$//; } @words;
    $searchstr .= "+" if defined $searchstr;
    if( @words == 1 ) {
      $searchstr .= URI::Escape::uri_escape($words[0]);
    }
    else {
      $searchstr .= "%22" . join("+", map(URI::Escape::uri_escape($_), @words)) . "%22";
    }
  }
  $$result= $searchstr // "";
  return $havewildcard;
}


# Check validity of parameter of some non-content tests (currently 
# -localfile, -modified, -header, -httpheader and -exif).  Also abused to
# check option arguments (of -transform and -downxform) by constructing a fake
# expression hash.
# -> Reference to test expression hash
#    Error handling function taking message and warning flag as arguments
sub checktestparam
{
  my ($expr, $error)= @_;
  my $testarg;

  if( $expr->{type} =~ /^(?:transform|downxform|localfile)$/ ) {
    my $err;
    transformstring("foo", $expr->{arg}, {}, \$err);
    &$error("Error in test evaluation of transform expression `$expr->{arg}':\n$err\n -")
        if $err;
    $globals{needtrace}= 1 if $expr->{arg} =~ /\b(?:trace|linkprops)\b/;
  }
  elsif( $expr->{type} eq "modified" ) {
    $testarg= $expr->{arg};
    if( $testarg =~ s/^([+-])\s*// ) {
      $expr->{subtype}= $1;
      $expr->{subtype} =~ tr/+-/-+/;
      # because we will compare seconds since epoch, not seconds up to now
    }
    else {
      $expr->{subtype}= "=";
    }
    if( $testarg =~ /^\s*(\d+)\s*([md]?)\s*$/i ) {
      $expr->{arg}= time() - ($2 && lc($2) eq "m"? $1*60 : $1*60*60*24);
    }
    else {
      $expr->{arg}= Date::Parse::str2time($testarg);
      unless( defined($expr->{arg}) ) {
        &$error("Argument `$testarg' of -modified is not of the form:\n" .
            "[ \"+\" | \"-\" ] ( <time/date> | <number> [ \"m\" | \"d\" ] )\n" .
                "specifying a modification time of at the latest, at the " .
                "earliest or exactly a certain time or date or a period of " .
                "minutes or days in the past\n -");
      }
    }
    if( $expr->{arg} > time() ) {
      &$error("Warning: The modification time  is in the future", 1);
    }
  }
  elsif( $expr->{type} =~ /^(?:(?:http)header|exif)$/ ) {
    $testarg= $expr->{arg};
    my @parts= split /:/, $expr->{arg}, 2;
    unless( @parts == 2 ) {
      &$error("Argument `$testarg' of -$expr->{type} is not of the form <test expr.>:<test expr.>");
    }
    $expr->{keyexpr}= parsetest($parts[0], $error);
    $expr->{valexpr}= parsetest($parts[1], $error);
  }
  elsif( $expr->{type} eq "urlgroup" ) {
    &$error("Unknown URL group \`$expr->{arg}'")
          unless $globals{urlgroups}{$expr->{arg}};
  }
}


my %matchtypestr= ( re => "regex", leven => "levenshtein" );
my @monthabbrev= qw( Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec );
my %cmpchars= ( "+" => ">",  "-" => "<",  "=" => "=" );

# Print a test's match expression and flags or other criterium to stdout.
# -> Reference to test expression hash
sub printmatchexpr
{
  my ($expr)= @_;
  my $type= $expr->{type} || "";

  if( $type eq "modified" ) {
    print $cmpchars{$expr->{subtype}};
    my ($s, $min, $h, $d, $m, $y)= localtime($expr->{arg});
    $y += 1900;
    map { $_= sprintf("%02d", $_) }  ($s, $min, $h);
    print " $monthabbrev[$m] $d $y  $h:$min:$s";
  }
  elsif( $type eq "size" || $type eq "rsize" ) {
    print $cmpchars{$expr->{subtype}}, " ", $expr->{arg};
  }
  elsif( $type eq "header" || $type eq "httpheader" ) {
    print "key: ";
    printmatchexpr($expr->{keyexpr});
    print "; value: ";
    printmatchexpr($expr->{valexpr});
  }
  elsif( $type eq "localfile" || $type eq "urlgroup" ) {
    $expr->{arg} =~ tr/\n/ /;
    print $expr->{arg};
  }
  elsif( $type eq "true" || $type eq "false" ) {
  }
  else {
    print $matchtypestr{$expr->{matchtype}} || $expr->{matchtype}, " ";
    if( ref($expr->{pattern}) ) {
      print join(" ", @{$expr->{pattern}});
      if( $type eq "leven" ) {
        print " ";
        print $expr->{distance} || sprintf("%.2d", $expr->{reldist});
        print "I" if $expr->{case};
      }
    }
    else {
      print $expr->{pattern};
      print " v" if $expr->{nouni2base};
    }
  }
}



{
my $pathsep;
my $curdir;

# Employ heuristics to distinguish between URL arguments and test or option
# arguments.
# -> Command-line argument
# <- 1 probably a URL, 0 probably not
sub lookslikeurl
{
  my ($arg)= @_;

  return 1 if $arg eq "-";                      # read URLs from STDIN
  return 0 if $arg =~ /^[|()!=$~-]/;            # test expression or option
  return 1 if $arg =~ m|^[a-z]{3,5}://|i;       # URL
  return 1 if $arg =~ m!^search:[a-z]+(?::|$)!i;# search engine
  return 1 if $arg =~ m|^[a-z]+:\\|i;           # Windoze drive or device
  return 0 if $arg =~ m|^/[^/]+/[Iv]?$|;        # regex test expression
  unless( defined($curdir) ) {
    my $p= File::Spec->catdir("perl", "rules");
    ($pathsep)= $p =~ m|perl(.+?)rules|i;
    $curdir= File::Spec->curdir();
  }
  if( $pathsep eq "/" ) {
    return 1 if $arg =~ m|^//|;                 # leading "//" insists on path
  }
  else {
    return 0 if $arg =~ m|^/|;                  # regex test
  }
  return 1 if $arg eq $curdir;                  # current directory
  return 1 if $arg =~ m|$pathsep|o && -e $arg;  # existing file
  return 1 if $arg =~ m|^.+$pathsep..+$|o;      # possibly path
  return 0;                                     # probably glob test
}

}


# Convert start URLs of all sections into jobs and add them to the global queue
# @jobs.  URL ranges are expanded using parseinputurl().  The stdin URL "-" is
# handled appropriately.
sub elaboratestarturls
{
  while( my ($ind, $sec) = each(@pipesecs) ) {
    next unless $sec->{starturls};
    my %havejobs;
    for my $url (@{$sec->{starturls}}) {
      if( $url eq "-" ) {
        push @{$globals{stdinsecs}}, $ind;
        next if $globals{stdin};
        $globals{stdin}= 1;
        $globals{stdintty}= -t STDIN;
        eval {
          require Fcntl;
          my $flags= fcntl(STDIN, Fcntl::F_GETFL, 0) or die;
          fcntl(STDIN, Fcntl::F_SETFL, $flags | Fcntl::O_NONBLOCK) or die;
          require Errno;
        };
        croak "Cannot set STDIN to non-blocking IO (required for \`-')."
            if $@;
      }
      else {
        my @newjobs= $url =~ /^search:/i ? makesearchjob($url, $ind) : parseinputurl($url, 1);
        next unless @newjobs;
        $url= $newjobs[0]{urlarg};
        @havejobs{map $_->{url}, @newjobs}= @newjobs;
      }
    }
    map { $_->{section}= $ind; } values(%havejobs);
    # Put jobs of later sections first.  This is intended to get the first
    # results quickly and relies on the assumption that scav scripts are
    # organised roughly "chronologically", with jobs advancing to later
    # sections.
    unshift @jobs, values(%havejobs);
  }
}


# Read URLs from STDIN if available and parse them to job hashes.  The URLs may
# contain glob patterns (both for files and glob-like patterns for other URLs).
# If an error or end of file is detected at STDIN, a flag is set to stop
# reading from STDIN.
# <- List of new job hashes
sub getstdinurls
{
  my @newjobs;
  my %uniqjobs;

  while( defined(my $stdinurl= <STDIN>) ) {
    $stdinurl =~ s/^\s+//;
    $stdinurl =~ s/\s+$//;
    @newjobs= parseinputurl($stdinurl);
    @uniqjobs{map $_->{url}, @newjobs}= @newjobs;
  }
  delete $globals{stdin}      # stop reading on error or eof
      unless $! == Errno::EAGAIN() || $! == Errno::EWOULDBLOCK();
  $globals{numstartjobs} += values(%uniqjobs) * @{$globals{stdinsecs}};
  @newjobs= ();
  for my $job (values %uniqjobs) {
    $job->{weight}= 1.0/($globals{numstartjobs} + @newjobs);
    $job->{gdtl}= $globalopts{maxdepth};
    delete $job->{urlarg};
    for my $sec (@{$globals{stdinsecs}}) {
      push @newjobs, dup($job);
      $newjobs[-1]{section}= $sec;
      $newjobs[-1]{dtl}= $pipesecs[$sec]{options}{maxdepth};
    }
  }
  return @newjobs;
}


# Parse a URL given on the command line or piped into STDIN.  Glob patterns are
# expanded.  URLs from STDIN are regarded as user input (for which robots.txt
# is ignored) iff STDIN is a tty.  Croaks on errors only for command-line
# arguments.
# -> URL argument (file glob patterns or glob-like patterns for URLs allowed)
#    Flag indicating the URL is from the command line
# <- List of job hash references
sub parseinputurl
{
  my ($url, $cmdarg)= @_;
  my $urlarg= $url;
  my %newjobs;
  my $urltype;
  my $errmsg;

  print STDERR "Read URL arg $urlarg\n" if $globals{debug};
  if( $url =~ s/^file://i ) {
    $url= URI::file->new($url)->file();
    $urltype= "file";
  }
  elsif( $url =~ /^$PROTOCOLS:\/\//o ) {
    $urltype= "url";
  }
  elsif( $url =~ /^[a-z]+:\/\//i ) {
    $errmsg= "wfind: Unknown scheme in URL `$urlarg'.";
    if( $cmdarg ) { croak "$errmsg  Aborting."; }
    else { fmt *STDERR{IO}, "$errmsg\n"; return (); }
  }
  elsif( $url =~ /^www\.[a-z0-9.]+\.[a-z]{2,3}(?:\/.*)?$/i ) {
    $url= "http://$url";
    $urltype= "url";
  }
  elsif( $url =~ /^ftp\.[a-z0-9.]+\.[a-z]{2,3}(?:\/.*)?$/i ) {
    $url= "ftp://$url";
    $urltype= "url";
  }
  else {
    $urltype= "file";
  }
  if( $urltype eq "file" ) {
    $!= 0;
    my @files= File::Glob::bsd_glob($url);
    if( !@files ) {
      fmt *STDERR{IO}, "wfind: Warning: URL argument `$urlarg' (file glob) refers to no existing files or directories.\n" unless $globalopts{silent};
    }
    elsif( @files == 1 && $url eq $files[0] ) {
      if( !-e $url ) {
        $errmsg= "wfind: File `$urlarg' does not exist.";
      }
      elsif( !-r $url ) {
        $errmsg= "wfind: Do not have read permission on file `$urlarg'.";
      }
      if( $errmsg ) {
        if( $cmdarg ) { croak "$errmsg  Aborting."; }
        else { fmt *STDERR{IO}, "$errmsg\n"; return (); }
      }
    }
    for my $f (@files) {
      if( -l $f ) {
        my $target= tracesymlink($f);
        unless( defined $target ) {
          fmt *STDERR{IO}, "wfind: Warning: Could not follow symbolic link `$f'.\n" unless $globalopts{silent};
          next;
        }
        $f= $target;
      }
      my $job= { url => URI::file->new_abs($f)->as_string(),
                 type => -d $f ? "dir" : "file", path => $f,
                 cachefile => cachefilename(),
                 userinput => $cmdarg || $globals{stdintty},
                 starturl => 1, section => 0, urlarg => \$urlarg };
      $newjobs{$job->{url}}= $job;
    }
  }
  else {
    for my $u (urlglob($url, $cmdarg)) {
      $u= canonuri($u);
      $newjobs{$u}= { url => $u, type => "url", section => 0,
                       cachefile => cachefilename(),
                       userinput => $cmdarg || $globals{stdintty},
                       starturl => 1, urlarg => \$urlarg };
    }
  }
  return values %newjobs;
}


# Create URL jobs submitting search engine forms from "search:..." pseudo-URLs
# that wfind / scav accepts.  "search:" has to be followed by the name of the
# search engine and optionally after a second colon by the number of result
# pages to request or the search terms.
# -> Search pseudo-URL
#    Index of section for which it is a start URL
# (<-) The generated URL jobs will be added to the start jobs of the section

my %engines= (
dogpile => [ "http://www.dogpile.com/search/web?q=", "&qsi=%O" ],
ask => [ "http://www.ask.com/web?q=", "&page=%P" ],
bing => [ "http://www.bing.com/search?q=", "&first=%O" ],
clusty => [ "http://clusty.com/search?query=", "&v:state=root|root-%o-10|0" ],
exalead => [ "http://www.exalead.com/search/web/results/?q=", "&start_index=%o" ],
duckduckgo => [ "https://duckduckgo.com/html?q=", undef ],
scholar => [ "http://scholar.google.com/scholar?as_vis=1&as_sdt=1&q=", "&start=%o" ],
google => [],
);

sub makesearchjob
{
  my ($searchurl, $section)= @_;

  my (undef, $engname, $arg)= split /:/, $searchurl, 3;
  $engname= lc $engname;
  my @candidates= grep substr(lc $_,0,length $engname) eq $engname, keys %engines;
  unless( @candidates == 1 ) {
    croak "Unknown search engine in search URL \`$searchurl'.  Available search engiens are ",
          cswordlist(" and ", keys %engines), ".  Aborting."
        unless @candidates;
    croak "Ambiguous search engine in search URL \`$searchurl'.  Could be ",
          cswordlist(" or ", @candidates), ".  Aborting.";
  }
  my $engine= $engines{$engname= $candidates[0]};
  croak "The search engine `$engname' does not allow automated queries."
      unless $engine->[0];
  my $pages= $arg if defined($arg) && $arg =~ /^\d+$/;
  my $searchstr;
  if( defined($arg) && $arg !~ /^\d+$/ ) {
    $searchstr= join("+", map(URI::Escape::uri_escape($_), split /\s+/, $arg));
  }
  else {
    my ($filtact)= grep $_->{type} eq "filter", @{$pipesecs[$section]{actions}};
    if( !$filtact && $globals{scav} ) {
      croak "Cannot infer search terms without filter action in the same section as search engine URL:\n$searchurl\nAborting.";
    }
    my $topexpr= $globals{scav} ? $filtact->{testexpr} : $pipesecs[0]{testexpr};
    my @globs;
    if( $topexpr->{type} eq "intext" && $topexpr->{matchtype} eq "glob" ) {
      @globs= ($topexpr);
    }
    elsif( $topexpr->{type} eq "and" ) {
      @globs= grep $_->{type} eq "intext" && $_->{matchtype} eq "glob",
              @{$topexpr->{subexprs}};
    }
    else {
      croak "Could not infer terms for search engine URL because the subsequent filter action did not have any top-level glob tests:\n$searchurl\nAborting.";
    }
    my $status= globtest2searchstring(\$searchstr, @globs);
    croak "Could not infer terms for search engine URL because the glob tests of the subsequent filter action did not contain any plain words:\n$searchurl\nAborting."
        unless length $searchstr;
    fmt *STDERR{IO}, "Warning: glob test expressions containing wildcards were discarded when infering terms for search engine URL:\n$searchurl\n"
        unless $status == 0;
  }
  my $baseurl= $engine->[0] . $searchstr;
  if( defined $pages && ! $engine->[1] ) {
    fmt *STDERR{IO}, "Warning: Search engine `$engname' does not support result page offsets, so only one result page can be obtained.  Consider using recursion with an approriate linke text such as `next'.\n";
  }
  my @urls= ( $baseurl );
  if( $engine->[1] ) {
    $pages ||= 5;
    for my $page (1 .. $pages - 1) {
      my $suffix= $engine->[1];
      $suffix =~ s/%o/10 * $page/e;
      $suffix =~ s/%O/10 * $page + 1/e;
      $suffix =~ s/%P/$page + 1/e;
      push @urls, "$baseurl$suffix";
    }
  }
  return map +{ url => $_, type => "url", section => 0, userinput => 1,
                cachefile => cachefilename(),
                starturl => 1, urlarg => \$searchurl }, @urls;
}


# Human-readable string with all search engine names allowed after search:
# pseudo-scheme, for -status.
sub allsearchengines
{
  return cswordlist(" and ", sort keys %engines);
}


# Elaborate glob-like patterns of URLs, analogously to curl.  Croaks on errors
# only if URL was given on the command line.
# -> Glob-like pattern
#    Flag indicating the URL is from the command line
# <- List of URLs
sub urlglob
{
  my ($pattern, $cmdarg)= @_;
  my $errmsg;

  my $baltest= $pattern;
  $baltest =~ s/\\[][{}]//g;
  $baltest =~ s/(?:\[[^][{}]*\]|\{[^][{}]*\})//g;
  $baltest =~ s/[^][{}]//g;
  if( $baltest ) {
    $errmsg= "Unmatched or nested brackets or braces in URL pattern \`$pattern'.";
    if( $cmdarg ) { croak "$errmsg  Aborting."; }
    else { fmt *STDERR{IO}, "$errmsg\n"; return (); }
  }
  my $pat2= $pattern;
  $pat2 =~ tr/[]{}/\001\002\003\004/;
  $pat2 =~ s/\\\001/\[/g;
  $pat2 =~ s/\\\002/\]/g;
  $pat2 =~ s/\\\003/\{/g;
  $pat2 =~ s/\\\004/\}/g;
  my @parts= split /(?=[\001\003])|(?<=[\002\004])/, $pat2;
  my @urls= ( "" );
  for my $part (@parts) {
    if( $part =~ /^\001/ ) {
      my @newparts= urlglobrange($part);
      unless( @newparts ) {
        $part =~ tr/\001\002/\[\]/;
        $errmsg= "Error parsing range $part in URL pattern \`$pattern'.";
        if( $cmdarg ) { croak "$errmsg  Aborting."; }
        else { fmt *STDERR{IO}, "$errmsg\n"; return (); }
      }
      my @newurls;
      map { $a= $_; push @newurls, map $a.$_, @newparts; } @urls;
      @urls= @newurls;
    }
    elsif( $part =~ /^\003/ ) {
      $part =~ s/^\003//;
      $part =~ s/\004$//;
      my @newparts= split /,/, $part;
      next unless @newparts;            # ignore {}
      my %uniqparts= map +($_, 1), @newparts;
      @newparts= keys(%uniqparts);
      my @newurls;
      map { $a= $_; push @newurls, map $a.$_, @newparts; } @urls;
      @urls= @newurls;
    }
    else {
      for (@urls) { $_ .= $part; }
    }
  }
  return @urls;
}


# Converts a range from a URL glob-like pattern to an array containing all its
# elements.
# -> Range delimited by \001 and \002 instead of [ and ]
# <- Array of elements in the range
sub urlglobrange
{
  my ($glob)= @_;
  my @range;

  my $ok= $glob =~ /^\001([a-z0-9]+)-([a-z0-9]+)(?::(\d+))?\002$/;
  my ($from, $to, $step)= ($1, $2, $3 || 1);
  $ok= $ok && (($from =~ /^\d+$/ && $to =~ /^\d+$/) ||
   (length($from) == length($to) && $from =~ /^[a-z]+$/ && $to =~ /^[a-z]+$/));
  unless( $ok ) {
    print STDERR "Incorrect format for range: $glob\n",
  "The range format is: \"[\" <from> \"-\" <to> [ \":\" <step> ] \"]\",\n",
  "where <from> and <to> have to be either both numerical or both strings\n",
  "of letters of equal length, and the optional <step> has to be numerical.\n";
    return ();
  }
  if( $from =~ /^\d/ ) {
    my $width= length($from);
    if( $from > $to ) {
      print STDERR "The smaller number has to be given first in a range.\n";
      return ();
    }
    my $format= "%0${width}d";
    @range= map sprintf($format, $from+$step*$_), (0 .. int(($to-$from)/$step));
  }
  else {
    if( $from gt $to ) {
      print STDERR "The lexicographically smaller string has to be given first in a range.\n";
      return ();
    }
    my $skip= 1;
    for my $elem ($from .. $to) {
      next if --$skip;
      $skip= $step;
      push @range, $elem;
    }
  }
  return @range;
}


# Simplify the expression tree by eliminating single-operand "and" and "or"
# expressions and combining associative expressions.  Set "require" field so
# we can treat "and" and "or"clauses as a special case of m-out-of-n
# requirements.
# -> Reference to scalar variable containing reference to top-level expression
#    hash.  (This is necessary because the top-level expression may have to be
#    changed if for instance it is an "and" with just one operand.)
#    String saying where the expression tree comes from (number of pipe
#    section, in -while or not)
sub simplifyexprs
{
  my ($topref, $where)= @_;
  my $exprref= $topref;  # reference to variable referencing current expression
  my $expr= $$topref;
  my $subind= 0;
  my @exprstack;
  my @indexstack;
  my $nokillwarn= $globalopts{silent};

  if( ! $expr->{subexprs} || ! @{$expr->{subexprs}} ) {
    if( $expr->{type} eq "and" ) {
      $$topref= dup($trueexpr);
    }
    elsif( $expr->{type} eq "or" ) {
      croak "wfind: Error: -or without arguments $where.\n";
    }
    return;
  }
  while( 13 )
  {
    # Descend so that sub-subexpressions are processed before subexpressions
    while( $subind < @{$expr->{subexprs}} ) {
      if( $expr->{subexprs}[$subind]{subexprs} ) {
        push @exprstack, $expr;
        push @indexstack, $subind;
        $exprref= \$expr->{subexprs}[$subind];
        $expr= $$exprref;
        $subind= 0;
      }
      else {
        ++$subind;
      }
    }
    if( $expr->{type} eq "and" || $expr->{type} eq "or" ) {
      my $isand= $expr->{type} eq "and";
      my $totalsubs= @{$expr->{subexprs}};
      if( defined($expr->{options}{require}) &&
          ($expr->{options}{require} <= 0 ||
           $expr->{options}{require} >= $totalsubs) ) {
        croak "wfind: Error: The number of -require'd subexpressions must be ".
                "between 1 and the total ($totalsubs), $where.";
      }
      $expr->{require}= $expr->{options}{require} ||
                        ($isand? $totalsubs : 1);
      my $killed;
      my @newsubs;
      for my $subex (@{$expr->{subexprs}}) {
        if( $subex->{type} eq "true" ) {
          if( --$expr->{require} == 0 ) {
            $killed= $subex;
            last;
          }
        }
        elsif( $subex->{type} eq "false" ) {
          --$totalsubs;
          if( $expr->{require} > $totalsubs ) {
            $killed= $subex;
            last;
          }
        }
        elsif( $subex->{type} eq $expr->{type} &&
                !defined($expr->{options}{require}) &&
                keys(%{$subex->{options}}) == 0 ) {
          push @newsubs, @{$subex->{subexprs}};
          $expr->{require} += @{$subex->{subexprs}}-1 if $isand;
          $totalsubs += @{$subex->{subexprs}}-1;
        }
        else {
          push @newsubs, $subex;
        }
      }
      if( ref($killed) ) {
        $$exprref= $killed;     # need not copy options for constant expr
        fmt *STDERR{IO}, "wfind: Warning: An -and or -or expression could " .
        "be decided statically because of a constant -false or -true $where. ".
        " You may want to review your expressions.\n"
                unless $nokillwarn;
        $nokillwarn= 1;
      }
      else {
        if( @newsubs > 1 ) {
          $expr->{subexprs}= \@newsubs;
        }
        elsif( @newsubs == 1 ) {
          my $superopts= $expr->{options};
          $expr= $$exprref= $newsubs[0];
          # Subexpression options override superexpression options:
          @$superopts{keys(%{$expr->{options}})}=
                values(%{$expr->{options}})
                        if keys(%{$expr->{options}});
          $expr->{options}= $superopts;
        }
        else {
          # This should only happen if there are no tests at all
          die "Zero subexpressions without kill subexpression in simplifyexprs"
                unless $expr == $$topref;
          $$exprref= dup($trueexpr);
        }
      }
    }
    $expr= pop @exprstack or return;
    $subind= pop(@indexstack) + 1;
    $exprref= @exprstack? \$exprstack[-1]{subexprs}[$indexstack[-1]] :
                                $topref;
  }
}


# Convert array of textual notation of link tag/attribute validity lists
# to hash of hashsets for efficient testing.  Unique attributes of link tags
# are automatically added where needed, and an "all" entry is discarded if
# present.
# -> Reference to array of link tags/attributes in the form "tag.attr"
# <- Reference to hash (with tag name keys) of hash sets (with attribute name
#    keys)
sub linktaghash
{
  my ($stringary)= @_;
  my %tagattrset;

  for my $item (@$stringary) {
    next if $item eq "all";
    my ($tag, $attr)= split /\./, $item;
    # Default attribute if unique; uniqueness enforced by allowed word list
    $attr ||= ( keys %{$alllinktags{$tag}} )[0];
    $tagattrset{$tag}{$attr}= 1;
  }
  return \%tagattrset;
}


# Preprocess proximity requirements.  Proximity requirements (from -wordnear or
# -charnear) are propagated down the expression tree.  If subexpressions have
# more relaxed explicit proximity requirements than their superexpressions, a
# warning is issued (though only once).  On the way back from the tree search,
# the maximum word and char context requirement is determined.
# -> Reference to expression hash
#    String saying where the expression tree comes from (number of pipe
#    section, in -while or not)
#    -wordnear requirement of superexpression (or undef)
#    -charnear requirement of superexpression (or undef)
#    Flag indicating that proximity mismatch warnings should not be issued
#    (again)
# <- List of maximal -wordnear and -charnear requirement plus one
sub preprocessprox
{
  my ($test, $where, $wnear, $cnear, $nowarn)= @_;

  $nowarn= $globalopts{silent} unless defined($nowarn);
  return (0, 0) unless $test->{class} eq "op";
  if( defined($wnear) ) {
    if( !defined($test->{options}{wordnear}) ) {
      $test->{options}{wordnear}= $wnear;
    }
    elsif( $test->{options}{wordnear} > $wnear ) {
      fmt *STDERR{IO}, "wfind: Warning: Proximity requirement of subexpression " .
      "cannot be less restrictive than that of superexpression, $where. ".
      " Restricting subexpression proximity to match superexpression.\n"
              unless $nowarn;
      $nowarn= 1;
      $test->{options}{wordnear}= $wnear;
    }
  }
  if( defined($cnear) ) {
    if( !defined($test->{options}{charnear}) ) {
      $test->{options}{charnear}= $cnear;
    }
    elsif( $test->{options}{charnear} > $cnear ) {
      fmt *STDERR{IO}, "wfind: Warning: Proximity requirement of subexpression " .
      "cannot be less restrictive than that of superexpression, $where.  ".
      "Restricting subexpression proximity to match superexpression.\n"
              unless $nowarn;
      $nowarn= 1;
      $test->{options}{charnear}= $cnear;
    }
  }
  $wnear= $test->{options}{wordnear};
  $cnear= $test->{options}{charnear};
  my $maxwnear= 0;
  my $maxcnear= 0;
  for my $subex (@{$test->{subexprs}}) {
    ($maxwnear, $maxcnear)=
        @{ zipwith(\&max2, [$maxwnear, $maxcnear],
                [preprocessprox($subex, $where, $wnear, $cnear, $nowarn)]) };
  }
  return ($maxwnear+1, $maxcnear+1);
}


# Recursively add the elementary expressions in a subtree to their section's
# lists of utests, htests and ctests (URL, header and content tests).
# -> Reference to pipe section hash or other hash to store lists in
#    References to expression hashes
sub addtotestlists
{
  my ($dest, @exprs)= @_;

  for my $expr (@exprs) {
    if( $expr->{class} eq "op" ) {
      addtotestlists($dest, @{$expr->{subexprs}});
    }
    else {
      push @{$dest->{$expr->{class}}}, $expr;
    }
  }
}


# Build up lists of URL, header and content tests of a pipe section.  A
# separate list of content tests is built that includes -linktext and link
# position tests, as this is needed for different purposes than the list from
# action test expressions.
# -> Reference to pipe section hash
sub buildtestlists
{
  my ($sec)= @_;

  addtotestlists($sec, grep(defined, map $_->{testexpr}, @{$sec->{actions}}));
  # -linktext, -linkafter and -linkbefore tests are listed separately for the
  # benefit of servercheck(), as they are only performed when the document
  # needs to be retrieved for other reasons.
  my %transienttestlists= ( ctests => [] );
  addtotestlists(\%transienttestlists, grep(defined,
      map { @$_{keys %treeopts}; } @{$sec->{actions}} ) );
  $sec->{allctests}= [ @{$sec->{ctests}}, @{$transienttestlists{ctests}} ];
}


# Return the maximum number of words of a list of word-related in-text tests.
# Other tests may be part of the list and will be ignored.
# -> List of references to test expression hashes
# <- Maximum word count
sub maxtestwords
{
  my $need= 0;

  for (@_) {
    next unless $_->{type} eq "intext" && ref($_->{pattern}) eq "ARRAY";
    $need= @{$_->{pattern}} if $need < @{$_->{pattern}};
  }
  return $need;
}


my %sensibleactopts;

my $searchenginedomain= qr/google|clusty|yippy|ask|search\.yahoo|live|alltheweb|
                                bing|search\.msn|dogpile|altavista|baidu/ix;

# Parse wfind command line arguments.  The initial arguments have to be the
# URLs to start searching at.  "--" forcibly concludes the URL list.  The
# following arguments are interpreted as part of the search expression.  Each
# pipe section's options and expressions are translated to scav actions.
# -> Reference to array of command line arguments.
# (<-) Various global variables are set according to the command line.
sub parsecmdline
{
  my ($args)= @_;

  my @arggroups= arysplit { $_[0] eq "--" } @$args;
  if( @arggroups > 2 ) {
    croak "Error: Separator \`--' must occur only once.  Aborting.";
  }
  if( @arggroups == 2 ) {            # obey "--" separator if present
    $pipesecs[0]{starturls}= $arggroups[0];
    @$args= @{ $arggroups[1] };
  }
  else {
    push @{$pipesecs[0]{starturls}}, shift(@$args)
        while @$args && lookslikeurl($$args[0]);
  }
  parseopts($args, \@pipesecs);
  elaboratestarturls();
  if( !@jobs && !$globals{stdin} && !$globalopts{version} && !$globalopts{help} &&
                !$globalopts{echo} && !$globalopts{scavecho} && !$globalopts{status} ) {
    print STDERR "Nothing to do - no start URLs given.\n";
    printhelp();
  }
  if( !defined($pipesecs[0]{options}{samedomain}) ) {
    my $searchengineurls= grep getdomain($_->{url}) =~ /\b$searchenginedomain\b/, @jobs;
    $pipesecs[0]{options}{samedomain}= $searchengineurls <= (@jobs-1)/2;
  }
  while( my ($secnr, $sec) = each @pipesecs ) {
    my $acthash= { type => "recurse", options => { %{$sec->{options}} },
      target => $secnr, testexpr => $sec->{while}{subexprs}[0],
      where => "in -while in pipe section $secnr" };
    delete @{$acthash->{options}}{grep !$sensibleactopts{all}{$_} &&
         !$sensibleactopts{$acthash->{type}}{$_}, keys(%{$acthash->{options}})};
    $acthash->{testexpr}= $sec->{while}{subexprs}[0] // dup($trueexpr);
    $acthash->{$_}= $sec->{$_}{subexprs}[0]
        for grep defined($sec->{$_}), keys(%treeopts);
        # currently all subtree options relate to link following
    push @{$sec->{actions}}, $acthash;
    $acthash= { type => "filter", testexpr => $sec->{testexpr},
        options => { %{$sec->{options}} }, where => "in pipe section $secnr" };
    delete @{$acthash->{options}}{grep !$sensibleactopts{all}{$_} &&
        !$sensibleactopts{$acthash->{type}}{$_}, keys(%{$acthash->{options}})};
    push @{$sec->{actions}}, $acthash;
    $acthash= { type => ($secnr == $#pipesecs ? "output" : "feed"),
              options => { %{$sec->{options}} }, target => $secnr + 1,
              testexpr => dup($trueexpr) };
    delete @{$acthash->{options}}{grep !$sensibleactopts{all}{$_} &&
         !$sensibleactopts{$acthash->{type}}{$_}, keys(%{$acthash->{options}})};
    push @{$sec->{actions}}, $acthash;
    delete @$sec{qw(testexpr while), keys(%treeopts)};
  }
  if( $pipesecs[-1]{options}{download} ||
      $pipesecs[-1]{options}{decompress} ||
      $pipesecs[-1]{options}{downxform} ) {
    $pipesecs[-1]{options}{download}= 1;
    $pipesecs[-1]{options}{decompress} //= 1;
    push @{$pipesecs[-1]{actions}}, { type => "download",
       options => $pipesecs[-1]{options}, testexpr => dup($trueexpr) };
  }
  cleanup_init();
}


# Calls functions to simplify test expressions, classify tests according to
# "class", determine amount of context required when parsing documents, convert
# link tag/attribute lists to hashes.  Also makes depths consistent,
# initialises depths and weights of jobs and does some other consistency
# checks and initialisations.  This function is common to wfind and scav.
# (<->) Global variables
sub cleanup_init
{
  my $havemimetest;

  $globals{fork}= 0 unless $globalopts{slaves} > 0;
  unless( defined($globalopts{maxdepth}) ) {
    $globalopts{maxdepth}= 0;
    for my $sec (@pipesecs) {
      $globalopts{maxdepth} +=
            $sec->{options}{maxdepth} // $globals{defpdepth};
    }
    $globalopts{maxdepth} ||= $globals{defgdepth};
  }
  while( my ($secnr, $sec) = each @pipesecs ) {
    my %secfwlinktags;
    if( $sec->{options}{inline} ) {
      my @fwtaglist= grep $fwlinktags{$_}, keys %{$sec->{options}{follow}};
      %secfwlinktags= map { $_ => 1; } @fwtaglist;
      $sec->{options}{forward}= linktaghash(\@fwtaglist);
    }
    for my $act (@{$sec->{actions}}) {
      simplifyexprs(\$act->{testexpr}, $act->{where});
      for my $o (grep defined($act->{$_}), keys(%treeopts)) {
        simplifyexprs(\$act->{$o}, " in -$o" . $act->{where} );
      }
      # Create shadow expression trees for link location tests - these are not
      # pruned during evaluation, so they need to be created only once.
      $act->{linkaftershadow}= shadowtests($act->{linkafter})
          if $act->{linkafter};
      $act->{linkbeforeshadow}= shadowtests($act->{linkbefore})
          if $act->{linkbefore};
      $act->{transientshadows}= [];
      listtransshadows($act->{transientshadows}, grep defined, @$act{qw(linkaftershadow linkbeforeshadow)});
      $act->{options}{follow}= linktaghash( [ grep ! $secfwlinktags{$_}, keys %{$act->{options}{follow}} ] );
      $globals{needtrace}= 1
          if $act->{type} eq "output" && ($act->{options}{print}{trace}
                              || $act->{options}{print}{linkprops});
      $sec->{haveaction}{$act->{type}}= 1;
      preprocessprox($act->{testexpr}, $act->{where});
    }
    $sec->{options}{maxdepth} //= $globals{defpdepth};
    $sec->{options}{maxdepth}= $globalopts{maxdepth}
        if $sec->{options}{maxdepth} > $globalopts{maxdepth};
    $sec->{resultcount}= 0 if $sec->{options}{maxresults};
    buildtestlists($sec);
    $sec->{needwords}= maxtestwords(@{$sec->{allctests}});
    $sec->{linkstos}= [ grep $_->{type} eq "linksto", @{$sec->{ctests}} ];
    for my $lt (@{$sec->{linkstos}}) {
      $lt->{options}{linktags}= linktaghash( [ keys %{$lt->{options}{linktags}} ] );
    }
    $havemimetest= 1 if $havemimetest ||
        shortgrep( sub { $_->{type} eq "type" }, @{$sec->{htests}} ) ||
        shortgrep( sub { $_->{type} eq "rtype" }, @{$sec->{ctests}} );
  }
  $globals{numstartjobs}= scalar(@jobs);
  for (@jobs) {
    $_->{weight}= 1.0/$globals{numstartjobs};
    $_->{dtl}= $pipesecs[$_->{section}]{options}{maxdepth};
    $_->{gdtl}= $globalopts{maxdepth};
  }
  $globalopts{silent}= 0 if $globalopts{verbose};
  File::MimeInfo::Magic::rehash() if $globals{mimeinfomagic} && $havemimetest;
}


#sectionscript################################################################
##############################################################################
####          Script parsing and creation of data structures from scripts
##############################################################################
##############################################################################

# Script arguments come in two kinds, URL args and other (general) args
# URL args are referred to by @0, @1, ... or @3@ (etc) meaning @3 and
# following.  @@ is the same as @0@.
# General args are referred to by $0, $1, ... or %0, %1, ... to have them URL
# encoded.  $2@ are all args from $2, space separated; %2@ are joined by "+".
# $@ is $0@, %@ is %0@.

# Substitute general script arguments in a string.  (URL arguments are not
# substituted in text and are handled in parsescript().)
# -> Original string
#    Reference to subroutine that returns a ref to an array of the general
#    arguments of the script when called with argument 1
#    Reference to hash for out-of-range general argument indices
sub substargs
{
  my ($str, $getargs, $oor)= @_;

  my @marks= $str =~ /[\$%](?:(\d+)\@?|\@)/g;
  return $str unless @marks;
  my $args= &$getargs(1);
  @marks= grep( /\d+/ && $_ > $#$args, @marks );
  @$oor{@marks}= (1) x @marks;
  return $str if @marks;
  $str =~ s/([\$%])(?:(\d+)(\@)?|(\@))/
            $3 || $4 ? ($1 eq "\$"? join(" ", @$args[($2||0)..$#$args]) :
    join("+",  map(URI::Escape::uri_escape($_), @$args[($2||0)..$#$args]))) : 
            $1 eq "\$"? $$args[$2] : URI::Escape::uri_escape($$args[$2])
            /eg;
  return $str;
}


# Create a new section.  A warning is output if the previous section has no
# actions other than "recurse", because then its URLs are going nowhere.
# -> Reference to array of section hashes
#    Reference to scalar containing current subsection type (urls, options or
#    actions)
#    Script file name (for error messages)
sub newsection
{
  my ($sections, $subsec, $fname)= @_;

  if( ! $sections->[-1]{actions} ||
      ! shortgrep { $_->{type} ne "recurse" } @{$sections->[-1]{actions}} ) {
    fmt *STDERR{IO}, "Warning: No non-recursive action in section $#$sections before line $. in $fname.\n";
  }
  push @$sections, { options => { %{$sections->[-1]{options}} },
                     utests => [], htests => [], ctests => [] };
  $$subsec= "urls";
}


my @scriptcmds= qw( nogargs nouargs 
                    filter sort head tail
                    recurse follow feed output download );
# test expressions as args to those ^
# filter, sort, head, tail affect following recurse, follow, feed etc.

# my %sensibleactopts;
# defined above because wfind also uses it to keep option hashes lean

BEGIN {
  %sensibleactopts= ( all => [qw(pessimistic)],
    filter => [qw(nostart)], transform => [qw(transform maxresults)],
    feed => [qw(transform maxresults)],
    follow => [qw(follow symlinks unrestricted samedomain plainurls)],
    recurse => [qw(follow symlinks unrestricted samedomain plainurls)],
    output => [qw(print maxresults)],
    download => [qw(decompress downxform maxresults)] );
  for my $k (keys %sensibleactopts) {
    $sensibleactopts{$k}= { map( { $_ => 1; } @{$sensibleactopts{$k}} ) };
  }
}

# Regex of actions that follow links:
my $FOLLOWACTRE= qr/^(?:follow|recurse)$/;

# Parse a scavenger script.
# -> Script file name
#    Reference to array of command-line arguments
#    Reference to option hash to be taken as default for the first section
# <- List of processing sections of the script
sub parsescript
{
  my ($fname, $args, $defopts)= @_;
  my $handle;
  my $line;
  my @sections;
  my %labels;
  my $recurseto= 0;
  my ($getargs, $splitcache, $argused, $noargopt, %uoor, %oor);
  my $subsection= "urls";
  my $parseerr;

  @sections= ( dup($secdefault) );
  $sections[0]{options}= { %$defopts };
  $getargs= sub { splitscavargs($args, \$splitcache, $_[0]); };
  open $handle, "<$fname" or return ();
scriptline:
  while( defined($line= <$handle>) ) {
    $line =~ s/^\s+//;
    $line =~ s/^#.*//;
    $line =~ s/^([^'"#]*(?:(?:"[^"]*"|'[^']*')[^'"#]*)*)\s\#.*$/$1/;
    $line =~ s/\s+$//;
    my @quoted;
    while( $line =~ s/('[^']*'|"[^"]*")/\x01/ ) {
      my $q= $1;
      if( $q =~ /^"/ ) {
        $q= substargs($q, $getargs, \%oor);
      }
      $q =~ s/^['"]//;
      $q =~ s/['"]$//;
      push @quoted, $q;
    }
    $line= substargs($line, $getargs, \%oor);
    $argused //= $. if $splitcache;
    my $linenr= $.;
    my @words= split /\s+/, $line;
    my %heredocs;
    for my $wd (@words) {
      if( $wd =~ /^<<(\w+)$/ ) {    # here document
        my $endmark= $1;
        $wd= $heredocs{$endmark}, next if $heredocs{$endmark};
        $wd= "";
        my $herelinenr= $.;
        my $hereline;
        $wd .= $hereline
            while defined($hereline= <$handle>) && $hereline !~ m!^$endmark$/$!;
        unless( defined $hereline ) {
          fmt *STDERR{IO}, "Error: Unterminated here document \"$endmark\" starting after line $herelinenr in $fname.";
          $parseerr= 1;
          last scriptline;
        }
        $heredocs{$endmark}= $wd;
      }
      else {
        $wd =~ s/\x01/shift @quoted/eg;
      }
    }
    next unless @words;
    $line= join " ", @words;
    if( $line =~ m!^(?:\w+://|search:\w|-(?:\s|$))!i ) {           # explicit URL or stdin
      newsection(\@sections, \$subsection, $fname) unless $subsection eq "urls";
      for my $url (@words) {
        if( $url eq "-" || $url =~ /^($PROTOCOLS|search):/io ) {
          push @{$sections[-1]{starturls}}, $url;
        }
        else {
          fmt *STDERR{IO}, "Warning: Unknown scheme in URL \`$url' in $fname, line $linenr.  Ignored.\n";
        }
      }
      next;
    }
    if( $line =~ /^\@(?:\@|\d+\@?)(?:\s|$)/ ) {       # URL from command line
      newsection(\@sections, \$subsection, $fname) unless $subsection eq "urls";
      my @argnrs= $line =~ /^(?:\@(\@|\d+\@?)\s*(?:\s|$))+$/;
      unless( @argnrs ) {
        fmt *STDERR{IO}, "Error: Cannot combine command-line URLs with explicit URLs on the same line in $fname, line $linenr.\n";
        $parseerr= 1;
      }
      my $uargs= &$getargs(0);
      $argused //= $linenr;
      for( @argnrs ) {
        my ($argnr)= /(\d+)/;
        $argnr ||= 0;
        if( $argnr > @$uargs ) {
          $uoor{$argnr}= 1;
          next;
        }
        if( /\@/ ) {
          push @{$sections[-1]{starturls}}, @$uargs[$argnr..$#$uargs];
        }
        else {
          push @{$sections[-1]{starturls}}, $$uargs[$argnr];
        }
      }
      next;
    }
    if( $line =~ /^(\w*):$/ ) {           # new section with/without label
      my $label= $1;
      newsection(\@sections, \$subsection, $fname)
          unless $#sections == 0 && $subsection eq "urls" &&
          (!$sections[-1]{starturls} || !@{$sections[-1]{starturls}});
      if( $label ) {
        push @{$labels{$label}}, { section => $#sections, scriptline => $linenr };
        $sections[-1]{name}= $label;
      }
      $recurseto= $#sections;
    }
    elsif( $words[0] =~ /^-\w+/ ) {        # option
      $subsection= "options";
      eval { parseopts(\@words, \@sections, "o", "in $fname, line $linenr") };
      $parseerr= 1 if $@;
      if( !$noargopt && ($globalopts{nogargs} || $globalopts{nouargs}) )  {
        if( $argused ) {
          fmt *STDERR{IO}, "Error: Options -nouargs and -nogargs (on line $linenr) must precede first argument usage (line $argused) in $fname.\n";
          $parseerr= 1;
        }
        $noargopt= 1;
      }
    }
    elsif( $words[0] =~ /^\w+$/ ) { # action
      my $action= lc(shift @words);
      my %acthash= ( type => $action, options => { %{$sections[-1]{options}} },
                     where => "in \`$action' on line $linenr in $fname" );
      unless( $action =~ /^(?:follow|feed|recurse|filter|transform|output|download)$/ ) {
        fmt *STDERR{IO}, "Error: Unknown action \`$action' in $fname, line $linenr.\n";
        $parseerr= 1;
        next;
      }
      $subsection= "actions";
      my $parseoptmode= "a";
      if( $action eq "feed" ) {
        unless( @words && $words[0] =~ /^\w+$/ ) {
          fmt *STDERR{IO}, "Error: Action \`feed' must be immediately followed by a target section name in $fname, line $linenr.\n";
          $parseerr= 1;
          next;
        }
        $acthash{targetname}= shift @words;
        $acthash{scriptline}= $linenr;
      }
      elsif( $action eq "transform" ) {
        $acthash{type}= "feed";
        unshift @words, "-transform";
      }
      elsif( $action eq "recurse" ) {
        $acthash{target}= $recurseto;
        $parseoptmode= "f";
      }
      elsif( $action eq "follow" ) {
        $parseoptmode= "f";
      }
      delete @{$acthash{options}}{grep !$sensibleactopts{all}{$_} &&
               !$sensibleactopts{$action}{$_} && !$localoptkeys{$_}, keys(%{$acthash{options}})};
      eval { parseopts(\@words, \%acthash, $parseoptmode, $acthash{where}) };
      $parseerr= 1 if $@;
      push @{$sections[-1]{actions}}, \%acthash;
      $acthash{options} ||= {};
      my @warnopts;
      for my $opt (keys %{$acthash{options}}) {
        push @warnopts, $opt
            unless $sensibleactopts{all}{$opt} || $sensibleactopts{$action}{$opt} || $localoptkeys{$opt};
      }
      fmt *STDERR{IO}, "Warning: Option", (@warnopts>1?"s ":" "), 
            cswordlist(" and ", @warnopts), " make", (@warnopts>1?"":"s"),
            " no sense for action \`$action' in $fname, line $linenr.\n"
          if @warnopts;
      if( $action =~ /^(?:follow|transform)$/ ) {
        $acthash{target}= @sections;
        newsection(\@sections, \$subsection, $fname);
      }
    }
    else {
      fmt *STDERR{IO}, "Error: Don't understand script line \`$line' in $fname, line $linenr.\n";
      $parseerr= 1;
    }
  }
  if( keys(%oor) || keys(%uoor) ) {
    fmt *STDERR{IO}, "Error: Too few URL command-line arguments for $fname.  Found indices ", cswordlist(" and ", keys %uoor), ".\n"
        if keys %uoor;
    fmt *STDERR{IO}, "Error: Too few general command-line arguments for $fname.  Found indices ", cswordlist(" and ", keys %oor), ".\n"
        if keys %oor;
    $parseerr= 1;
  }
  while( my ($label, $defs) = each %labels ) {
    $sections[$$defs[0]{section}]{name}= $label;
    next unless @$defs > 1;
    fmt *STDERR{IO}, "Error: Label $label multiply defined, at lines ",
        cswordlist(" and ", map($_->{scriptline}, @$defs)), " in $fname.\n";
    $parseerr= 1;
  }
  if( ! $sections[0]{starturls} || ! @{$sections[0]{starturls}} ) {
    $sections[0]{starturls}= [ @{ &$getargs(0) } ];
  }
  my @haveurls;
  while( my ($ind, $sec)= each @sections ) {
    $sec->{tests}= [];
    for my $act (@{$sec->{actions}}) {
      $act->{$_}= $act->{$_}{subexprs}[0]
          for grep defined($act->{$_}), keys(%treeopts);
      push @{$sec->{tests}}, $act->{testexpr};
      if( $act->{targetname} ) {
        unless( $labels{$act->{targetname}} ) {
          fmt *STDERR{IO}, "Error: Label \`$act->{targetname}' undefined, referenced in $fname, line $act->{scriptline}.\n";
          $parseerr= 1;
          next;
        }
        $act->{target}= $labels{$act->{targetname}}[0]{section};
        delete $act->{scriptline};
      }
      $haveurls[$act->{target}]= 1
          if defined $act->{target};
    }
    $haveurls[$ind]= 1 if $sec->{starturls} && @{$sec->{starturls}};
  }
  my @nourls= grep ! $haveurls[$_], (0..$#sections);
  if( !$globalopts{echo} && @nourls ) {
    fmt *STDERR{IO}, "Error: Section", (@nourls>1?"s":""), " ",
        cswordlist(" and ", @nourls), " ha", (@nourls>1?"ve":"s"),
        " no URLs to work on in $fname.\n";
    $parseerr= 1;
  }
  croak "Errors parsing $fname.  Aborting." if $parseerr;
  return @sections;
}


# Split script command-line arguments into URL arguments and general arguments.
# The result of the split is cached with the help of an external scalar, and
# only one of the argument type lists is returned.
# -> Reference to list of all command-line arguments (will not be modified)
#    Reference to scalar for caching the results (must be undef initially)
#    Flag indicating that general arguments (not URL args) should be returned
# <- Reference to array containing URL or general arguments (depending on flag)
sub splitscavargs
{
  my ($args, $cache, $retgargs)= @_;

  $retgargs ||= 0;
  return $$cache->[$retgargs] if $$cache;

  if( $globalopts{nouargs} || $globalopts{nogargs} ) {
    if( $globalopts{nouargs} ) {
      $$cache= [ [], [ @$args ] ];
    }
    else {
      $$cache= [ [ @$args ], [] ];
    }
    return $$cache->[$retgargs] if $$cache;
  }

  my @arggroups= arysplit { $_[0] eq "--" } @$args;
  if( @arggroups > 2 ) {
    croak "Error: Separator \`--' must occur only once on the command line unless -nouargs or -nogargs is present at the top of the script.  Aborting.";
  }
  if( @arggroups == 2 ) {     # obey "--" separator if present
    $$cache= \@arggroups;
    return $arggroups[$retgargs];
  }

  @arggroups= ( [], [] );
  while( my ($ind, $arg) = each(@$args) ) {
    unless( lookslikeurl($arg) ) {
      $arggroups[1]= [ @$args[$ind..$#$args] ];
      last;
    }
    push @{$arggroups[0]}, $arg;
  }
  $$cache= \@arggroups;
  return $arggroups[$retgargs];
}


# Parse the commandline of the scav command.  The first argument is taken to be
# the scavenger script file name.  The remaining command line is parsed by
# parsescript().
# -> Reference to array of command line arguments
sub parsescavcmdline
{
  my ($args)= @_;

  return if !@$args && ($globalopts{version} || $globalopts{status} || $globalopts{help});
  printhelp() unless @$args;
  my $scriptfile= shift @$args;
  $pipesecs[0]{options}{decompress}= 1;
  @pipesecs= parsescript($scriptfile, $args, $pipesecs[0]{options});
  elaboratestarturls();
  cleanup_init();
}



#sectionnonctest##############################################################
##############################################################################
####             Non-content tests and test-related subroutines
##############################################################################
##############################################################################

# Create a "shadow" test expression tree from which we can remove
# subexpressions if/when they are completely determined (such as a FALSE in an
# AND subexpression).
# -> Test expression (sub)tree
#    when they are evaluated because the result may change during parsing
# <- Shadow (sub)tree, reference to hash
sub shadowtests
{
  my ($test)= @_;

  return { test => $test, require => $test->{require},
        subexprs => [ map shadowtests($_), @{$test->{subexprs} || []} ] };
}


# Negate the result of a test, including values which are not "t"rue or
# "f"alse.
# -> Test value
# <- Negated test value
sub testvalnot
{
  my ($val)= @_;

  return "t" if $val eq "f";
  return "f" if $val eq "t";
  return $val;
}


# Check if all of a list of shadowed tests has been decided.
# -> List of test shadows
# <- 1 if all have the value true, false or undecidable; 0 otherwise
sub decided
{
  for (@_) {
    return 0 unless $_->{value} && $_->{value} =~ /^[tfu]/;
  }
  return 1;
}


# Apply a test to a string (such as a URL, an HTTP header line or part of a
# document).  If an array of the string's words is also given, word matches
# (non-verbatim tests) are attempted only starting with the first word, because
# then the calling context (performintexttests()) is expected to handle the
# word array.  Otherwise the string is split, and matches are attempted at all
# positions.
# -> Reference to hash representing the test
#    String to be tested
#    Minimum offset at which to start the search (in chars or words depending
#    on the test; optional)
#    Reference to array of words of the string (or undef)
#    Maximal word offset to try a match at in word array (ignored if word array
#    is not given; may be undef, then all possible offsets according to pattern
#    array length are tried)
# <- List containing match position and length of match, if the string matches
#    the prescribed test; otherwise the empty list.  The position and length
#    are in words for word-oriented tests, otherwise in characters.
sub evalstrtest
{
  my ($test, $str, $minoff, $words, $maxoff)= @_;

  $minoff ||= 0;
  if( ref($test->{pattern}) ne "ARRAY" ) {
    if( $test->{matchtype} eq "re" || $test->{matchtype} eq "glob" ) {
      return () if $minoff >= length($str);
      my $teststr= substr($str, $minoff);
      $teststr= &$uni2base($teststr) unless $test->{nouni2base};
      # TODO: The offset / length returned here may be in terms of chars after
      # conversion with &$uni2base, but seems to be used in
      # performintexttests() as though it were original characters.
      return ($minoff+length($1), length($2)) if $str =~ /^(.*?)($test->{pattern})/;
      return ();
    }
    else {
      die "`", $test->{matchtype}, "' is not a non-word-oriented match type, in evalstrtest.";
    }
  }
  unless( $words ) {
    my @wds;
    splitwords(\@wds, undef, $str);
    $words= \@wds;
  }
  my $patterns= $test->{pattern};
  if( ! defined($maxoff) || $maxoff > @$words - @$patterns ) {
    $maxoff= @$words - @$patterns;
  }
  return () if $maxoff < 0;
  if( $test->{matchtype} eq "re" || $test->{matchtype} eq "glob" ) {
tryrewordmatch:
    for my $off ($minoff..$maxoff) {
      for my $ind (0..$#$patterns) {
        next tryrewordmatch if $$words[$ind+$off] !~ /$$patterns[$ind]/;
      }
      return ($off, scalar(@$patterns));
    }
    return ();
  }
  elsif( $test->{matchtype} eq "leven" ) {
    my @wlengths;
    my $threshdist;
    if( defined($test->{distance}) ) {
      $threshdist= $test->{distance};
    }
    else {
      @wlengths= map length($_), @$words;
    }
trylevwordmatch:
    for my $off ($minoff..$maxoff) {
      for my $ind (0..$#$patterns) {
        unless( defined($test->{distance}) ) {
          $threshdist= $test->{reldist} *
                      max2($wlengths[$ind+$off], $test->{plengths}[$ind]);
        }
        my $dist= $test->{case} ?
          Text::LevenshteinXS::distance($$words[$ind+$off], $$patterns[$ind]) :
      Text::LevenshteinXS::distance(lc($$words[$ind+$off]), $$patterns[$ind]);
        next trylevwordmatch if $dist > $threshdist;
      }
      return ($off, scalar(@$patterns));
    }
    return ();
  }
  elsif( $test->{matchtype} eq "soundex" ) {
    my @scodes= phoneticenc(@$words[0..$#$patterns+$maxoff]);
    for my $off ($minoff..$maxoff) {
      return ($off, 0+@$patterns) if phoneticeq($patterns, \@scodes);
      shift @scodes;
    }
    return ();
  }
  else {
    die "Unknown word-oriented match type `$test->{matchtype}' in evalstrtest";
  }
}


# Determine result of an operator test expression, executing subexpression
# tests as needed.  If a subtree is already completely decided, it is
# removed from the shadow tree.
# -> Shadow of operator test expression
#    Reference to subroutine which performs subexpression tests
#    Arguments to pass to the subroutine after the shadow of the subexpression
#    test
sub testop
{
  my ($shadow, $subeval, @args)= @_;
  my $test= $shadow->{test};
  my $op= $test->{type};

  return unless @{$shadow->{subexprs}};
  if( $op eq "not" ) {
    my $subexpr= $shadow->{subexprs}[0];
    &$subeval($subexpr, @args);
    $shadow->{value}= testvalnot($subexpr->{value});
    $shadow->{subexprs}= [] if $shadow->{value} =~ /^(?:t|f)/;
  }
  else {
    my $req= $shadow->{require};
    my $tot= @{$shadow->{subexprs}};
    my $todo= "";
    for my $subexpr (@{$shadow->{subexprs}}) {
      &$subeval($subexpr, @args);
      if( $subexpr->{value} eq "t" )     { --$req; --$tot; $subexpr= undef; }
      elsif( $subexpr->{value} eq "f" )  { --$tot; $subexpr= undef; }
      elsif( $subexpr->{value} ne "u" )  { $todo .= $subexpr->{value}; }
      # Short circuit:
      if( $req == 0 ) {
        $shadow->{value}= "t";
        $shadow->{subexprs}= [];
        return;
      }
      elsif( $req > $tot ) {
        $shadow->{value}= "f";
        $shadow->{subexprs}= [];
        return;
      }
    }
    if( !$todo ) {      # no tests remain but not t or f
      $shadow->{value}= "u";
      $shadow->{subexprs}= [];
    }
    else {
      # Extract most expensive evaluation still to be done as value:
      $todo =~ s/.*c.*/c/;
      $todo =~ s/.*h.*/h/;
      $shadow->{value}= $todo;
      $shadow->{require}= $req;
      @{$shadow->{subexprs}}= grep defined($_), @{$shadow->{subexprs}};
    }
  }
}


# Perform tests on a URL job.  The first argument specifies the subroutine
# which performs the actual tests.  The test and (if applicable) while
# expression of the job are evaluated using this subroutine.
# -> Reference to function taking as its argument a reference to a shadow
#    expression, a reference to an URL job hash and possibly more arguments
#    Reference to an array which contains references to the test expression and
#    while expression shadow trees
#    Other arguments to pass to the test function (usually including the URL
#    job hash)
sub performtests
{
  my ($testfunc, $shadows, @args)= @_;

  for my $shad (@$shadows) {
    &$testfunc($shad, @args) if $shad->{test};
  }
}


# Perform a test on a document's URL.  The "value" field of every subexpression
# is set to "t"rue, "f"alse or other (if the expression depends on non-URL
# tests).
# -> Reference to the shadow of the test expression tree
#    Reference to the hash representing the document/URL
sub urltest
{
  my ($shadow, $job)= @_;
  my $test= $shadow->{test};
  my $result;

  if( $test->{class} eq "op" ) {
    testop($shadow, \&urltest, $job);
    return;
  }
  elsif( $test->{class} eq "utests" ) {
    # Test real (canonical) URL for URL shortener redirects; in other cases no
    # head request has yet been done when this is called.
    my $testurl= $job->{shortened} ? $job->{realurl} : $job->{url};
    if( $test->{type} eq "true" ) {
      $shadow->{value}= "t";
    }
    elsif( $test->{type} eq "false" ) {
      $shadow->{value}= "f";
    }
    elsif( $test->{type} eq "urlgroup" ) {
      $shadow->{value}= inurlgroup($testurl, $test->{arg})? "t" : "f";
    }
    elsif( $test->{type} eq "localfile" ) {
      my $fname= transformstring($testurl, $test->{arg}, $job);
      $shadow->{value}= defined($fname) && $fname ne "" && -e $fname ? "t" : "f";
    }
    else {
      if( $test->{type} eq "name" ) {
        if( $job->{type} eq "url" ) {
          $testurl =~ s/\?.*$//;
          $testurl =~ s/\/+$//; # remove trailing slashes to get directory name
          $testurl =~ s/^.*\///;
        }
        elsif( $job->{type} eq "file" ) {
          $testurl= (File::Spec->splitpath($testurl))[2];
        }
        else {          # local directory
          my $dirs= (File::Spec->splitpath($testurl))[1];
          my @dirary= File::Spec->splitdir($dirs);
          $testurl= pop @dirary;
        }
      }
      elsif( $test->{type} ne "url" ) {
        die "Unknown test type `$test->{type}' encountered in urltest().";
      }
      $shadow->{value}= evalstrtest($test, URI::Escape::uri_unescape($testurl)) ? "t" : "f";
    }
  }
  elsif( $test->{class} eq "htests" ) {
    $shadow->{value}= "h";
  }
  elsif( $test->{class} eq "ctests" ) {
    $shadow->{value}= "c";
  }
  else {
    die "Unknown expression class `$test->{class}' encountered in urltest().";
  }
}


# Evaluates a numerical elementary test expression.  Returns "t"rue or "f"alse,
# depending on how the actual value compares to the reference test argument and
# the kind of test ("+" actual >= ref?; "-" actual <= ref?; otherwise =).
# -> Reference to test expression hash
#    Actual numerical value the test refers to
#    Grace interval to either side of test value
sub evalnumtest
{
  my ($test, $actual, $tol)= @_;

  if( $test->{subtype} eq "+" ) {
    return $actual >= $test->{arg}-$tol? "t" : "f";
  }
  elsif( $test->{subtype} eq "-" ) {
    return $actual <= $test->{arg}+$tol? "t" : "f";
  }
  else {
    return $actual >= $test->{arg}-$tol && $actual <= $test->{arg}+$tol?
                "t" : "f";
  }
}


# Perform a HTTP header or file system stat test.
# -> Reference to the shadow of the test expression tree
#    Reference to the hash representing the document/URL
sub headertest
{
  my ($shadow, $job)= @_;
  my $test= $shadow->{test};

  if( $test->{class} eq "op" ) {
    testop($shadow, \&headertest, $job);
  }
  elsif( $shadow->{value} eq "h" ) {
    if( $test->{type} eq "size" ) {
      my $size;
      if( $job->{type} eq "file" ) {
        $size= $job->{stat}{size};
        $shadow->{value}= evalnumtest($test, $size, 0);
      }
      else {
        $size= $job->{headers}{"Content-Length"}[0];
        if( defined($size) ) {
          $shadow->{value}= evalnumtest($test, $size, 0);
        }
        else {
          $shadow->{value}= "c";
        }
      }
    }
    elsif( $test->{type} eq "modified" ) {
      my $time;
      if( $job->{type} eq "file" ) {
        $time= $job->{stat}{time};
        $shadow->{value}= evalnumtest($test, $time, 0);
      }
      else {
        $time= $job->{headers}{"Last-Modified"}[0];
        if( defined($time) && defined($time= Date::Parse::str2time($time)) ) {
          # Ignore "last modified" time when set to retrieval time by server
          if( $time <= $job->{requesttime}+$globals{modtolerance} &&
              $time >= $job->{requesttime}-$globals{modtolerance} ) {
            $shadow->{value}= "u";
          }
          else {
            $shadow->{value}= evalnumtest($test, $time,
                                          $globals{modtolerance});
          }
        }
        else {
          $shadow->{value}= "u";
        }
      }
    }
    elsif( $test->{type} eq "type" ) {
      if( $job->{type} eq "file" ) {
        $shadow->{value}= "c";
      }
      else {
        my $type= $job->{headers}{"Content-Type"}[0];
        if( defined($type) ) {
          $shadow->{value}= evalstrtest($test, $type) ? "t" : "f";
        }
        else {
          $shadow->{value}= "c";
        }
      }
    }
    elsif( $test->{type} eq "httpheader" ) {
      if( $job->{type} eq "file" ) {
        $shadow->{value}= "u";
      }
      else {
        $shadow->{value}= "u";
testhttpheaders:
        while( my ($hdr, $list)= each %{$job->{headers}} ) {
          last unless evalstrtest($test->{keyexpr}, $hdr);
          for my $val (@$list) {
            $shadow->{value}= evalstrtest($test->{valexpr}, $val) ? "t" : "f";
            # "or" semantics in case of duplicate headers:
            last testhttpheaders if $shadow->{value} eq "t";
          }
        }
      }
    }
    else {
      die "Unknown test type `$test->{type}' encountered in headertest().";
    }
  }
}


# Perform tests on the cache file which can be performed without parsing it.
# Currently this applies to the size, mime and exif type tests.
# -> Reference to the shadow of the test expression tree
#    Reference to the hash representing the document/URL
#    Reference to a File::Type object
sub quickcontenttest
{
  my ($shadow, $job)= @_;
  my $test= $shadow->{test};

  if( $test->{class} eq "op" ) {
    testop($shadow, \&quickcontenttest, $job);
  }
  elsif( $shadow->{value} eq "c" ) {
    if( $test->{type} eq "size" || $test->{type} eq "rsize" ) {
      my $size;
      if( $test->{type} eq "size" ) {
        $size= ( stat($job->{origfile}) )[7];
      }
      else {
        $size= ( stat($job->{decfile}) )[7];
        # Use "decfile" file, which is decompressed, but not converted to
        # different format (for PS->PDF)
      }
      $shadow->{value}= evalnumtest($test, $size, 0);
    }
    elsif( $test->{type} eq "type" || $test->{type} eq "rtype" ) {
      my $mt= getmimetype($job->{decfile});
      if( !$mt && $job->{headers}{"Content-Type"} ) {
        $mt= $job->{headers}{"Content-Type"}[0];
      }
      return unless $mt;
      $mt =~ s/\/x-/\//;        # without "x-"
      $shadow->{value}= evalstrtest($test, $mt) ? "t" : "f";
      if( $shadow->{value} eq "f" ) {
        $mt =~ s/\//\/x-/;      # with "x-"
        $shadow->{value}= evalstrtest($test, $mt) ? "t" : "f";
      }
    }
    elsif( $test->{type} eq "exif" ) {
      $job->{exif}= Image::ExifTool::ImageInfo($job->{cachefile},
            { List => 0, Binary => 0, Unknown => 0, IgnoreMinorErrors => 1 })
          unless exists $job->{exif};
      my $exifdata= $job->{exif};
      $shadow->{value}= "u", return if !$exifdata || $exifdata->{Error};
      delete @$exifdata{qw(ExifToolVersion Error Warning)};
      while( my ($key, $value)= each %$exifdata ) {
        next unless evalstrtest($test->{keyexpr}, $key);
        $shadow->{value}= evalstrtest($test->{valexpr}, $value)? "t" : "f";
      }
    }
  }
}


# Determine the MIME type of a file.  If available, File::MimeInfo::Magic is
# used, otherwise the file command.  If neither is available or the MIME type
# could not be determined, the empty string is returned.
# -> File name
# <- MIME type or empty string
sub getmimetype
{
  my ($fname)= @_;
  my $type= "";

  if( $globals{mimeinfomagic} ) {
    $type= File::MimeInfo::Magic::mimetype($fname) || "";
  }
  elsif( $globals{file} ) {
    my @filecmd= ($globals{file}, "-i", $fname);
    my $type= system @filecmd;
    $type =~ s/^.*:\s*//;
    $type =~ s/;.*$//;
  }
  return $type;
}


#sectiondoctest###############################################################
##############################################################################
####                Document parsing and content tests
##############################################################################
##############################################################################

# Build list of the shadows of linksto tests, document header tests and intext
# tests (separately).  The test shadows are appended to the lists pointed to by
# the given references.
# -> Reference to array to which to add intext test shadows
#    Reference to array to which to add linksto test shadows
#    Reference to array to which to add header test shadows
#    Root(s) of test expression shadow (sub)tree(s)
# (<-) References to lists of the shadows of non-linksto and of linksto content
#    test expressions
sub listcshadows
{
  my ($intextlist, $linkstolist, $headerlist, @shadows)= @_;

  for my $shad (@shadows) {
    next if !$shad->{test} || keys(%{$shad->{test}})==0 ||
                ($shad->{value} && $shad->{value} =~ /[tfu]/);
    if( $shad->{test}{type} eq "intext" ) {
      push @$intextlist, $shad;
      next;
    }
    if( $shad->{test}{type} eq "linksto" ) {
      push @$linkstolist, $shad;
      next;
    }
    if( $shad->{test}{type} eq "header" ) {
      push @$headerlist, $shad;
      next;
    }
    listcshadows($intextlist, $linkstolist, $headerlist, @{$shad->{subexprs}});
  }
}


# Build list of shadows to expression trees of transient in-text test options
# -linkafter and -linkbefore.  The "transient" flag of the shadow hashes is
# set, and the value is set to "c" (content dependent).
# -> Reference to list array
#    Root(s) of test expression shadow (sub)tree(s)
sub listtransshadows
{
  my ($list, @shadowtrees)= @_;

  for my $shad (@shadowtrees) {
    push @$list, $shad;
    $shad->{transient}= 1;
    $shad->{value}= "c";
    listtransshadows($list, @{$shad->{subexprs}});
  }
}


my $MAXPOS= 0x7FFFFFFF;         # max 32 bit signed ought to suffice


# Assign default value to primitive content test.  An undecided header test is
# set to be undecidable (because the header was apparently not there), other
# content tests are set to false (because the search text was not found).
# -> Reference to shadow hash of primitive test expression
# (<-) The shadow's "value" field is assigned its default value
sub primdefault
{
  my ($shadow)= @_;

  return if $shadow->{value} =~ /[tfu]/;
  if( $shadow->{test}{type} eq "header" ) {
    $shadow->{value}= "u";    # non-existent HTML/PDF header
  }
  else {
    $shadow->{value}= "f";    # unsuccessful search
  }
}


my $POSINF= 0x7FFFFFFF;
my @INTERALL= (0, $POSINF, 0, $POSINF);
# This pathological "interval" is designed to leave any other interval
# invariant under formation of the convex hull using convexhull():
my @INTERNONE= ($POSINF, 0, $POSINF, 0);


# Evaluate in-text test with regard to proximity requirements imposed by
# operator superexpressions.  Every occurrence of the search expression is
# checked against all proximity intervals, taking negation into account.
# -> Reference to shadow of test expression hash
#    Reference to array containing four numbers, the word and char interval
#    boundaries (inclusive) in which results have to lie
#    Reference to array containing current word and char position of the
#    parsing function performintexttests(); undef if parsing has completed
# <- Test result value followed (if "t") by list of intervals matching the
#    proximity requirements.  Each interval is a reference to an array
#    containing the start and end of the phrase in words and in chars.
sub testproxintext
{
  my ($shadow, $interval, $parsepos)= @_;
  my @results;

  if( $shadow->{value} eq "t" ) {
    for (@{$shadow->{matchpos}}) {
      # Match starts before prescribed interval - try next:
      next if $_->[0] < $interval->[0] || $_->[2] < $interval->[2];
      # Match starts after prescribed interval - done:
      last if $_->[0] > $interval->[1] || $_->[2] > $interval->[3];
      # Match ends after prescribed interval - try next:
      next if $_->[1] > $interval->[1] || $_->[3] > $interval->[3];
      push @results, $_;
    }
  }
  return "t", @results if @results;
  if( $parsepos &&
    ($parsepos->[0] < $interval->[1] && $parsepos->[1] < $interval->[3]) ) {
    return "c";
  }
  return "f";
}


# Return a reference to an array whose four elements are alternatingly the
# minimum and the maximum of the elements of the arrays referenced by the two
# arguments.  This gives the interval (in both words and chars) enclosing two
# given intervals, i.e. the convex hull of these intervals.
sub convexhull
{
  my ($op1, $op2)= @_;

  return [ min2($op1->[0], $op2->[0]), max2($op1->[1], $op2->[1]),
           min2($op1->[2], $op2->[2]), max2($op1->[3], $op2->[3]) ];
}

# The following holds:
# narrowinterval(inter, convexhull(I1, I2), wn, cn) = 
# narrowinterval(narrowinterval(inter, I1, wn, cn), I2, wn, cn)

# Narrow down an interval as a result of proximity constraints w.r.to a match
# or matches.  In other words, return the sub-interval that contains the
# match and only words / chars close to it as required by the proximity
# conditions.
# -> Reference to interval in words and chars
#    Reference to interval occupied by the match(es).  Responsibility for
#    checking that this is within the target interval (first argument) lies
#    with the calling context.
#    Word proximity requirement (or undef)
#    Char proximity requirement (or undef)
# <- Reference to new interval for other phrases matching the proximity
#    requirement.
sub narrowinterval
{
  my ($inter, $pos, $wnear, $cnear)= @_;
  my ($from, $to);
  my $result;

  $inter ||= \@INTERALL;
  if( defined($wnear) && $pos->[0] <= $pos->[1] ) {
    $from= max2($inter->[0], $pos->[1]-$wnear);
    $to= min2($inter->[1], $pos->[0]+$wnear);
    return \@INTERNONE if $from > $to;
    $result= [ $from, $to ];
  }
  else {
    $result= [ @{$inter}[0,1] ];
  }
  if( defined($cnear) && $pos->[2] <= $pos->[3] ) {
    $from= max2($inter->[2], $pos->[3]-$cnear);
    $to= min2($inter->[3], $pos->[2]+$cnear);
    return \@INTERNONE if $from > $to;
    push @$result, $from, $to;
  }
  else {
    push @$result, @{$inter}[2,3];
  }
  return $result;
}


# Try to evaluate a (sub)expression, taking proximity requirements into
# account.  This function handles the recursion over nested logical operators.
# Specifically, it answers the question: "at which positions are there matches
# so that this subexpression can count towards the required true subexpressions
# in the superexpression?"
# -> Reference to shadow hash of operator test expression
#    Reference to array of 4 values representing word and char intervals in
#    which matches are required to be located, when called recursively;
#    otherwise undef, indicating that superexpressions impose no proximity
#    requirements
#    Reference to array containing current word and char position of the
#    parsing function performintexttests(); undef if parsing has completed
# <- In recursive calls, returns the test result value followed (if "t")
#    by the list of intervals of phrases which match both tests and proximity
#    requirements.  Each interval is a reference to an array containing the
#    positions of the first word, last word, first char and last char of the
#    phrase.
sub testproxop
{
  my ($shadow, $interval, $parsepos)= @_;
  my $test= $shadow->{test};
  my $op= $test->{type};
  my ($req, $tot, $wnear, $cnear);

  if( $op eq "not" ) {
    my $sub= $shadow->{subexprs}[0];
    my @negresult= $sub->{test}{class} eq "op" ?
                        testproxop($sub, $interval, $parsepos) :
                        testproxintext($sub, $interval, $parsepos);
    my $value= testvalnot(shift @negresult);
    if( $value eq "t" ) {
      return "t", \@INTERNONE;
    }
    else {
      return $value;
    }
  }
  $req= $shadow->{require};
  if( !$req ) {
    $shadow->{subexprs}= [];
    if( !$shadow->{value} || $shadow->{value} eq "t" ) {
      $shadow->{value}= "t";
      return "t", \@INTERNONE;
    }
    else {
      return $shadow->{value};
    }
  }
  $tot= @{$shadow->{subexprs}};
  $wnear= $test->{options}{wordnear};
  $cnear= $test->{options}{charnear};

  my (@primsubs, @opsubs);
  my $unknown= 0;
  for my $subexpr (@{$shadow->{subexprs}}) {
    # Divide subexpressions up into operator subexpressions and in-text tests:
    if( $subexpr->{test}{class} eq "op" ) {
      push @opsubs, $subexpr;
    }
    elsif( $subexpr->{test}{type} eq "intext" ) {
      push @primsubs, $subexpr;
    }
    else {
    # Handle not-in-text tests and operator subexpressions which have
    # already been decided without requiring any proximity checking.  If they
    # decide this subexpression, set our "value" field so the next level gets
    # to know.
      if( $subexpr->{value} eq "t" ) {
        $subexpr= undef;
        --$req;
        --$tot;
      }
      elsif( $subexpr->{value} eq "f" ) {
        $subexpr= undef;
        --$tot;
      }
      else {
        ++$unknown;
      }
      if( $req <= 0 ) {         # expr is true due to not-in-text tests alone
        $shadow->{value}= "t";
        $shadow->{subexprs}= [];
        return "t", \@INTERNONE;
      }
      elsif( $req > $tot ) {    # expr is false due to not-in-text tests alone
        $shadow->{value}= "f";
        $shadow->{subexprs}= [];
        return "f";
      }
    }
  }
  @{$shadow->{subexprs}}= grep defined($_), @{$shadow->{subexprs}};
  $shadow->{require}= $req;
  # Return if the result depends exclusively on not-in-text tests which are
  # undecidable:
  if( @primsubs+@opsubs == 0 ) {
    $shadow->{value}= "u";
    $shadow->{subexprs}= [] unless $shadow->{transient};
    return "u";
  }

  my @subs= (@primsubs, @opsubs);
  my $nprims= @primsubs;
  my (@ind, @subranges, @totrange, @unknownflags, @results);
  my ($ignore, $unknownassume, $unknowndep, $incomplete);

  # The elements of @ind correspond to the subexpressions and keep track of
  # which are used in "N out of M" type logical functions.  A value of -1
  # indicates a subexpression is unused.  Other values refer to which
  # occurrance (match) of previous subexpressions is just being taken for
  # granted.  $ignore is the number of subexpressions to come which may be
  # unused.

  # $ignore keeps track of the number of remaining subexpressions that can be
  # ignored because they are not required to be true.
  $ignore= @subs - $req;

  # $unknowndep is set to true if the value of this expression may depend on
  # the value of an undecidable subexpression.  It is only used when there are
  # no solutions without undecidables.  $unknownassume gives the number of
  # undecidable subexpressions that are currently assumed to be true, and
  # @unknownflags describes which.
  $unknowndep= $unknown >= $req;
  $unknownassume= 0;

  # @totrange stores the interval containing all sibling subexpressions matches
  # so far.  @intervals stores the interval in which future subexpressions
  # matches must lie.
  @totrange= ( \@INTERNONE );

  while( 13 )
  {
#debug    print join(" ", @ind), "\n";  # debug
#debug    print Dumper(\@totrange, \@subranges);        # debug
    if( @ind && $ind[-1] == 0 )
    {           # when current subexpr is first used, find matches
      my $subinter= narrowinterval($interval, $totrange[-1], $wnear, $cnear);
#debug      print "finding match for subexpr $#ind in interval:\n", Dumper($subinter);  # debug
      my @ranges= $#ind < $nprims ?
                        testproxintext($subs[$#ind], $subinter, $parsepos) :
                        testproxop($subs[$#ind], $subinter, $parsepos);
      my $value= shift @ranges;
#debug      print "found ", 0+@ranges, " matches: $value\n";    # debug
      $incomplete ||= $value eq "c";
      if( @ind == @subs - $ignore ) {           # found solution(s)
        if( $unknownassume || $value eq "u" ) {
          $unknowndep= 1;
        }
        else {
#debug    print "found ", 0+@ranges, " results\n";      # debug
          push @results, map(convexhull($totrange[-1], $_), @ranges);
        }
        pop @ind;
        pop @totrange;
        pop @subranges;
        pop @unknownflags;
        next;
      }
      elsif( !$unknowndep && $value eq "u" ) {
        $unknownflags[-1]= 1;
        ++$unknown;
        ++$unknownassume;
      }
      elsif( !@ranges ) {       # no match for subexpression
        if( !$unknowndep && $unknownassume < $unknown ) {
          $unknownflags[-1]= 2;
          ++$unknownassume;
        }
        else {
          pop @ind;
          pop @totrange;
          pop @subranges;
          pop @unknownflags;
          next;
        }
      }
      else {
        $subranges[-1]= \@ranges;
        $totrange[-1]= convexhull($totrange[-1], $ranges[0]);
      }
    }
    elsif( @ind && $ind[-1] > 0 ) {
      $totrange[-1]= convexhull($totrange[-2], $subranges[-1][$ind[-1]]);
    }
    if( @ind < @subs - $ignore ) {              # keep descending
      # Initially the first subexpressions are ignored, as represented by
      # assigning them the subexpression match index -1.  After the first
      # backtracking, this will be incremented to 0, and matches will be
      # searched for and used.
      push @ind, ((-1) x $ignore), 0;
      push @totrange, ($totrange[-1]) x ($ignore+1);
        # no deep copy of $totrange[-1], but that's OK
      push @subranges, ([ \@INTERNONE ]) x ($ignore+1);
        # interval neutral w.r.to convexhull(); no deep copy again
      push @unknownflags, (0) x ($ignore+1);
      $ignore= 0;
      # Go find matches for current subexpression before descending further:
      redo;
    }
  }
  continue {
    while( @ind && $ind[-1] >= 0 && $ind[-1] == $#{$subranges[-1]} ) {
      # backtrack
#debug      print "backtracking\n";     # debug
      pop @ind;
      pop @totrange;
      pop @subranges;
      --$unknownassume if $unknownflags[-1];
      --$unknown if $unknownflags[-1] == 1;
      pop @unknownflags;
    }
    last if !@ind;
#debug    getc();       # debug
    ++$ignore if $ind[-1] < 0;
    ++$ind[-1];
  }

  my $retval;
  if( @results ) {
    $retval= "t";
  }
  elsif( $incomplete ) {
    $retval= "c";
  }
  elsif( $unknowndep ) {
    $retval= "u";
  }
  else {
    $retval= "f";
  }
#debug  print "done ($retval)\n";       # debug
#debug  print Dumper(\@results);        # debug

  if( $interval ) {             # recursive call
    return $retval, @results;
  }
  else {        # top level proximity-restricted operator
    $retval =~ s/u/c/ if $parsepos;
    $shadow->{value}= $retval;
    $shadow->{subexprs}= [] if $retval =~ /^[tf]/ && ! $shadow->{transient};
    $shadow->{matchpos}= \@results;
    return;
  }
}


# Try to evaluate an expression tree.  This may be called at intervals during
# parsing to see whether the value of the expression tree is already
# determined, and after parsing to generate the end result.
# -> Reference to shadow of expression tree
#    Reference to array containing current word and char position of the
#    parsing function performintexttests(); undef if parsing has completed
sub resolvevalue
{
  my ($shadow, $pos)= @_;

  return if ! $shadow->{transient} && $shadow->{value} =~ /^[tfu]/;

  if( $shadow->{test}{class} eq "op" ) {
    if( !defined($shadow->{test}{options}{wordnear}) &&
        !defined($shadow->{test}{options}{charnear}) &&
        ! $shadow->{transient} ) {
      testop($shadow, \&resolvevalue, $pos);
    }
    else {
      testproxop($shadow, undef, $pos);
    }
  }
  elsif( !$pos ) {
    primdefault($shadow);
  }
}


# Tests if parsing can be aborted because the test expressions can already be
# fully evaluated.  An evaluation attempt will only be performed every
# $globals{evalinterval} characters; otherwise false is returned right away.
# -> Reference to URL job hash
#    Reference to parsing data hash
# <- true if parsing can be aborted
sub readdone
{
  my ($job, $pd)= @_;

  return 0 if $pd->{pos} < $pd->{nextevalpos};
  map { resolvevalue($_, [$pd->{wpos}, $pd->{pos}]); } @{$pd->{shadows}};
  # Rebuild test shadow lists to prevent unnecessary testing
  @$pd{qw(itshadows ltshadows hdshadows)}= ( [], [], [] );
  listcshadows($pd->{itshadows}, $pd->{ltshadows}, $pd->{hdshadows},
                        @{$pd->{shadows}});
  $pd->{nextevalpos} += $globals{evalinterval};
  return 0 if $pd->{transshadows} && @{$pd->{transshadows}};
  return decided(@{$pd->{shadows}});
}


# Perform in-text tests, which are all content tests not preceded by an option
# declaring them something else.  Until a sufficient amount of text determined
# by the "needwords" entry in the parse data hash has accumulated, no tests are
# actually performed.  Even then, tests are run only periodically at
# $globals{intexttestinter} characters to save overhead.  The last argument,
# when true, overrides this behaviour and forces tests to be run now.
# -> Reference to parsing data hash
#    New text segment from parser (UTF8 encoded)
#    End-of-document flag, must be true for last call
# <- True if tests were performed
sub performintexttests
{
  my ($pd, $text, $eof)= @_;

  # return 0 if $text =~ /^\s+$/;
  $text =~ s/\s+/ /g;
# print STDERR "intext: $text\n";
  splitwords($pd->{wordcache}, $pd->{wordpos}, $text);
# print STDERR "word cache: ", join(" ", @{$pd->{wordcache}}), "\n";
  $pd->{textcache} .= $text;
  my $surplus= length($pd->{textcache}) - $pd->{charoverlap};
  my $wsurplus= @{$pd->{wordcache}} - $pd->{wordoverlap};
  my $regularrun= $wsurplus > 0 && $surplus > 0 &&
                  length($pd->{textcache}) > $globals{intexttestinter};
  return 0 unless $regularrun || $eof;
  # Exempt trailing words from comparisons; they will again be tested next time
  my $maxwoff= $eof? undef : $wsurplus;
  for my $shad (@{$pd->{itshadows}}, @{$pd->{transshadows} || []})
  {
    next if $shad->{value} ne "c" && ! $shad->{transient};
    my $minoff= 0;
    my ($pos, $len)= evalstrtest($shad->{test}, $pd->{textcache},
                                $minoff, $pd->{wordcache}, $maxwoff);
    while( defined($pos) ) {
      $shad->{value}= "t";
      my $end= $pos + $len - 1;
      # TODO: only do this if necessary
      if( ref($shad->{test}{pattern}) ) { # word-oriented test
        my $cpos= $pd->{wordpos}[$pos];
        my $cend= $pd->{wordpos}[$end] +
                        length($pd->{wordcache}[$end]) - 1;
        push @{$shad->{matchpos}},
                [ $pd->{wpos}+$pos, $pd->{wpos}+$end, $cpos, $cend ];
      }
      else {                            # char-oriented test
        $pos += $pd->{pos};
        $end += $pd->{pos};
        my ($wpos, $wend);
        for( $wpos= 0;
             $wpos < @{$pd->{wordpos}} && $pd->{wordpos}[$wpos] <= $pos;
             ++$wpos ) {}
        --$wpos;
        for( $wend= $wpos;
             $wend < @{$pd->{wordpos}} && $pd->{wordpos}[$wend] <= $end;
             ++$wend ) {}
        --$wend;
        push @{$shad->{matchpos}},
                [ $pd->{wpos}+$wpos, $pd->{wpos}+$wend, $pos, $end ];
      }
      $minoff= max2($minoff, $end) + 1;
      # $minoff is a char or word offset depending on the type of test.
      # The max2() prevents infinite loops if command-line parsing misses a
      # zero-length test.
      ($pos, $len)= evalstrtest($shad->{test}, $pd->{textcache},
                                $minoff, $pd->{wordcache}, $maxwoff);
    }
  }
  $surplus= max2(0, $surplus);
  $wsurplus= max2(0, $wsurplus);
  $pd->{textcache}= substr($pd->{textcache}, $surplus);
  $pd->{pos} += $surplus;
  @{$pd->{wordcache}}= @{$pd->{wordcache}}[$wsurplus..$#{$pd->{wordcache}}];
  @{$pd->{wordpos}}= @{$pd->{wordpos}}[$wsurplus..$#{$pd->{wordpos}}];
  $pd->{wpos} += $wsurplus;
  return 1;
}
#TODO: discard matches when no longer needed (when is that exactly?)
#TODO: pass and store match position info from caller (eg PDF page)


# Perform -linksto tests for a number uf URLs.
# -> Reference to parse data hash containing a "ltshadows" entry referencing
#    an array of -linksto test expression hashes
#    Tag from which the URL originated, or empty string if not from HTML
#    Hash reference mapping attribute names to link URLs; or Array reference if
#    not from HTML
sub performlinkstotests
{
  my ($pd, $tagname, $attrurls)= @_;

  for my $shad (@{$pd->{ltshadows}}) {
    next if $shad->{value} ne "c";
    if( $tagname ) {
      my $linkattrs= $shad->{test}{options}{linktags}{$tagname};
      next unless $linkattrs;
      for my $url (@$attrurls{grep $linkattrs->{$_}, keys %$attrurls}) {
        $shad->{value}= "t", last if evalstrtest($shad->{test}, $url);
      }
    }
    else {
      $shad->{value}= "t" if shortgrep { evalstrtest($shad->{test}, $_) } @$attrurls;
    }
  }
}


# Perform HTML/PDF header tests.  If the header's key matches the test's key
# test expression, the test's result is set depending on whether the value
# matches its test expression.  A true result is never changed, so if multiple
# headers with the same key exist, the result is true if at least one of them
# matches.
# -> Reference to array of shadows of header tests
#    Header key
#    Header value
sub performheadertests
{
  my ($hdshadows, $key, $value)= @_;

  for my $shad (@$hdshadows) {
        # "or" semantics for multiple conflicting headers
    next if $shad->{value} eq "t";
    next unless evalstrtest($shad->{test}{keyexpr}, $key);
    $shad->{value}= evalstrtest($shad->{test}{valexpr}, $value)?
                                                                "t" : "f";
  }
}


# Perform -linktext tests.  Unlike the other test-performing functions, this
# does not require a shadow expression tree and returns a boolean value.
# -> Reference to test expression hash
#    Link text to test
# <- True if test successful
sub performlinktexttests
{
  my ($test, $ltext)= @_;

  return 1 unless defined($test);
  if( $test->{class} eq "op" ) {
    my $result;
    if( $test->{type} eq "not" ) {
      $result= !performlinktexttests($test->{subexprs}[0], $ltext);
    }
    elsif( $test->{type} eq "and" ) {
      $result= 1;
      for (@{$test->{subexprs}}) {
        last unless ($result &&= performlinktexttests($_, $ltext));
      }
    }
    else {
      $result= 0;
      for (@{$test->{subexprs}}) {
        last if ($result ||= performlinktexttests($_, $ltext));
      }
    }
    return $result
  }
  else {
    return !!evalstrtest($test, $ltext);
  }
}


# Get a token from a HTML::PullParser object while shutting up warnings about
# illegal UTF-8 (not useful to the user).
# TODO: smarter way of dealing with charsets misreported as UTF-8
# -> Reference to HTML::PullParser object
# <- Reference to token array
sub html_token_silent
{
  my ($p)= @_;

  local $SIG{__WARN__}= sub {} unless $globals{debug};
  return $p->get_token();
}

# Partial regex for extracting the character encooding from the HTTP header or
# HTML meta tag.  \S+? is used to allow non-word chars while letting the
# following part of the regex determine the right-hand delimiter.
my $CHARSETRE= qr/;\s*charset\s*=\s*(\S+?)\s*/i;

# Hash of tags which do not separate words
my %zerowidthtags= map { $_ => 1; } 
        qw(i b tt u strike s big small sub sup em strong code samp kbd var cite
           dfn abbr acronym q a blink font basefont wbr nobr bdo);

# Tags with non-text content:
my %nontexttags= ( script => 1, style => 1 );


# Parse HTML document.
# -> File name
#    Reference to URL job hash
#    Reference to parsing data hash
#    Reference to section option hash
# (<-) The "value" fields of the test expression shadows are set.
#      Link URLs are written to the link hashsets.
sub readhtml
{
  my ($fname, $job, $pd, $secopts)= @_;
  my $baseurl= $job->{realurl} || $job->{url};
  my $noabslinks= $baseurl =~ /^file:\/\//i;
  my $tagspace= 0;
  my $ignoretext= "";
  my $nointexttests= $secopts->{asplain};
        # readplain() has already been called for -asplain
  my @forms;
  my (@inhyperref, @linktext, @currlinks);
  my $form;
  my $domain= getdomain($job->{url}) if $job->{type} eq "url";
  my ($cleanurl, $upurl);

  if( $pd->{httpdirentries} ) {
    $cleanurl= $baseurl;
    $cleanurl =~ s/\?.*$//;
    $cleanurl =~ s/\/+$//;
    $upurl= $cleanurl;
    $upurl =~ s/\/+[^\/]*$//;
  }

  my $fh= opendecoding($fname, $job->{charset});
  if( !$fh ) {
    printopenerr($fname, $job);
    setnocontent($job);
    return;
  }
  my $p= HTML::PullParser->new( file => $fh,
    text => '"T", dtext' . ($pd->{needplainurls}? ', skipped_text, text':''),
    start => '"S", tagname, attr' . ($pd->{needplainurls}? ', skipped_text, text':''),
    end => '"E", tagname' . ($pd->{needplainurls}? ', skipped_text, text':'') );

  print STDERR "readhtml: $job->{url}\n" if $globals{debug};

  while( defined(my $tok= html_token_silent($p)) ) {
    getplainurls($tok->[-2] . $tok->[-1], $pd, $domain) if $pd->{needplainurls};
    if( $tok->[0] eq "T" ) {
      next if $ignoretext;
      my $txt= $tok->[1];       # dodecode($job->{charset}, $tok->[1]);
      # Perform in-text tests if necessary, do a trial evaluation periodically
      # and disable further in-text tests if we are done.
      if( !$nointexttests && performintexttests($pd, ($tagspace? " ":"") . $txt , 0)
                && readdone($job, $pd) ) {
        $nointexttests= 1;
      }
      # Space from link start tag or before:
      $linktext[$tagspace - 2] .= " " if $tagspace > 1 && $tagspace < @linktext;
      $tagspace= 0;
      $linktext[-1] .= $txt if @linktext;
    }
    elsif( $tok->[0] eq "S" ) {
      $tagspace= @linktext+1 unless $zerowidthtags{$tok->[1]};
      if( $nontexttags{$tok->[1]} ) {   # ignore content of tags like <style>
        $ignoretext= $tok->[1];
      }
      if( $tok->[1] eq "base" ) {
        $baseurl= $tok->[2]{href} if $tok->[2]{href};
        $noabslinks= $baseurl !~ /^\w+:\/\//;
        if( $pd->{httpdirentries} ) {
          $cleanurl= $baseurl;
          $cleanurl =~ s/\?.*$//;
          $cleanurl =~ s/\/+$//;
          $upurl= $cleanurl;
          $upurl =~ s/\/+[^\/]*$//;
        }
      }
      if( $tok->[1] eq "meta" && exists($tok->[2]{content}) &&
         (exists($tok->[2]{name}) || exists($tok->[2]{"http-equiv"})) ) {
        my $key= exists($tok->[2]{name})?
                        $tok->[2]{name} : $tok->[2]{"http-equiv"};
        performheadertests($pd->{hdshadows}, $key, $tok->[2]{content});
        next;           # no links in "meta name content" tag
      }
      if( $tok->[1] eq "meta" && defined($tok->[2]{"http-equiv"}) &&
          $tok->[2]{"http-equiv"} eq "content-type" &&
          defined($tok->[2]{content}) &&
          $tok->[2]{content} =~ /text\/html$CHARSETRE(?:;|$)/io ) {
        $job->{charset}= $1;
      }
      if( $form && $tok->[1] eq "input" ) {
        push @{$form->{inputs}},
                { map { $_ => $tok->[2]{$_}; } qw(value id name type) };
      }
      next unless $alllinktags{$tok->[1]};
      my %links;
      my $codebase;
      if( $tok->[1] eq "meta" ) {
        # Exception from the rule - extract URL from meta tag if refresh
        if( ($tok->[2]{"http-equiv"} || "") eq "refresh"
            && $tok->[2]{content} =~ /url=([^\s]+)/i ) {
          $links{content}= $1;
          next if $noabslinks && $links{content} =~ m!^/!;
        }
      }
      else {
        %links= %{$tok->[2]};
        $codebase= $links{codebase};
        delete @links{grep !$alllinktags{$tok->[1]}{$_}, keys %links};
        if( $links{srcset} ) {
          $links{srcset} =~ s/\s+\S+$//;
          $links{srcset} =~ s/^.*[\s,]//;
        }
        # Don't allow links to absolute file paths from downloaded documents:
        if( $codebase ) {
          $codebase= canonuri($codebase, $baseurl);
          if( $codebase =~ /^\// ) {
            # Require scheme in links if base was relative to (unknown) server
            delete @links{grep $links{$_} !~ /^\w+:\/\//, keys %links};
          }
          elsif( $codebase !~ /^\w+:\/\// ) {
            # Only links with scheme or relative links if base is relative
            delete @links{grep $links{$_} =~ /^\//, keys %links};
          }
        }
        else {
          delete @links{grep $links{$_} =~ m!^/!, keys %links}
                if $noabslinks;
        }
      }
      map { $_= canonuri($_, $codebase || $baseurl); } values %links;
      next unless %links;
      # -linksto tests: tag/attribute types are filtered in test function:
      performlinkstotests($pd, $tok->[1], \%links);
      # Forwarding links, according to "forward" option (derived from -follow
      # option):
      my @forwlinks= @links{grep $secopts->{forward}{$tok->[1]}{$_}, keys %links};
      @forwlinks= grep getdomain($_) eq $domain, @forwlinks
        if $domain && $secopts->{samedomain};
      @{$pd->{fwlinks}}{@forwlinks}= (1) x @forwlinks;
      # Prepare form processing:
      if( $tok->[1] eq "form" && $tok->[2]{action} ) {
        push @forms, [ +{ %{$tok->[2]} } ];
      }
      # Actions allowing current tag/attribute URL:
      my %folsbyattr;
      for my $fol (@{$pd->{folllinks}}) {
        my $followattrs= $fol->{options}{follow}{$tok->[1]};
        next unless $followattrs;
        map { push @{$folsbyattr{$_}}, $fol if $followattrs->{$_}; } keys %links;
      }
      next unless %folsbyattr || $tok->[1] eq "a";
      if( $tok->[1] eq "form" ) {
        push @{$forms[-1]}, @{$folsbyattr{action}};
        next;
      }
      my $linktexts= [ grep defined, @{$tok->[2]}{qw(title rel alt)} ];
      if( $HTML::Tagset::emptyElement{$tok->[1]} ) {
        # Add link URLs from empty tags:
        for my $attr (sort keys %links) {
          addlink($links{$attr}, $folsbyattr{$attr}, $domain, $linktexts,
                  $pd->{pos} + length($pd->{textcache}), $pd->{wpos} + @{$pd->{wordcache}});
        }
      }
      else {
        # Trigger saving of link tag content for -linktext tests and HTTP
        # directory heuristics
        push @inhyperref, $tok->[1];
        push @linktext, "";
        push @currlinks, [ \%links, \%folsbyattr, $linktexts ];
      }
    }
    elsif( $tok->[0] eq "E" ) {
      $tagspace= @linktext+1 unless $zerowidthtags{$tok->[1]};
      # To be tolerant towards bad HTML we search for a link tag to close
      # rather than just checking the innermost.
      my $linkind= $#inhyperref;
      while( $linkind >= 0 ) {
        last if $inhyperref[$linkind] eq $tok->[1];
        --$linkind;
      }
      if( $linkind >= 0 ) {
        my ($links, $folsbyattr, $alllinktexts)= @{$currlinks[$linkind]};
        my $linktext= join "", @linktext[$linkind .. $#linktext];
        push @$alllinktexts, $linktext;
        # Add link URLs from non-empty tags:
        for my $attr (sort keys %$links) {
          addlink($$links{$attr}, $$folsbyattr{$attr}, $domain, $alllinktexts,
                  $pd->{pos} + length($pd->{textcache}), $pd->{wpos} + @{$pd->{wordcache}});
        }
        # Identify possible HTTP directory listing entries, which we assume to
        # be plain <a href=...> tags.
        if( $pd->{httpdirentries} && $tok->[1] eq "a" && $links->{href} ) {
          $linktext =~ s/^\s+//;
          $linktext =~ s/\s+$//;
          if( $links->{href} eq "$cleanurl/$linktext"  ||
              $links->{href} eq "$cleanurl/$linktext/" ) {
            $pd->{httpdirentries}{$links->{href}}= 1;
          }
          else {
            my $cleanref= $links->{href};
            $cleanref =~ s/\?.*$//;
            $cleanref =~ s/\/+$//;
            if( $cleanref eq $upurl || $cleanref eq "$upurl/" ) {
              $pd->{httpdirentries}{$links->{href}}= "uplink";
            }
            elsif( $cleanref ne $cleanurl ) {
              # too many links to other destinations => no HTTP dir listing
              ++$pd->{httpdirentries}{"#"};
              $pd->{httpdirentries}= undef
                if $pd->{httpdirentries}{"#"} > $httpdirmaxother;
            }
          }
        }
        splice @inhyperref, $linkind, 1;
        $linktext[$linkind-1] .= $linktext[$linkind] if $linkind > 0;
        splice @linktext, $linkind, 1;
        splice @currlinks, $linkind, 1;
        --$tagspace if $tagspace;
      }
      elsif( $tok->[1] eq "form" ) {
        undef $form;
      }
      elsif( $tok->[1] eq "head" ) {
        # Mark remaining header tests as undecidable
        map { $_->{value} =~ s/c/u/; } @{$pd->{hdshadows}};
      }
      $ignoretext= "" if $tok->[1] eq $ignoretext;
    }
  }
  getplainurls("", $pd, $domain) if $pd->{needplainurls};
  performintexttests($pd, "", 1) unless $nointexttests;
  for $form ( @forms ) {
    next unless @$form > 1;
    my $formattrs= shift @$form;
    $formattrs->{inputs} ||= [];
    my $formdata= fillhtmlform($job->{url}, $formattrs);
    next unless $formdata;
    if( $formattrs->{method} && $formattrs->{method} =~ /^post$/i ) {
      for (@$form) {
        push @{$_->{links}{post}}, { url => $formattrs->{action}, post => $formdata };
      }
      next;
    }
    my $parlist= join("&", map "$_=".query_escape($formdata->{$_}), keys %$formdata);
    my $linkurl= $form->{actionurl} =~ /\?/ ? "$form->{actionurl}&$parlist" :"$form->{actionurl}?$parlist";
    for (@$form) {
      $_->{links}{$linkurl}= 1;
    }
  }
  undef $pd->{httpdirentries}{"#"} if $pd->{httpdirentries};
}



# Fill out a form according to a matching form data file.  Predefined values in
# the form are retained, but may be overridden by form file entries.
# -> URL of web page containing the form
#    Reference to form attribute hash from HTML parsing
# <- Reference to hash containing key-value pairs of form data; or undef if no
#    matching form data file was found.
sub fillhtmlform
{
  my ($url, $form)= @_;
  my %complete;

  for my $formfile (@{$globals{forms}}) {
    for my $urlcheck (@{$formfile->{urls}}) {
      if( $url =~ $urlcheck->[0] && (!$urlcheck->[1] ||
          (defined($form->{$urlcheck->[1]}) && $form->{$urlcheck->[1]} eq $urlcheck->[2])) ) {
        for my $predef (@{$form->{inputs}}) {
          next unless defined($predef->{name}) && defined($predef->{value});
          $complete{$predef->{name}}= $predef->{value};
        }
        for my $entry (@{$formfile->{entries}}) {
          my ($input)= grep defined($_->{$entry->[0]}) && $_->{$entry->[0]} eq $entry->[1], @{$form->{inputs}};
          next unless defined $input;
          if( defined($entry->[2]) ) {
            $complete{$input->{name}}= $entry->[2];
          }
          else {
            delete $complete{$input->{name}};
          }
        }
        return \%complete;
      }
    }
  }
  return undef;
}



sub readpdf
{
}



my $PLAINURLRE= qr!\b$PROTOCOLS:(?://|\\/\\/)[^\s"'>]+!o;
my $PLAINURLTAILRE= qr!\S*!;

# Extract plain-text URLs from text.
# -> Text
#    Reference to parse data hash
#    Current domain if this is a URL job, otherwise undef
sub getplainurls
{
  my ($text, $pd, $domain)= @_;
  my (@links, @samedomainlinks);

  if( $pd->{truncplainlink} ) {
    $text =~ /^($PLAINURLTAILRE)/o;
    $links[0]= $pd->{truncplainlink} . $1;
    pop @links unless $links[0] =~ /^$PLAINURLRE$/o;
    undef $pd->{truncplainlink};
  }
  push @links, $text =~ /$PLAINURLRE/go;
  $text =~ /($PLAINURLRE)\z/o;
  my $lastlink= $1;             # possibly truncated last hyperlink in $text
  if( $lastlink && $links[-1] eq $lastlink ) {
    $pd->{truncplainlink}= pop @links;
  }
  if( !$lastlink && $text =~ /\b(\w{1,6})\z/ ) {
    # save last word on the off chance that it may be start of URL
    $pd->{truncplainlink}= $1;
  }
  return unless @links;
  # Convert escaped slashes that can occur in JavaScript:
  @links= map { s!\\/!/!g if m!^\w+:\\/\\/!; $_; } @links;
  # Remove what seems to be punctuation and canonicalise:
  @links= map { s/[\.,;:]$//; canonuri($_); } @links;
  performlinkstotests($pd, "", \@links);
  map { addlink($_, $pd->{needplainurls}, $domain); } @links;
}


# Read a text line from a file handle suppressing warnings (such as about
# illegal characters in the chosen encoding, which happen when web servers
# misreport the encoding).
# -> File handle reference
# <- Text line read
sub readline_silent
{
  my ($h)= @_;

  local $SIG{__WARN__}= sub {} unless $globals{debug};
  return readline($h);
}


# Parse plain-text file (or an input pipe which decodes a different kind of
# document).
# -> File name or reference to file handle (which will be closed after
#    processing)
#    Reference to URL job hash
#    Reference to parse data hash
#    Reference to section options
# (<-) The "value" fields of the test expression shadows are set.
sub readplain
{
  my ($fname, $job, $pd, $secopts)= @_;
  my $domain= getdomain($job->{url}) if $job->{type} eq "url";
  my $handle;
  my $readdone;

  # Set header test results to undecidable unless the calling context has
  # already set them
  map { $_->{value} =~ s/c/u/; } @{$pd->{hdshadows}};
  if( ref($fname) ) {
    $handle= $fname;
  }
  else {
    if( !defined($handle= opendecoding($fname, $job->{charset})) ) {
      printopenerr($fname, $job);
      setnocontent($job);
      return;
    }
  }
  while( defined($_= readline_silent($handle)) )
  {
    getplainurls($_, $pd, $domain) if $pd->{needplainurls};
    if( !$readdone && performintexttests($pd, $_, 0) && readdone($job, $pd) ) {
      $readdone= 1;
      last unless $pd->{needplainurls};
    }
  }
  close $handle;
  getplainurls("", $pd, $domain) if $pd->{needplainurls};
  performintexttests($pd, "", 1) unless $readdone;
}


# Get a token from a RTF::Tokenizer object while shutting up its error messages
# about bad RTF code (not useful to the user).
# -> Reference to RTF::Tokenizer object
# <- List of token, argument and parameter as received from get_token()
sub rtf_token_silent
{
  my ($p)= @_;

  local $SIG{__WARN__}= sub {} unless $globals{debug};
  return $p->get_token();
}

my %rtfencmap= ( ansi => "iso-8859-1", mac => "MacRoman", pc => "cp437", pca => "cp850" );
# Ignore whole group when encountering these controls:
my $RTFIGNORE= qr/^(?:fonttbl|filetbl|colortbl|stylesheet|listtable|listoverridetable|bkmkstart|bkmkend|pict)$/i;
my $RTFSPACE= qr/^(?:cell|page|softpage|column|softcol|line|softline|enspace|emspace)$/;
my $RTFTIMKEYS= qr/^(?:crea|rev|prin|bup)tim$/;
my %rtfchars= (
  tab => "\t", line => "\n", softline => "\n", row => "\n",
  par => "\n\n", sect => "\n\n",
  endash => "\x{2013}", emdash => "\x{2014}", "_" => "\x{2011}",
  enspace => "\x{2002}", emspace => "\x{2003}", "~" => "\x{00A0}",
  zwj => "\x{200D}", zwnj => "\x{200C}", "-" => "\x{00AD}",
  bullet => "\x{2022}", lquote => "\x{2018}", rquote => "\x{2019}",
  ldblquote => "\x{201C}", rdblquote => "\x{201D}",
  "\\" => "\\", "{" => "{", "}" => "}"
);
my $RTFMAXLVL= 0x7FFFFFFF;
# Separator character for path variable
my $RTFPSEP= "/";
my $RTFCTLCHARS= qr/[a-zA-Z]/;

# Parse file in rich text format using the RTF::Tokenizer module.
# -> File name
#    Reference to URL job hash
#    Reference to parsing data hash
#    Reference to section option hash
# (<-) The "value" fields of the test expression shadows are set.
sub readrtf
{
  my ($fname, $job, $pd, $secopts)= @_;
  my ($type, $arg, $param);
  my $bracelevel= 0;
  my $path= "";
  my $skiplevel= $RTFMAXLVL;
  my $leftovers= "";
  my $domain= getdomain($job->{url}) if $job->{type} eq "url";
  my $baseurl= $job->{realurl} || $job->{url};
  my %hldata;
  my %fieldlink;
  my $infoval;
  my $unicodeequiv= 1;
  my $unicodeskip= 0;
  my $readdone;

  my $p= eval { RTF::Tokenizer->new( file => $fname, sloppy => 1 ) };
  if( $@ ) {
    printopenerr($fname, $job);
    setnocontent($job);
    return;
  }
  ($type, $arg, $param)= rtf_token_silent($p);
  while( defined($type) && $type ne "eof" )
  {
    if( $type eq "group" ) {
      if( $arg > 0 ) {
        ++$bracelevel;
        $path .= $RTFPSEP;
      }
      else {
        $skiplevel= $RTFMAXLVL if $bracelevel == $skiplevel;
        --$bracelevel;
        if( $path =~ /${RTFPSEP}hl$/o ) {
          my $url= canonuri($hldata{hlloc}, $baseurl);
          addlink($url, $pd->{folllinks}, $domain, [ $hldata{hlfr} ],
                  $pd->{pos} + length($pd->{textcache}), $pd->{wpos} + @{$pd->{wordcache}});
        }
        elsif( $path =~ /${RTFPSEP}field$/o ) {
          if( $fieldlink{target} ) {
            $fieldlink{target} =~ s/^\s*\"//;
            $fieldlink{target} =~ s/\"\s*$//;
            my $url= canonuri($fieldlink{target}, $baseurl);
            addlink($url, $pd->{folllinks}, $domain, [ $fieldlink{text} ],
                    $pd->{pos} + length($pd->{textcache}), $pd->{wpos} + @{$pd->{wordcache}});
          }
          %fieldlink= ();
        }
        elsif( $path =~ /${RTFPSEP}info$RTFPSEP($RTFCTLCHARS+)$/o ) {
          my $key= $1;
          if( length($infoval) ) {
            performheadertests($pd->{hdshadows}, $key, $infoval);
            $baseurl= $infoval if $key eq "hlinkbase";
          }
        }
        $path =~ s/${RTFPSEP}$RTFCTLCHARS*$//o;
        $bracelevel= 0 if $bracelevel < 0;
      }
    }
    elsif( $bracelevel >= $skiplevel ) {
      next;
    }
    elsif( $type eq "control" ) {
      next if $arg eq "*";
      # Append first non-"*" control name to path
      $path .= $arg if $path =~ /$RTFPSEP$/;
      if( $arg =~ /$RTFIGNORE/o ) {
        $skiplevel= $bracelevel;
      }
      elsif( $arg eq "upr" ) {          # skip non-unicode branch
        $skiplevel= $bracelevel + 1;
      }
      elsif( $arg eq "hl" ) {
        @hldata{qw(hlloc hlsrc hlfr)}= "" x 3;
      }
      elsif( $arg eq "'" && length($param) == 2 ) {
        if( $unicodeskip ) {
          --$unicodeskip;
          next;
        }
        my $ch= eval("\"\\x$param\"");
        my $uni= dodecode($job->{charset}, $ch);
        $leftovers .= $uni;
      }
      elsif( $arg eq "uc" ) {
        $unicodeequiv= $param || 1;
      }
      elsif( $arg eq "u" ) {
        $leftovers .= pack("U", $param);
        $unicodeskip= $unicodeequiv;
      }
      elsif( $rtfchars{$arg} ) {
        $leftovers .= $rtfchars{$arg};
      }
      elsif( $arg =~ /$RTFSPACE/o ) {
        $leftovers .= " ";
      }
      elsif( $rtfencmap{$arg} ) {
        $job->{charset}= $rtfencmap{$arg};
      }
      elsif( $arg =~ /^$RTFCTLCHARS+$/o &&      # guard against RE special chars
             $path =~ /${RTFPSEP}info$RTFPSEP$arg$/ ) {
        if( $arg =~ /$RTFTIMKEYS/o ) {
          $infoval= parsertftime($p) || "";
        }
        elsif( length($param) ) {
          $infoval= $param;
        }
        else {
          $infoval= "";
        }
      }
    }
    elsif( $type eq "text" ) {
      my $txt= $leftovers . dodecode($job->{charset}, $arg);
      $leftovers= "";
      if( $unicodeskip ) {
        if( length($txt) < $unicodeskip ) {
          $unicodeskip -= length($txt);
          next;
        }
        $txt= substr($txt, $unicodeskip);
        $unicodeskip= 0;
      }
      if( $path =~ /${RTFPSEP}hl${RTFPSEP}hl([a-z]+)$/ ) {
        $hldata{$1} .= $txt;
        next unless $path =~ /hlfr$/;
      }
      elsif( $path =~ /${RTFPSEP}field${RTFPSEP}fldinst/o ) {
        if( defined($fieldlink{target}) || $txt =~ s/^\s*HYPERLINK// ) {
          $fieldlink{target} .= $txt;
        }
        next;   # skip \fldinst content other than hyperlinks
      }
      elsif( $path =~ /${RTFPSEP}field${RTFPSEP}fldrslt/o ) {
        if( $fieldlink{target} ) {
          $fieldlink{text} .= $txt;
        }
      }
      elsif( $path =~ /${RTFPSEP}info$RTFPSEP$RTFCTLCHARS+$/o ) {
        $infoval .= $txt;
        next;
      }
      getplainurls($txt, $pd, $domain) if $pd->{needplainurls};
      if( !$readdone && performintexttests($pd, $_, 0) && readdone($job, $pd) ) {
        $readdone= 1;
        last unless $pd->{needplainurls};
      }
    }
  }
  continue {
    ($type, $arg, $param)= rtf_token_silent($p);
  }
  getplainurls("", $pd, $domain) if $pd->{needplainurls};
  performintexttests($pd, $leftovers, 1) unless $readdone;
}

my $RTFTIMCTL= qr/^(?:yr|mo|dy|hr|min|sec)$/;

# Parse RTF date/time control sequences and return the number of seconds since
# epoch.
# -> Reference to RTF::Tokenizer object
# <- Seconds since epoch corresponding to the parsed time, or undef if not at
#    least hour and minutes could be parsed.  The default for omitted values is
#    the current date, time 00:00:00.
sub parsertftime
{
  my ($p)= @_;
  my %timdata= ( sec => 0, min => 0,  hr => 0 );
  my %vlddata;
  my ($type, $arg, $param)= $p->get_token();

  @timdata{qw(yr mo dy)}= @{[localtime()]}[5,4,3];
  while( defined($type) && $type ne "eof" && $type ne "group" ) {
    last if $type ne "control" || $arg !~ /$RTFTIMCTL/;
    $timdata{$arg}= $arg eq "mo"? $param-1 : $param;
    $vlddata{$arg}= 1;
    ($type, $arg, $param)= $p->get_token();
  }
  $p->put_token($type, $arg, $param);
  return undef unless $vlddata{hr} && $vlddata{min};
  return Time::Local::timelocal(@timdata{qw(sec min hr dy mo yr)});
}


# Parse file in rich text format using the external program "unrtf" as an input
# filter.
# -> File name
#    Reference to URL job hash
#    Reference to parsing data hash
#    Reference to section option hash
# (<-) The "value" fields of the test expression shadows are set.
sub readunrtf
{
  my ($fname, $job, $pd, $secopts)= @_;
  my $leftovers= "";
  my $domain= getdomain($job->{url}) if $job->{type} eq "url";
  my $readdone;

  # Set header test results to undecidable unless the calling context has
  # already set them
  map { $_->{value} =~ s/c/u/; } @{$pd->{hdshadows}};
  # Using HTML output and readhtml() would allow to capture hyperlinks.  This
  # might be best done as a preprocessing step, not in a pipe.  Also, one would
  # probably have to remove the "&UnknownEntity;" entities.
  if( !open(IN, "-|", "$globals{unrtf} --text '$fname' 2>/dev/null") ) {
    printpipeerr("unrtf", "$!", $?, $job->{path} || "");
    setnocontent($job);
    return;
  }
# unrtf seems to always output text in ISO-8859-1 encoding
  binmode IN, ":encoding(iso-8859-1)";
  while( <IN> )
  {
    # Discard unrtf blurb at the start
    next if 1 .. /^-----------------/;
    next if /^\#\#\#/;
    s/&UnknownEntity;//g;       # discard junk
    getplainurls($_, $pd, $domain) if $pd->{needplainurls};
    if( !$readdone && performintexttests($pd, $_, 0) && readdone($job, $pd) ) {
      $readdone= 1;
      last unless $pd->{needplainurls};
    }
  }
  close IN;
  getplainurls("", $pd, $domain) if $pd->{needplainurls};
  performintexttests($pd, "", 1) unless $readdone;
}


sub readole
{
}


my $BASE64GRP= qr![A-Za-z0-9+/]{4}!;
my $BASE64EOL= qr!(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=|)\r?\n!;
my $BASE64RE= qr!^(?:$BASE64GRP*$BASE64EOL)*[A-Za-z0-9+/=]*\r?$!io;
# my $BASE64RE= qr!^(?:begin-base64\s+\d+\s+[^\n]+\n)?(?:$BASE64GRP*$BASE64EOL)*[A-Za-z0-9+/=]*\r?$!io;
# MIME::Base64 and base64 do not allow a begin line.  I took uudecode out of
# @filecaps because it requires one and therefore is the odd one out.  If
# supporting the begin line (and concluding ==== line) is desirable, the
# decoding function has to mediate between the differing requirements depending
# on what capabilities we have.

# Determine mime type for the purpose of finding a parsing function.  To work
# around the various mime type package which do not work consistently and
# return contradictory results, we do this ourselves so that wfind shows the
# same behaviour on all systems.
# -> Reference to URL job hash
# (<-) The "parsetype" field of the job hash is set to a parseable mime type,
#      or to the empty string otherwise.
sub setparsetype
{
  my ($job)= @_;
  my $headsize= 2048;
  my $headdata;
  my $type;

  $type= "unknown";
  if( $job->{headers} && $job->{headers}{"Content-Type"}  && 
        $job->{headers}{"Content-Type"}[0] =~ /^text\/ftp-dir-listing/i ) {
    $job->{parsetype}= "ftpdir";
    return;
  }
  open IN, $job->{cachefile} or $job->{parsetype}= "", return;
  read IN, $headdata, $headsize;
  close IN;
  if( $headdata =~ /^\320\317\021\340\241\261\032\341/s ) {
    if( $job->{url} =~ /\.xls$/i ) {
      $type= "xls";
    }
    elsif( $job->{url} =~ /\.ppt$/i ) {
      $type= "ppt";
    }
    else {
      $type= "doc";
    }
  }
  elsif( $headdata =~ /^%PDF-/s ) {
    $type= "pdf";
  }
  elsif( $headdata =~ /<!doctype\s+html\b|<html\b|<head\b|<title\b/is ) {
    $type= "html";
    if( $headdata =~ /^\xEF\xBB\xBF/ ) {   # Unicode UTF-8 "Byte-Order Mark"
      $job->{charset}= "UTF-8";
    }
    elsif( $headdata =~ /<meta\s+http-equiv="?content-type"?\s+
                    content="text\/html$CHARSETRE[;">\/\s].*\n/iox ) {
      # Update charset from content header if present in our initial data
      $job->{charset}= $1;
    }
  }
  elsif( $headdata =~ /^%!PS-Adobe-/s ) {
    $type= "ps";
  }
  elsif( $headdata =~ /^\{\\rtf/s ) {
    $type= "rtf";
  }
  elsif( $headdata =~ /^\'\'\'|^\.\\\"|^\'\\\"|^\'\.\\\"/m ) {
#       $job->{url} =~ /\w\.[1-8](?:\.gz|\.bz2)?$/i ) {
# Second term will recognise manual pages by their name - this is wrong, as it
# prevents decompression.
    # TODO: The first regex may be too permissive; I already removed ^\\\" from
    # file's magic, which would have been just two characters
    $type= "troff";
  }
  else {
    my $unprintable= $headdata;
    $unprintable =~ tr/\x09-\x0D\x20-\x7E//d;
    if( !$unprintable ) {
      $type= "plain";
    }
    else {
      my $ctlchars= $unprintable;
      $ctlchars =~ tr/\x80-\xFF//d;
      $unprintable =~ tr/\x00-\x7F//d;
      if( !$ctlchars ) {
        if( length($unprintable) < $headsize * $globals{plain8bitfrac} &&
            looksliketext($headdata) && (!defined($job->{charset}) ||
  eval { Encode::decode($job->{charset}, $headdata, Encode::FB_CROAK); }) ) {
          $type= "plain";
        }
        elsif( length($unprintable) > $headsize*$globals{asplain8bitfrac} ) {
          # treat largely 7-bit data as plain text if -asplain is given (see
          # readdocument())
          $type= "text"
        }
      }
    }
  }
  if( $type =~ /^(?:unknown|text)$/ ) {
    # Unless document type could be determined, consider compression types.
    # Ignore "text" type, which may be wrongly assigned to other formats.
    if( !$job->{parsetype} && $job->{headers} &&
           $job->{headers}{"Content-Encoding"} && 
           $job->{headers}{"Content-Encoding"}[0] ne "identity" ) {
        # try server's content coding first if applicable
      $type= $job->{headers}{"Content-Encoding"}[0];
      $type =~ s/^x-//;
    }
    else {
      if( $headdata =~ /^PK\x03\x04[\x09-\x14]/ ) {
        $type= "deflate";
      }
      elsif( $headdata =~ /^\x1f\x8b\x08[\x00-\x1f]/ ) {
        $type= "gzip";
      }
      elsif( $headdata =~ /^BZh[1-9]\x31\x41\x59\x26\x53\x59/ ) {
        $type= "bzip2";
      }
      elsif( $headdata =~ /^\037\235/ ) {
        $type= "compress";
      }
      elsif( $headdata =~ /^begin\s+\d+\s+[^\n]+\n/ ) {
        $type= "uuencoded";
      }
      elsif( $headdata =~ /$BASE64RE/o ) {
        $type= "base64";
      }
      elsif( $headdata =~ /=[0-9A-F]{2}/ && $headdata !~ /[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\xFF]/ ) {
        $type= "quoted-printable";
      }
#      elsif( $headdata =~ /^7z\274\257\047\034/ ) {
#       $type= "7zip";
#      }
    }
  }
  $job->{parsetype}= $type;
  print STDERR "set parse type to $type\n" if $globals{debug};
}


my $nolinks= { folllinks => [], fwlinks => {} };

{

my $linkind;

# Calls one of the document parsing functions depending on its file type and
# options.
# -> Reference to URL job hash
#    Reference to array of test expression shadows
# <- Reference to hash which maps "fwlinks" to a hashset containing
#    forwarding/frame links, "folllinks" to an array of links and auxiliary data
#    for link following actions, and possibly "httpdirentries" to a hashset of
#    links that could be HTTP directory listing entries.
sub readdocument
{
  my ($job, $shadows)= @_;
  my $sec= $pipesecs[$job->{section}];
  my $secopts= $sec->{options};
  my ($parsename, $parse);

  print STDERR "parse type: ", $job->{parsetype} || "??", "\n" if $globals{debug};
  if( $secopts->{asplain} && $job->{parsetype} =~ /text|ps|troff|html/i ) {
    $job->{parsetype}= "plain";
  }
  print STDERR "effective parse type: ", $job->{parsetype} || "??", "\n" if $globals{debug};
  $parse= $globals{"proc_" . $job->{parsetype}} if $job->{parsetype};
  unless( $parse ) {
    # If the result depends on content tests and we cannot parse the document,
    # it cannot be a result (independently of -pessimistic, because only
    # parsable documents qualify).  We also obviously cannot follow any links.
    map { $_->{value} =~ tr/c/f/; } @$shadows;
    return $nolinks;
  }

  my $pd= { pos => 0, wpos => 0, nextevalpos => $globals{evalinterval},
        textcache => "", wordcache => [], wordpos => [],
        wordoverlap => $sec->{needwords}-1, charoverlap => $globals{charoverlap},
        folllinks => [], fwlinks => {},
        shadows => $shadows, transientshadows => [],
        itshadows => [], ltshadows => [], hdshadows => [] };
  # Heuristical preconditions for possible HTTP directory listing:
  if( $secopts->{httpdirs} &&
      $job->{type} eq "url" && $job->{url} =~ /^http/i &&
      ($job->{realurl} || $job->{url}) =~ /$HTTPDIRRE/o &&
      ($job->{parsetype} || "") eq "html" ) {
    $pd->{httpdirentries}= {};
  }
  for my $act (@{$sec->{actions}}) {
    next unless $act->{type} =~ /$FOLLOWACTRE/o;
    push @{$pd->{folllinks}}, { options => $act->{options},
        action => $act, links => {}, linktexttest => $act->{linktext},
        linkaftershadow => $act->{linkaftershadow},
        linkbeforeshadow => $act->{linkbeforeshadow} };
    push @{$pd->{transshadows}}, @{$act->{transientshadows}};
    push @{$pd->{needplainurls}}, $pd->{folllinks}[-1]
        if $act->{options}{plainurls};
  }
  $linkind= 0;
  listcshadows($pd->{itshadows}, $pd->{ltshadows}, $pd->{hdshadows},
                        @$shadows);
  &$parse($job->{cachefile}, $job, $pd, $secopts);
  if( $secopts->{asplain} && !$secopts->{plainurls} &&
        $job->{parsetype} eq "html" ) {
    readhtml($job->{cachefile}, $job, $pd, $secopts);
  }
  map { resolvevalue($_); } (@$shadows, @{$pd->{transshadows}});
  filterlinkposition(@{$pd->{folllinks}});
  my %links;
  @links{qw(folllinks fwlinks httpdirentries)}=
                @$pd{qw(folllinks fwlinks httpdirentries)};
  return \%links;
}


# Filter collected links according to -linkafter and -linkbefore options.  This
# function leaves the char position lists but not the word position lists of
# link URLs filtered; neither are assumed to be used later.
# -> References to parse hashes related to link-following actions
#    ("folllinks" element of parse data hash or reference to a subset)
sub filterlinkposition
{
  for my $fol (@_) {
    next unless $fol->{linkaftershadow} || $fol->{linkbeforeshadow};
    my $linkafterpos= $fol->{linkaftershadow}{matchpos} if $fol->{linkaftershadow};
    my $linkbeforepos= $fol->{linkbeforeshadow}{matchpos} if $fol->{linkbeforeshadow};
    if( $fol->{linkaftershadow} && !($linkafterpos && @$linkafterpos) ||
        $fol->{linkbeforeshadow} && !($linkbeforepos && @$linkbeforepos) ) {
      $fol->{links}= ();
      next;
    }
    if( !$linkbeforepos || !@$linkbeforepos ) {
      my $firstpos= $linkafterpos->[0][3];
      for my $urldata (values %{$fol->{links}}) {
        @{$urldata->{pos}}= grep !defined || $_ > $firstpos, @{$urldata->{pos}};
      }
    }
    elsif( !$linkafterpos || !@$linkafterpos ) {
      my $lastpos= $linkbeforepos->[-1][2];
      for my $urldata (values %{$fol->{links}}) {
        @{$urldata->{pos}}= grep !defined || $_ < $lastpos, @{$urldata->{pos}};
      }
    }
    else {
      my ($aftind, $befind)= (0, 0);
      for my $urldata (values %{$fol->{links}}) {
        $urldata->{pos_}= [ grep !defined, @{$urldata->{pos}} ];
      }
      while( 13 ) {
        while( $befind <= $#$linkbeforepos && $$linkafterpos[$aftind][3] > $$linkbeforepos[$befind][2] ) {
          ++$befind;
        }
        last if $befind > $#$linkbeforepos;
        my $firstpos= $linkafterpos->[$aftind][2];
        my $lastpos= $linkbeforepos->[$befind][3];
        for my $urldata (values %{$fol->{links}}) {
          push @{$urldata->{pos_}}, grep( ($_ > $firstpos && $_ < $lastpos), @{$urldata->{pos}});
        }
        while( $aftind <= $#$linkafterpos && $$linkafterpos[$aftind][3] <= $$linkbeforepos[$befind][2] ) {
          ++$aftind;
        }
        last if $aftind > $#$linkafterpos;
      }
      for my $urldata (values %{$fol->{links}}) {
        $urldata->{pos}= $urldata->{pos_};
      }
    }
    delete @{$fol->{links}}{grep !@{$fol->{links}{$_}{pos}}, keys %{$fol->{links}}};
  }
}


# Add a link URL to the results of one or several link-following actions,
# taking link text tests and -samedomain into account if applicable.  Because
# title and other HTML tag attributes qualify as link texts, multiple link
# texts can be passed.
# -> Link URL
#    Reference to array of parse hashes related to link-following actions
#    ("folllinks" element of parse data hash or reference to a subset)
#    Domain of URL of document containing the link (or undef if local document)
#    Reference to array of link texts (or undef for file formats without link
#    text).  Be warned that this reference is used as-is for the link
#    properties hash, not copied.
#    Position in document in characters
#    Position in document in words
sub addlink
{
  my ($url, $folstructs, $domain, $linktexts, $pos, $wpos)= @_;
  my $linkdomain;
  my $props;

  $linktexts //= [];
  for my $fol (@$folstructs) {
    next if $fol->{options}{samedomain} && $domain &&
            ($linkdomain //= getdomain($url)) ne $domain;
    next if $fol->{linktexttest} && @$linktexts &&
            ! shortgrep( sub { performlinktexttests($fol->{linktexttest}, $_); }, @$linktexts);
    if( $fol->{links}{$url} ) {
      $fol->{links}{$url}= dup $fol->{links}{$url};
      push @{$fol->{links}{$url}{indices}}, $linkind;
      push @{$fol->{links}{$url}{linktexts}}, @$linktexts;
      push @{$fol->{links}{$url}{pos}}, $pos;
      push @{$fol->{links}{$url}{wpos}}, $wpos;
    }
    else {
      $props //= { index => $linkind, indices => [ $linkind ],
                   linktexts => $linktexts, linktext => $$linktexts[0],
                   pos => [ $pos ], wpos => [ $wpos ] };
      $fol->{links}{$url}= $props;
    }
  }
  ++$linkind;
}

}

#sectionget###################################################################
##############################################################################
####                Retrieval and high-level test functions
##############################################################################
##############################################################################


# Convert HTTP headers from a HTTP::Headers object (or the derived
# HTTP::Response) to a hash.
# <- Reference to hash of HTTP headers.
sub headers_as_hash
{
  my ($obj)= @_;

  my %hdrs;
  map { $hdrs{$_}= [ $obj->header($_) ]; } $obj->header_field_names();
  return \%hdrs;
}


{

# Variables local to slave process:
my %locals;

# Use a LWP::UserAgent to retrieve the document specified by a URL job.  The
# document is cached at the cache file location given in the job hash.
# -> Reference to URL job hash
#    If !=0/undef, indicates that only a head request is to be performed.
sub retrieve
{
  my ($job, $headonly)= @_;
  my $ua= $locals{ua};
  my $rawfile;
  my $response;
  my $mimet;
  my $dec_result;
  my $retry= 5;

  if( $job->{download} ) {
    return if $job->{download} eq "done" ||
                ($headonly && $job->{download} eq "head");
    goto retrieve_failed if $job->{download} eq "failed";
  }
  if( $job->{type} eq "dir" ) {
    $job->{download}= "done";
    return;
  }
  elsif( $job->{type} eq "file" ) {
    $job->{path} ||= uri2file($job->{url});
    my @sr= stat($job->{path}) or goto retrieve_failed;
    $job->{stat}= { size => $sr[7], time => $sr[9] };
    $job->{realurl}= $job->{url};
    $job->{charset}= $globals{defcharset};
    if( $headonly ) {
      $job->{download}= "head";       # put off possible decoding
      return;
    }
    $job->{download}= "done";
    decode_content($job) and return;
  }
  else {
    return if $headonly && $job->{download} eq "head";
    $job->{requesttime}= time();
    my %headers;
    $headers{Referer}= $job->{referrer} if $job->{referrer};
    while( 13 ) {
      if( $job->{post} ) {      # can't get headers only for POST request
        $response= $ua->post($job->{url}, $job->{post}, ":content_file" => $job->{cachefile}, %headers);
      }
      elsif( ! $headonly ) {
        $response= $ua->get($job->{url}, ":content_file" => $job->{cachefile}, %headers);
      }
      else {
        $response= $ua->head($job->{url}, %headers);
        if( $response->code() == 405 || $response->code() == 501 ) {
          # try GET if HEAD not allowed/implemented
          $headonly= 0;
          $response= $ua->get($job->{url}, ":content_file" => $job->{cachefile}, %headers);
        }
      }
      last if $response->is_success();
      if( --$retry && ($response->code() == 500 || $response->code() == 507 ||
          ($response->code() >= 502 && $response->code() <= 504)) ) {
        sleep 1;
        next;
      }
      goto retrieve_failed;
    }
    # print STDERR "successfully retrieved ", $job->{url}, ($headonly? " (head)":""), "\n" if $globals{debug} && $response->is_success();
    $job->{download}= $headonly? "head" : "done";
    $job->{realurl}= $response->base()->canonical()->as_string();
    $job->{headers}= headers_as_hash($response);
    if( $job->{headers}{"Content-Type"} && 
        $job->{headers}{"Content-Type"}[0] =~ /$CHARSETRE(?:;|$)/io ) {
      $job->{charset}= $1;
    }
    else {
      $job->{charset}= $globals{defcharset};
    }
    return if $headonly || $job->{robotstxt};
    decode_content($job) and return;
  }
retrieve_failed:
  if( $globalopts{verbose} && $job->{download} ne "failed" ) {
                                # don't repeat error message
    if( $job->{type} eq "file" && !$job->{stat} ) {
      print STDERR "Cannot stat $job->{path}\n";
    }
    elsif( $job->{type} eq "url" && !$response->is_success() ) {
      fmt *STDERR{IO}, "Could not retrieve $job->{url}: response ",
                        $response->code(), " (", $response->message(), ")\n";
    }
    else {
      print STDERR "Could not decode $job->{url}\n";
    }
  }
  unless( $job->{robotstxt} ) {
    ++$locals{nonex};
    $locals{nonexwgt} += $job->{weight};
    if( $job->{url} =~ m!^https://!i ) {
      ++$locals{nohttps};
      $locals{nohttpswgt} += $job->{weight};
    }
  }
#  unlink $job->{cachefile} if -f $job->{cachefile};
  $job->{cachefile}= "";
  $job->{download}= "failed";
}


# Decompress downloaded files as applicable after calling setparsetype() to
# determine the type.  Several decompressions / decodings may happen in
# sequence, such as uudecoding, gzip decompression, and ps-to-pdf conversion.
# The multiple filenames associated with a job are set correctly.  The
# "cachefile" is the final result of the decoding which the parsing functions
# operate on, the "decfile" is the result of decompression only (before format
# conversions such as PS->PDF), which is used to determine the "real" size and
# MIME type, and the "origfile" is the retrieved or locally present file, used
# for downloading to the current directory if so requested.  Some or all of
# these may be equal.
# -> Reference to URL job hash
# <- 0 error, 1 successfully decompressed or not compressed
sub decode_content
{
  my ($job)= @_;
  my $orig;             # original file or cache file name
  my $cachename;        # original cache file name
  my $dec;              # name of decompressed file
  my $tmp;              # name to use as destination for decompression
  my $conv;             # name of format-converted file (PS->PDF)
  my ($src, $dest);
  my ($decodename, $decoder);

  $orig= $job->{type} eq "file"? $job->{path} : $job->{cachefile};
  $cachename= $job->{cachefile};
  $dec= "$cachename.dec";
  $tmp= "$cachename.tmp";
  $conv= "$cachename.conv";
  $job->{origfile}= $orig;
  # If no compression is found, the original file is to be parsed:
  $job->{cachefile}= $job->{decfile}= $job->{origfile};
  setparsetype($job);   # operates on $job->{cachefile}
  $decodename= "dec_" . $job->{parsetype};
  while( $decoder= $globals{$decodename} ) {
    $src= $job->{cachefile};
    $dest= $decodename eq "dec_ps" ? $conv : $dec;
    return 0 unless &$decoder($src, $tmp) && rename($tmp, $dest);
    $job->{cachefile}= $dest;
    $job->{decfile}= $dest unless $decodename eq "dec_ps";
    setparsetype($job);
    $decodename= "dec_" . $job->{parsetype};
  }
  return 1;
}


# Convert a multi-valued test result to boolean true or false.
# -> Reference to hash of top-level test expression shadow
#    "Pessimistic" flag
# <- 1 or 0
sub shadow2bool
{
  my ($shadow, $pessimistic)= @_;
  
  return $pessimistic ?  $shadow->{value} eq "t" : $shadow->{value} ne "f";
}


# Read the contents of a local directory and return a list of URL jobs for the
# file and subdirectory it contains.  Files are turned into jobs regardless of
# actions of this pipe section.  Subdirectories yield one job for each link
# following action that tested as true.  For a simple  directory recursion, a
# single unconditional "recurse" action would suffice.
# -> Reference to directory job hash
# <- List of references to file/directory job hashes
sub dirchildren
{
  my ($job)= @_;
  my $symlinks= $pipesecs[$job->{section}]{options}{symlinks};
  my @entries;
  my (@filejobs, @subdirjobs, @jobs);
  my $goners= 0;

  if( !opendir(DIR, $job->{path}) ) {
    $job->{download}= "failed";
    $job->{value}= 0;
    ++$locals{nonex};
    $locals{nonexwgt} += $job->{weight};
    return ();
  }
  @entries= map File::Spec->catfile($job->{path}, $_),
                File::Spec->no_upwards(readdir DIR);
  closedir DIR;
direntry:
  for my $entry (@entries) {
    my @trace;
    if( -l $entry ) {
      next direntry unless $symlinks;
      my $target= tracesymlink($entry);
      unless( defined $target ) {
        print STDERR "Could not follow symbolic link `$entry'.\n"
                if $globalopts{verbose};
        ++$goners;
        next direntry;
      }
      $entry= $target;
    }
    my $newjob= { url => URI::file->new($entry)->as_string(), path => $entry };
    if( -d $entry ) {
      @$newjob{qw(type dtl gdtl)}= ("dir", $job->{dtl}-1, $job->{gdtl}-1);
      push @subdirjobs, $newjob;
    }
    elsif( -f $entry ) {
      @$newjob{qw(type dtl gdtl section)}= ("file", 0, $job->{gdtl}, $job->{section});
      push @filejobs, $newjob;
    }
  }
  my $childcount= @filejobs + @subdirjobs + $goners;
  my $childweight= $childcount ? $job->{weight} / $childcount : 0; 
  map { $$_{weight}= $childweight; }  (@filejobs, @subdirjobs);
  $locals{nonex} += $goners;
  $locals{nonexwgt} += $goners * $childweight if $goners;
  @jobs= @filejobs;
  for my $act (@{$pipesecs[$job->{section}]{actions}}) {
    next unless $act->{type} =~ /$FOLLOWACTRE/o && $act->{value};
    my @actjobs= @subdirjobs;
    map { $_->{section}= $act->{target}; } @actjobs;
    push @jobs, @actjobs;
  }
  return @jobs;
}


# Read the contents of an FTP directory listing and return a list of URL jobs
# for each file and subdirectory it contains.
# -> Reference to FTP directory job hash
#    Flag for continuing directory recursion
# <- List of references to FTP URL job hashes
# Note: one might optimise by putting the dir listing data (size, time) into
# the hashes of its children, but reaping the benefits would require
# optionally skipping the HEAD request next time round.  Not done for now.
sub ftpdirchildren
{
  my ($job)= @_;
  my (@filejobs, @subdirjobs, @jobs);

  open IN, "<", $job->{cachefile} or return ();
  while( <IN> ) {
    my @words= split /\s+/;
    my $newjob= { url => canonuri($words[-1], $job->{url}), type => "url",
              cachefile => cachefilename(), section => $job->{section} };
    if( $words[0] =~ /^d/ ) {
      next if $words[-1] =~ /^\.\.?\/?$/;
      @$newjob{qw(dtl gdtl)}= ($job->{dtl}-1, $job->{gdtl}-1);
      push @subdirjobs, $newjob;
    }
    else {
      @$newjob{qw(dtl gdtl section)}= (0, $job->{gdtl}, $job->{section});
      push @filejobs, $newjob;
    }
  }
  close IN;
  my $childcount= @filejobs + @subdirjobs; 
  my $childweight= $childcount ? $job->{weight} / $childcount : 0;
  map { $$_{weight}= $childweight; }  (@filejobs, @subdirjobs);
  @jobs= @filejobs;
  for my $act (@{$pipesecs[$job->{section}]{actions}}) {
    next unless $act->{type} =~ /$FOLLOWACTRE/o && $act->{value};
    my @actjobs= @subdirjobs;
    map { $_->{section}= $act->{target}; } @actjobs;
    push @jobs, @actjobs;
  }
  return @jobs;
}


# Convert a list of link URLs into URL job hashes.  If the parent job is a
# local file and the -unrestricted option is not active, web links are
# discarded.  The weight of the children is the weight of the parent divided by
# the number of children, their depth-to live is one less except for forwarded
# links and files in HTTP directory listings.  The pipe section of the children
# is the target section of the recurse or follow action handling them, and
# their referrer is set to the parent's URL for web-to-web links.
# -> Reference to parent URL job hash
#    Hash reference containing references to array of link following actions,
#    hash sets of forwarding links and HTTP directory entries if applicable.
#    Flag indicating recursion over follow links is to be continued
# <- List of new URL jobs, possibly fewer than URLs given
sub urlchildren
{
  my ($job, $ln)= @_;
  my $fileparent= $job->{type} eq "file";
  my $httpdir= defined $ln->{httpdirentries};
  my (@links, @postdata, @children);
  my $newjob;
  my $blackcount= 0;

  for my $k (keys %{$ln->{fwlinks}}) {
    $newjob= link2job($k, $job, $fileparent, 0, \$blackcount);
    next unless $newjob;
    $newjob->{forwarded}= 1;
    $newjob->{starturl}= 1 if $job->{starturl};
    push @children, $newjob;
  }

  for my $fol (@{$ln->{folllinks}}) {
    next unless $fol->{action}{value};
    my $posturls= delete( $fol->{links}{post} ) || [];
    for my $post (@$posturls) {
      $newjob= link2job($post->{url}, $job, $fileparent, $fol->{options}{unrestricted}, \$blackcount);
      next unless $newjob;
      $newjob->{section}= $fol->{action}{target};
      --$newjob->{dtl};
      --$newjob->{gdtl};
      $newjob->{post}= $post->{post};
      $newjob->{linktrace}= [ { post => $newjob->{post} }, @{$job->{linktrace} // []} ]
            if $globals{needtrace};
      push @children, $newjob;
    }
    while( my ($url, $linkprops) = each %{$fol->{links}} ) {
      $newjob= link2job($url, $job, $fileparent, $fol->{options}{unrestricted}, \$blackcount);
      next unless $newjob;
      $newjob->{section}= $fol->{action}{target};
      $newjob->{linktrace}= [ $linkprops, @{$job->{linktrace} // []} ]
            if $globals{needtrace};
      if( $httpdir ) {
        # We do not treat HTTP directories completely separately but filter
        # their links here.  This allows to honour -follow without duplicating
        # the code used for general links.
        next unless $ln->{httpdirentries}{$url} &&
                    $ln->{httpdirentries}{$url} ne "uplink";
        # Do not decrease DTL for documents reached from a HTTP directory, but
        # for subdirectories.  Because we cannot really know whether directory
        # entries are themselves directory listings until we have downloaded
        # them (and not really then either), there is a small chance that wfind
        # will run amok.
        if( $url =~ /$HTTPDIRRE/o ) {
          --$newjob->{dtl};
          --$newjob->{gdtl};
        }
      }
      else {
        --$newjob->{dtl};
        --$newjob->{gdtl};
      }
      push @children, $newjob;
    }
  }

  my $childweight= $job->{weight} / (1 + @children + $blackcount);
  $locals{blacklisted} += $blackcount;
  $locals{blacklistwgt} += $blackcount * $childweight;
  my $trace= $globals{needtrace}? 
    [ $job->{url}, @{$job->{trace} || []} ] : undef;
  for my $child (@children) {
    $child->{weight}= $childweight;
    $child->{trace}= $trace;
  }
# print "urlchildren returning ", 0+@children, " children\n";   # DEBUG
  return @children;
}


# Convert link URL to a job hash.  The section and depths-to-live are set to
# the same values of the original job and may have to be modified later.
# -> Link URL
#    Reference to the job hash from which the link URL comes
#    Flag incidating the link comes from a file
#    Flag incidating unrestricted link following (from file to WWW)
#    Reference to counter for blacklisted URLs
sub link2job
{
  my ($url, $job, $fileparent, $unrest, $blackcount)= @_;

  # Ignore links to protocols we don't process (such as mailto):
  return undef unless $url =~ /^$PROTOCOLS:\/\//o;
  my $obj= URI->new($url);
  my $isfile= $obj->scheme() eq "file";
  if( $isfile ) {
    return undef if !$fileparent;    # no links from WWW to local files
    return undef if $fileparent && -d $obj->file();  # no links from files to dirs
  }
  else {     # no links from files to WWW without -unrestricted
    return undef if $fileparent && ! $unrest;
  }
  if( !$isfile && inurlgroup($url, "blacklist") ) {
    ++$$blackcount;
    return undef;
  }
  return +{ url => $url, type => $isfile? "file" : "url",
              path => $isfile? $obj->file() : undef,
              section => $job->{section},
              referrer => !($fileparent || $isfile)? $job->{url} : undef,
              dtl => $job->{dtl}, gdtl => $job->{gdtl} };
}


# This is the central function of wfind.  It calls subroutines to perform URL
# tests, retrieve the document, perform header and content tests and extract
# links.
# -> Reference to URL job hash
#    Reference to thread data hash
#    If this is a recursive call due to redirections/frames, a reference to 
#    an array containing the URLs already searched.  This will be appended to
#    if further redirections are encountered.
# <- List of references to new URL jobs.
sub getandtest
{
  my ($job)= @_;

# print STDERR "getandtest(): $job->{url}, section $job->{section}, DTL $job->{dtl}\n";     # debug

  $job->{download}= "";

  if( $job->{robotstxt} ) {
    retrieve($job, 0);
    return ();
  }

  if( inurlgroup($job->{url}, "shorteners") ) {
    retrieve($job, 1);
    $job->{shortened}= 1;
  }

  my @shadows;
  my $ln;
  my $actions= $pipesecs[$job->{section}]{actions};

  for my $act (@$actions) {
    if( $act->{options}{nostart} && $job->{starturl} ) {
      $act->{value}= 0;
      next;
    }
    if( $act->{type} =~ /$FOLLOWACTRE/o && (!$job->{dtl} || !$job->{gdtl}) ) {
      $act->{value}= 0;
      next;
    }
    if( $act->{testexpr}{type} eq "true" ) {
      $act->{value}= 1;
      next;
    }
    $act->{value}= undef;
    $act->{shadow}= shadowtests($act->{testexpr});
    push @shadows, $act->{shadow};
  }

  goto jobdone if decided(@shadows);
  performtests(\&urltest, \@shadows, $job);
  goto jobdone if decided(@shadows);
  # Try head request unless we already know the content will be needed
  retrieve($job, !needcontent($actions));
  if( $job->{download} eq "failed" ) {
    setnocontent($job);
    goto jobdone;
  }
  performtests(\&headertest, \@shadows, $job);
  goto jobdone if decided(@shadows);
  retrieve($job, 0);
  if( $job->{download} eq "failed" ) {
    setnocontent($job);
    goto jobdone;
  }
  performtests(\&quickcontenttest, \@shadows, $job);
  goto jobdone if decided(@shadows);
  if( $job->{type} eq "dir" || $job->{parsetype} eq "ftpdir" ) {
    # If we arrive here, the results depend on content tests, which we cannot
    # perform on local and ftp directories.  We resolve the hit results
    # pessimistically, but the link following condition optimistically,
    # ignoring -pessimistic.
    for my $act (@$actions) {
      $act->{value}= $act->{type} =~ /$FOLLOWACTRE/o;
      delete $act->{shadow};
    }
  }
  else {
    $ln= readdocument($job, \@shadows);
  }

jobdone:
  my $unreachable;
  while( my ($ind, $act) = each @$actions ) {
    if( $unreachable ) {
      $act->{value}= 0;
      $job->{actvals}[$ind]= 0;
      delete $act->{shadow};
      next;
    }
    if( $act->{shadow} ) {
      $act->{value}= shadow2bool($act->{shadow}, $act->{options}{pessimistic});
      delete $act->{shadow};
    }
    $job->{actvals}[$ind]= $act->{value};
    print STDERR "Action $act->{type} without value at the end of getandtest()\n"
        if $globals{debug} && !defined($act->{value});
    if( $act->{type} eq "filter" && !$act->{value} ) {
      $unreachable= 1;
      next;
    }
    next if !$act->{value} || $job->{download} eq "failed" || 
                               ($job->{download} eq "done" && $ln);
    if( $act->{type} =~ /$FOLLOWACTRE/o ) {
      retrieve($job, 0);
      $ln= readdocument($job, \@shadows) if $job->{download} eq "done";
    }
    elsif( $act->{type} eq "download" ) {
      retrieve($job, 0);
    }
    elsif( $act->{type} eq "output" && $job->{download} ne "head" &&
           hsetany($act->{options}{print}, qw(size type time)) ) {
      retrieve($job, 1);
    }
  }

  # local directory
  if( $job->{type} eq "dir" ) {
    return dirchildren($job);
  }
  # FTP directory listing
  if( $job->{parsetype} && $job->{parsetype} eq "ftpdir" ) {
    return ftpdirchildren($job);
  }

  $ln //= $nolinks;
# print "before urlchildren: ", Dumper($ln, $job);
  return urlchildren($job, $ln);
}


# Find out if we are sure to need the content of a URL.
# -> Reference to array of actions
# <- 1 if needed, 0 otherwise
sub needcontent
{
    my ($actions)= @_;

    for my $act (@$actions) {
      return 1 if $act->{type} =~ /$FOLLOWACTRE/o &&
                ($act->{value} ||
                 ($act->{shadow} && $act->{shadow}{value} =~ /^[tc]/));
      return 1 if $act->{shadow} && $act->{shadow}{value} eq "c";
    }
    return 0;
}


# Set default test results when content is unavailable.
# -> Reference to URL job hash (including action hashes with shadow entry)
sub setnocontent
{
  my ($job)= @_;

  for my $act (@{$pipesecs[$job->{section}]{actions}}) {
    $act->{shadow}{value}= "f"
      if $act->{type} =~ /$FOLLOWACTRE/o || !defined($act->{shadow}{value})
                                || $act->{shadow}{value} !~ /^[tfu]/;
  }
}


# Create UserAgent object and initialise other local variables of this slave.
# -> Slave index
# (->) some of %globals and %globalopts
# (<-) %locals
sub initlocals
{
  my ($index)= @_;

  my $ua= LWP::UserAgent->new( agent => $globalopts{useragent},
        env_proxy => 1, keep_alive => $globals{keepalive},
        requests_redirectable => [qw(HEAD GET POST)],
        default_headers => HTTP::Headers->new(
                        "Accept-Charset" => $globals{"accept-charset"},
                        "Accept-Encoding" => $globals{"accept-encoding"}) );
  $ua->timeout($globalopts{timeout})
        if defined($globalopts{timeout});
  $ua->max_redirect($globalopts{redirects})
        if defined($globalopts{redirects});
  $ua->max_size($globalopts{maxsize})
        if defined($globalopts{maxsize});
  $ua->parse_head(0);
  if( $globals{lwpconnect} && $ENV{https_proxy} ) {
    my $proxy= $ENV{https_proxy};
    $proxy =~ s!^(?:\w+://)?!connect://!;
    $proxy =~ s!/?$!/!;
    $ua->proxy("https", $proxy);
  }
  $locals{ua}= $ua;
  $locals{index}= $index;
  $locals{msgprefix}= $index >= 0? "Slave $index: " : "Crawler: ";
  @locals{@cumulatives}= (0) x @cumulatives;
}


# Return cumulative quantities in %locals.
# (->) %locals
# <- Array of values of @cumulatives, in order
sub getcumulatives
{
  return @locals{@cumulatives};
}

}


# Slave process which reads URL jobs from a socket, calls getandtest() and
# sends the document's links back over the socket if the recursion continues.
# -> Slave index (for debug messages)
#    Reference to socket file handle
sub waitforwork
{
  my ($index, $socket)= @_;

  initlocals($index);
  while( 13 )
  {
    my $job= recvdata($socket);

    last if $job->{die} && $job->{die} eq "yes, really";

    print STDERR "Slave $index: $job->{url}\n" if $globals{debug};

    my @newjobs= getandtest($job);
    unshift @newjobs, $job;
    push @newjobs, { "end of list" => 1 };
    my $status= senddata($socket, @newjobs);
  }
  my @returns= getcumulatives();
  senddata($socket, \@returns);
}


#sectionsched#################################################################
##############################################################################
####                    Forking and URL scheduling
##############################################################################
##############################################################################

{

# Array containing references to hashes of data for each slave.
my @slavedata;

# Bit mask for select() containing the bits corresponding to the sockets for
# communication with the slaves.
my $slavemask;

sub createslaves
{
  $slavemask= "";
  for my $ind (0..$globalopts{slaves}-1)
  {
    my ($slavesock, $mastersock);
    if( !eval { socketpair($slavesock, $mastersock, AF_UNIX, SOCK_DGRAM, PF_UNSPEC) } ) {
      fmt *STDERR{IO}, "Could not create socketpair for communication with slave process $ind.  Terminating.\n";
      killslaves();
      exit 1;
    }
    my $pid= fork();
    if( !defined($pid) ) {
      fmt *STDERR{IO}, "Could not fork slave process $ind.  Terminating.\n";
      killslaves();
      exit 1;
    }
    if( !$pid ) {       # forked slave process
      $globals{isslave}= 1;
      close $mastersock;
      @jobs= ();
      @slavedata= ();
      waitforwork($ind, $slavesock);
      exit(0);
    }
    close $slavesock;
    push @slavedata,
      { socket => $mastersock, fileno => fileno($mastersock), pid => $pid };
    vec($slavemask, $slavedata[-1]{fileno}, 1)= 1;
  }
}


# Send a value to all slaves to tell them to die and wait for them to terminate.
sub killslaves
{
  return unless @slavedata;
  print STDERR "Assisting suicide of slave processes...\n" if $globals{debug};
  for my $ind (0..$#slavedata) {
    next unless defined($slavedata[$ind]{pid}) &&
                kill(0, $slavedata[$ind]{pid});
    senddata($slavedata[$ind]{socket}, { die => "yes, really" });
    my $cumresults;
    # Read from queue until cumulative quantities arrive, in case killslaves()
    # was called on END {} when crashing.
    do { $cumresults= recvdata($slavedata[$ind]{socket}); }
      while( defined($cumresults) && ref($cumresults) eq "HASH" );
    map { $globals{$cumulatives[$_]} += $cumresults->[$_]; } (0..$#cumulatives);
    waitpid($slavedata[$ind]{pid}, 0);
  }
  @slavedata= ();
}


# Send data structure over a socket after serialising it using Storable.
# -> Socket handle reference
#    Hash or array references (no non-ref scalars, since Storable does not
#    work on those)
# <- 1 OK, 0 error
sub senddata
{
  my ($sock, @refs)= @_;
  my $success= 1;

  for my $dataref (@refs) {
    my $msg= Storable::freeze($dataref);
    my $sent= send($sock, $msg, 0);
    $success &&= defined($sent) && $sent == length($msg);
  }
  return $success;
}


# Receive data from a (datagram) socket and deserialise it using
# Storable::thaw().
# -> Socket handle reference
# <- Reference to deserialised data structure or undef if an error occurred or
#    if the socket has been set to non-blocking operation and no data was
#    available
sub recvdata
{
  my ($sock)= @_;
  my $data;

  my $status= recv($sock, $data, 1000000, 0);
  return undef unless defined($status);
  return Storable::thaw($data);
}


# Test whether any of the slaves is done with its task by checking if the
# socket file descriptors are readable.  The timeout is given by
# $globals{dispatchwait}.
# <- Index of the first slave that has completed its job, or undef
sub slaveready
{
  my $donemask= $slavemask;

  return undef unless select($donemask, undef, undef, $globals{dispatchwait});
  for my $ind (0..$#slavedata) {
    return $ind if vec($donemask, $slavedata[$ind]{fileno}, 1);
  }
  return undef;
}


# Perform search with threading.  URL jobs are assigned to the available
# threads, taking the holdoff period and connection caching into account.
sub dispatcher
{
  my ($doneind, $result);
  my @deferredjobs;

  # discard stuff we don't need:
  map { delete $_->{urlarg}; } @jobs; # created only for -echo
  createslaves();
  # initialise cumulative values:
  @globals{@cumulatives}= (0) x @cumulatives;
  while( @jobs || @deferredjobs || $globals{stdin} ||
                        shortgrep { defined($_->{job}) } @slavedata )
  {
    while( shortgrep(sub { !defined($_->{job}) }, @slavedata)  &&
                                        (my $newjob= getnewjob()) ) {
      if( shortgrep(sub { defined($_) && $_->{url} eq $newjob->{url} },
                map $_->{job}, @slavedata) ) {
        push @deferredjobs, $newjob;
        # avoid concurrent jobs with equal URLs to prevent duplicate downloads
        serverdone($newjob);
        next;
      }
      my $idler= pickslave($newjob, \@slavedata);
      $slavedata[$idler]{job}= $newjob;
      senddata($slavedata[$idler]{socket}, $newjob);
    }
    $doneind= slaveready();
    if( $globals{stdin} ) {
      addnewjobs( getstdinurls() );
    }
    next unless defined($doneind);
    my @newjobs;
    do {
      push @newjobs, recvdata($slavedata[$doneind]{socket});
    }
    while( !$newjobs[-1]{"end of list"} );
    pop @newjobs;
    my $donejob= shift @newjobs;
    $slavedata[$doneind]{job}= undef;
    unshift @newjobs, finishjob($donejob);
    push @newjobs, @deferredjobs;
    @deferredjobs= checkinprogress(\@newjobs);
    addnewjobs(@newjobs);
  }
  killslaves();
}


# Compare new jobs to those currently being processed.  The purpose of this is
# to defer jobs with the same URL as one in progress until the processing has
# terminated, to avoid duplicate downloads.
# -> Reference to array of new jobs (references to job hashes)
# <- List of deferred jobs.  These are removed from the array of new jobs.
sub checkinprogress
{
  my ($news)= @_;

  my @inprogress=  grep defined($_), map $_->{job}, @slavedata;
  my @deferred;
newjob:
  for my $new (@$news) {
    for my $inp (@inprogress) {
      if( $inp->{url} eq $new->{url} ) {
        push @deferred, $new;
        $new= undef;
        next newjob;
      }
    }
  }
  @$news= grep defined($_), @$news if @deferred;
  return @deferred;
}

}



# Perform search without forking.  The URLs in the job queue are processed
# sequentially.
sub crawler
{
  initlocals(-1);
  while( 13 )
  {
    my $currjob;

    if( $globals{stdin} ) {
      addnewjobs( getstdinurls() );
    }
    while( !($currjob= getnewjob()) ) {
      last unless @jobs || $globals{stdin};
      select(undef, undef, undef, $globals{dispatchwait});
      if( $globals{stdin} ) {
        addnewjobs( getstdinurls() );
      }
    }
    last unless $currjob;
    print STDERR "Crawler: $currjob->{url}\n" if $globals{debug};
    my @newjobs= getandtest($currjob);
    unshift @newjobs, finishjob($currjob);
    addnewjobs(@newjobs);
  }
  @globals{@cumulatives}= getcumulatives();
}


{

my $cachefname= "aaaaaaaaaa";

sub cachefilename
{
  return File::Spec->catfile($globals{tmpdir}, $cachefname++);
}

}


# Perform the actions of a job that has been retrieved and processed, except
# for the "filter", "done", "follow" and "recurse" actions that have been
# completed in getandtest() in the slave process.
# -> Reference to URL job hash
# <- List of jobs arising from "feed" actions
sub finishjob
{
  my ($donejob)= @_;
  my @njobs;

  serverdone($donejob);
  if( $donejob->{robotstxt} ) {
    parserobotstxt($donejob);
    return ();
  }
  push @attic, $donejob;
  while( my ($ind, $act) = each @{$pipesecs[$donejob->{section}]{actions}} ) {
    next unless $donejob->{actvals}[$ind];
    if( defined($act->{options}{maxresults}) ) {
      next unless $act->{options}{maxresults} > 0;
      --$act->{options}{maxresults};
# TODO: prevent unused results being generated in the future
# is $minsection still helpful? no
    }
    if( $act->{type} eq "output" ) {
      printresult($donejob, $act->{options});
    }
    elsif( $act->{type} eq "download" ) {
      downloadresult($donejob, $act->{options});
    }
    elsif( $act->{type} eq "feed" ) {
      my $newjob= $act->{options}{transform} ?
                  transformjob($donejob, $act->{options}{transform}) :
      # Copy, but no deep copy.  It should be OK to share sub-hashes such as
      # HTTP headers, which are read only.
                 { %$donejob };
      next unless $newjob;
      $newjob->{section}= $act->{target};
      $newjob->{dtl}= $pipesecs[$newjob->{section}]{options}{maxdepth};
      $newjob->{starturl}= 1;
      push @njobs, $newjob;
    }
  }
  return @njobs;
}


# Keys defining all a job's data after downloading and decoding:
my @alljobdata= qw(cachefile realurl headers stat download parsetype charset);

# Add new URL jobs to the job list unless they are already there.  May only be
# called from the main (dispatcher) thread.
# -> Hash references to job data.  This list is assumed not to contain
#    duplicates.
sub addnewjobs
{
  my @news= @_;

  return unless @news;
newjob:
  for my $new (@news) {
    my $insertbefore;
    while( my ($ind, $old) = each @jobs ) {
      next if $old->{section} > $new->{section};
      if( ($old->{url} eq $new->{url} ||
            ($old->{realurl} || "") eq $new->{url}) &&
            $old->{section} eq $new->{section} ) {
        if( eqpostdata($old, $new) ) {
          $old->{dtl}= max2($old->{dtl}, $new->{dtl});
          $old->{gdtl}= max2($old->{gdtl}, $new->{gdtl});
          $old->{weight} += $new->{weight};
          next newjob;
        }
      }
      if( !defined($insertbefore) && ($old->{section} < $new->{section} ||
          ($old->{section} == $new->{section} && $old->{dtl} > $new->{dtl})) ) {
        $insertbefore= $ind;
      }
      last if $old->{section} < $new->{section};
    }
    my $oldcopy;
    for my $retired (@attic) {
      next unless ($retired->{url} eq $new->{url} ||
        ($retired->{realurl} || "") eq $new->{url}) && eqpostdata($retired, $new);
      next newjob if $retired->{section} == $new->{section} &&
          $retired->{dtl} >= $new->{dtl} && $retired->{gdtl} >= $new->{gdtl};
      $oldcopy= $retired;
    }
    @$new{@alljobdata}= @$oldcopy{@alljobdata} if $oldcopy;
    $new->{cachefile} ||= cachefilename();
    $insertbefore //= @jobs;
    splice @jobs, $insertbefore, 0, $new;
  }
}


# Compare the POST data of two URL jobs for equality.  Two non-POST jobs
# compare as equal.
# -> References to two URL job hashes
# <- 1 if POST data are equal (or both non-existent), 0 otherwise
sub eqpostdata
{
  my ($job1, $job2)= @_;

  if( $job1->{post} && $job2->{post} ) {
    return 0 unless 0+keys(%{$job1->{post}}) == 0+keys(%{$job2->{post}});
    for my $key (keys %{$job2->{post}}) {
      return 0 unless defined($job1->{post}{$key}) &&
                 $job1->{post}{$key} eq $job2->{post}{$key};
    }
    return 1;
  }
  else {
    return !$job1->{post} && !$job2->{post};
  }
}


# Get a new job from the queue @job after asking servercheck() whether this is
# OK (robots allowed, holdoff period complete).  The jobs which violate robot
# exclusion are removed from the queue if encountered before the legitimate new
# job; jobs which have to wait for the holdoff period are moved to the end of
# the queue.
# <- Reference to new URL job data hash, or undef if no queued job can be
#    legitimately started.
sub getnewjob
{
  my $ind= 0;

  while( $ind <= $#jobs )
  {
    my $queued= $jobs[$ind];
    my $check= servercheck($queued);
    if( ref($check) eq "HASH" ) {       # newly created robots.txt job
      return $check;
    }
    elsif( $check eq "ok" ) {
      splice @jobs, $ind, 1;
      return $queued;
    }
    elsif( $check eq "norobots" ) {
      ++$globals{norobots};
      $globals{norobwgt} += $queued->{weight};
      splice @jobs, $ind, 1;
      next;
    }
    ++$ind;
  }
  return undef;
}


{

# Hash storing robot rules and holdoff time for each server
my %servers;


# Parse a server's robots.txt and generate the corresponding allow/disallow
# lists in the server data hash.
# -> Reference to URL job hash
sub parserobotstxt
{
  my ($job)= @_;
  my $server= $job->{robotstxt};
  my $data;
  local $/= undef;

  if( $job->{download} eq "done" && open(ROBTXT, $job->{cachefile}) &&
      defined($data= <ROBTXT>) &&
      $data !~ /<!doctype\s+html\b|<html\b|<head\b|<title\b/is ) {
    close ROBTXT;
    my $endl= qr/\r\n|\n|\r/;
    $data =~ s/\#.*?($endl)/$1/so;
    my @sections= split /$endl\s*$endl/so, $data;
    my $oursec;
robotsection:
    for my $sec (@sections) {
      while( $sec =~ s/User-agent:[\t ]*(\S+)[\t ]*$endl// ) {
        my $thisua= $1;
        if( $thisua eq "*" ) {
          $oursec= $sec;
          next;
        }
        $thisua =~ quotemeta($thisua);
        # robotstxt.org recommends a "case-insensitive substring match" to
        # determine whether a section is meant for you.  We additionally
        # require that the substring (User-agent name) ends at word boundaries,
        # so that "fin" will not be confused with "wfind", for example.
        $thisua= "\\b$thisua" unless $thisua =~ /^\W/;
        $thisua= "$thisua\\b" unless $thisua =~ /\W$/;
        if( $globalopts{useragent} =~ /$thisua/i ) {
          $oursec= $sec;
          last robotsection;
        }
      }
    }
    if( $oursec ) {
      my $done;
      for my $line (split /$endl/, $oursec) {
        if( $line =~ /^\s*Disallow:\s*$/ ||
            $line =~ /^\s*Allow:\s*(?:\*|\/)\s*$/ ) {   # Blank check
          undef $servers{$server}{allowed};
          undef $servers{$server}{disallowed};
          $done= 1;
        }
        elsif( $line =~ /^\s*Disallow:\s*(?:\*|\/)\s*$/ ) {
          $servers{$server}{disallowed}= ".*";
        }
        elsif( $line =~ /^\s*Disallow:\s*(\S+)/ && !$done ) {
          $servers{$server}{disallowed} .= "|" . quotemeta($1);
        }
        elsif( $line =~ /^\s*Allow:\s*(\S+)/ && !$done ) {
          $servers{$server}{allowed} .= "|" . quotemeta($1);
        }
        elsif( $line =~ /^\sCrawl-delay:\s*(\d+)/ ) {
          $servers{$server}{crawldelay}= $1;
        }
      }
      unless( $done ) {
        # We assume the paths from robots.txt to end at word boundaries, so
        # /dir does not (dis)allow /directx.html.  robotstxt.org says /help
        # should refer to /help.html as well as /help/... but gives no example
        # with a different trunk name.
        if( $servers{$server}{allowed} ) {
          $servers{$server}{allowed} =~ s/^\|//;
          $servers{$server}{allowed}= "^(?:" .
                          $servers{$server}{allowed} .  ")\\b";
        }
        if( $servers{$server}{disallowed} ) {
          $servers{$server}{disallowed} =~ s/^\|//;
          $servers{$server}{disallowed}= "^(?:" .
                         $servers{$server}{disallowed} . ")\\b";
        }
      }
    }
  }
  # Override default holdoff if allowed
  if( defined($servers{$server}{crawldelay}) &&
        $globalopts{holdoff} =~ /^-/ ) {
    $servers{$server}{holdoffhttp}= $servers{$server}{crawldelay};
  }
}


# Find out whether a URL is allowed for wfind.  The URL is checked against the
# server's allow and disallow lists from robots.txt if applicable.
# -> Server name
#    URL
# <- 1 if allowed, otherwise 0
sub robotsallowed
{
  my ($server, $url)= @_;
  my $allow= $servers{$server}{allowed};
  my $dis= $servers{$server}{disallowed};

  $url =~ s/^\w+:\/\/[^\/#?]+//;
  # We follow the example of Googlebot here in letting all allowed paths
  # override all disallowed paths.  This is not quite standard but makes sense
  # (why have an allow directive if it does not override anything?).
  return 1 if $allow && $url =~ /$allow/i;
  return 0 if $dis && $url =~ /$dis/i;
  return 1;
}


# Decide whether downloading and testing a document may go ahead now.
# Obeisance of the server's robots.txt and the holdoff period are enforced.
# If the download is legitimate, the server is marked as "in use", and further
# downloads from the same server are disallowed until serverdone() is called.
# This function may only be called from the dispatcher thread.
# -> Reference to hash of job/URL
# <- "ok" if ok to download or already downloaded, "norobots" if forbidden by
#    robots.txt, "wait" if holdoff period has not completed.  If robots.txt has
#    yet to be downloaded, a reference to an URL job hash for that purpose is
#    returned.
sub servercheck
{
  my ($job)= @_;
  my $sec= $pipesecs[$job->{section}];

  if( $job->{type} =~ /^(?:file|dir)$/ || $job->{forwarded} ||
        ($job->{userinput} && !$globalopts{userholdoff}) ||
        ($job->{download} || "") =~ /^(?:done|failed)$/ || 
        ( !@{$sec->{htests}} && !@{$sec->{ctests}} &&
          ((!$sec->{haveaction}{follow} && !$sec->{haveaction}{recurse}) ||
            !$job->{dtl} || !$job->{gdtl} ) &&
          !$sec->{haveaction}{download} ) ) {
    $job->{noservercheck}= 1;
    return "ok";
  }
#  print STDERR "servercheck ", $job->{url}, "\n";
  my ($scheme, $server) = URI::Split::uri_split($job->{url});
  print("undef server for ", ($job->{url} || "(undef URL)"), "\n")
        if !defined($server);
  $server =~ s/:\d+$//;
  $scheme =~ s/s$//i;
  $scheme= lc($scheme);
  if( !defined($servers{$server}{next}) ) {
    initholdoff($server);
    # Create job to download robots.txt:
    return { type => "url", url => "http://$server/robots.txt",
                robotstxt => $server, cachefile => cachefilename(),
                weight => 0 };
  }
  if( $scheme eq "http" && !robotsallowed($server, $job->{url}) ) {
    print STDERR $job->{url}, " disallowed by robots.txt\n"
        if $globalopts{verbose};
    ++$servers{$server}{norobots};
    return "norobots";
  }
  return "wait" if serverisbusy($server, $scheme);
  serversetbusy($server, $scheme, 1);
  return "ok";
}


# Initialise server holdoff periods in line with command-line option -holdoff.
# The holdoff may be accounted for separately or in common for the two
# protocols http(s) and ftp(s).  Common accounting means that a transfer with
# one protocol can delay an imminent transfer with the other.  The sign of the
# value of the -holdoff option has the following meaning: no sign: separate
# accounting, +: common accounting, -: separate accounting with http holdoff
# overridden by Crawl-Delay in robots.txt if present.  The default is -10:
# separate accounting with 10 seconds delay with robots.txt override allowed.
# -> Server name
# (<-) appropriate entries in $servers{..} hash
sub initholdoff
{
  my ($server)= @_;

  $servers{$server}{next}= -1;      # serves as initialisation flag
  my $ho= abs($globalopts{holdoff});
  if( $globalopts{holdoff} =~ /^\+/ ) {
    $servers{$server}{holdoff}= $ho;
  }
  else {
    @{$servers{$server}}{qw(holdoffhttp holdoffftp)}= ($ho) x 2;
    @{$servers{$server}}{qw(nexthttp nextftp)}= (-1) x 2;
  }
}


# Mark server as busy or remove busy mark.  If the global holdoff value starts
# with a "+" sign, different hash entries are used for different schemes,
# allowing http and ftp accesses to proceed independently.  If the holdoff
# value is zero, the server is never actually marked as busy, which allows
# simultaneous transfers.
# -> Server name
#    URL scheme (http or ftp)
#    1 -> mark as busy, 0 -> remove mark, -1 -> remove mark with no holdoff
sub serversetbusy
{
  my ($server, $scheme, $set)= @_;

  $scheme= "" if $globalopts{holdoff} =~ /^\+/;
  if( $set > 0 ) {
    $servers{$server}{"next$scheme"}= -1
        unless $servers{$server}{"holdoff$scheme"} == 0;
  }
  elsif( $set == 0 ) {
    $servers{$server}{"next$scheme"}=
        time() + $servers{$server}{"holdoff$scheme"};
  }
  else {
    $servers{$server}{"next$scheme"}= 0;
  }
}


# Returns whether a server is busy.  If the global holdoff value starts with a
# "+" sign, different hash entries are used for different schemes, allowing
# http and ftp accesses to proceed independently.  If the holdoff value is
# zero, the server is never reported to be busy, which allows simultaneous
# transfers.
# -> Server name
#    URL scheme (http of ftp)
# <- Non-zero if server has been left alone for the prescribed holdoff period,
#    zero otherwise.
sub serverisbusy
{
  my ($server, $scheme)= @_;

  $scheme= "" if $globalopts{holdoff} =~ /^\+/;
  return $servers{$server}{"holdoff$scheme"} != 0 &&
         ($servers{$server}{"next$scheme"} < 0 ||
          time() < $servers{$server}{"next$scheme"} );
}


# Resets the holdoff period timer of the server corresponding to a URL which
# has been downloaded.  May only be called from the dispatcher thread.
# -> Reference to hash of job/URL
sub serverdone
{
  my ($job)= @_;

  if( $job->{robotstxt} ) {
    serversetbusy($job->{robotstxt}, "http", -1);
    return;
  }
  if( $job->{noservercheck} ) {
    delete $job->{noservercheck};    # not necessarily true in later sections
    delete $job->{forwarded};
    return;
  }
  my ($scheme, $server) = URI::Split::uri_split($job->{url});
  $server =~ s/:\d+$//;
  $scheme =~ s/s$//i;
  $scheme= lc($scheme);

  if( ($job->{download} || "") =~ /^(?:done|failed)$/ ) {
    serversetbusy($server, $scheme, 0);
  }
  else {
    # no holdoff if no download was performed or after HEAD request
    serversetbusy($server, $scheme, -1);
  }
#  print STDERR "serverdone: $server: ", Dumper($servers{$server}), "\n";
}


# Print which servers refused access to wfind and for how many URLs.
sub printnorobotservers
{
  my @norobserv= grep $servers{$_}{norobots}, keys %servers;
  my @norobstr= map $_ . " (" . $servers{$_}{norobots} . ")", @norobserv;
  fmt "Access was denied to wfind by (# of URLs): ",
        cswordlist(" and ", @norobstr), ".\n";
}


# Pick a suitable slave process to handle a job.  Among the available slaves,
# the one which has most recently executed a request to the same server is
# chosen, because it is most likely to have an open connection to it.
# -> Reference to URL job hash
#    Reference to array of slave data hashes (read only)
# <- Index of chosen slave, or undef if none was available
sub pickslave
{
  my ($job, $slavedata)= @_;
  my ($server, $serverslaves);

  goto pickrandom if $job->{type} ne "url";
  (undef, $server) = URI::Split::uri_split($job->{url});
  goto pickrandom unless defined $server;   # may be malformed, for output
  $server =~ s/:\d+$//;
  $serverslaves= ($servers{$server}{slaves} ||= []);
  my $ind= 0;
  for my $s (@$serverslaves) {
    if( !$$slavedata[$s]{job} ) {
# TODO: could try to avoid slaves with cached connections for jobs which don't require downloads
      splice(@$serverslaves, $ind, 1);
      unshift @$serverslaves, $s;
      return $s;
    }
    ++$ind;
  }
pickrandom:
  my @idleinds= grep !defined($slavedata->[$_]{job}),
                                                (0 .. $globalopts{slaves}-1);
  my $s= $idleinds[0];
  unshift @$serverslaves, $s if defined($s) && defined($serverslaves);
  return $s;
}

}


my @transformjobcopy= qw(section dtl gdtl weight);

# Transform a job's URL with a Perl expression.  This takes care of the
# -transform command-line option.
# -> Reference to URL job hash
#    Perl expression transforming $_
# <- Reference to a URL job with the transformed URL but otherwise the same
#    properties (pipe section etc.) as the argument job; undef if an error
#    occurred during the eval or if the result of the transformation was the
#    empty string or undef.
sub transformjob
{
  my ($job, $expression)= @_;

  return $job unless $expression;
  my $newurl= transformstring( $job->{shortened} ? $job->{realurl} : $job->{url}, $expression, $job );
  return undef if !defined($newurl) || $newurl eq "";
  my %newjob= ( url => $newurl, cachefile => cachefilename() );
# TODO: creation of job from URL is similar as in parseurlarg(); refactor?
  if( $newjob{url} =~ /^file:\/\//i ) {
    $newjob{type}= "file";
    $newjob{path}= $newjob{url};
    $newjob{path} =~ s/^file:\/\///i;
  }
  else {
    $newjob{type}= "url";
  }
  @newjob{@transformjobcopy}= @{$job}{@transformjobcopy};
  if( $globals{needtrace} ) {
    $newjob{trace}= [ $job->{url}, @{$job->{trace} // []} ];
    $newjob{linktrace}= [ { index => -1, indices => [ ],
                  linktext => $expression, linktexts => [ $expression ] },
                  @{$job->{linktrace} // []} ];
  }
  return \%newjob;
}


{
our @trace;
our @linkprops;
our $contentname;

# Transform a string with a Perl expression.
# -> String to transform
#    Perl expression operating on $_
#    Reference to hash with supplementary data (usually URL job hash), from
#    which the "trace" and "linktrace" elements are copied and made available
#    to the transformation expression as @trace and @linkprops.  If the job's
#    headers contain a Content-Disposition header with a file name, it is made
#    available in the $contentname scalar.
#    Reference to scalar for eval error message
# <- Result of the transformation, or undef if an eval error occurred
sub transformstring
{
  my ($str, $expression, $suppl, $evalerr)= @_;
  my $jail= new Safe;

  return $str unless $expression;
  $jail->deny_only(
      qw(:base_thread :sys_db :filesys_write :ownprocess :others :dangerous));
  $jail->permit("time");
  $jail->share_from('URI::Escape', [ 'uri_unescape', 'uri_escape', 'uri_escape_utf8' ]);
  $jail->share_from('URI::Split', [ 'uri_split', 'uri_join' ]);
  if( $suppl ) {
    @trace= @{$suppl->{trace}} if ref($suppl->{trace}) eq "ARRAY";
    @linkprops= @{$suppl->{linktrace}} if ref($suppl->{linktrace}) eq "ARRAY";
    $contentname= $1 if $suppl->{headers} && ref($suppl->{headers}{"Content-Disposition"}) eq "ARRAY" &&
       ($suppl->{headers}{"Content-Disposition"}[0] // "") =~ /\bfilename=([^;]+)/i;
  }
  $jail->share('@trace', '@linkprops', '$contentname');
  local $_= $str;
  @_= ();
  $jail->reval($expression);
  if( $@ ) {
    $$evalerr= $@ if $evalerr;
    return undef;
  }
  return $_;
}

}


END {
  killslaves() if $globals{fork} && !$globals{isslave};
}


#sectionout###################################################################
##############################################################################
####                    Output subroutines
##############################################################################
##############################################################################

{

# Print out requested information (default: file name/URL only) of a result.
# This is the "output" action.
# -> Reference to job hash
#    Reference to option hash relevant for this output
sub printresult
{
  my ($job, $opts)= @_;
  my $url;
  my @columns;

  $url= $job->{shortened} ? $job->{realurl} : $job->{url};
  if( $opts->{print}{url} ) {
    push @columns, $url;
  }
  if( $opts->{print}{furl} || ($opts->{print}{trace} && ! $opts->{print}{url}) ) {
    push @columns, ($job->{type} eq "url"? $url : 
                    $job->{path} || uri2file($url) || $url);
  }
  if( $opts->{print}{type} ) {
    my $type;
    if( $job->{decfile} ) {
      $type= getmimetype($job->{decfile});
    }
    unless( $type ) {
      if( $job->{headers}{"Content-Type"} ) {
        $type= $job->{headers}{"Content-Type"}[0];
        $type =~ s/;.*$//;
      }
      $type ||= "?";
    }
    push @columns, $type;
  }
  if( $opts->{print}{size} ) {
    my $size;
    if( $job->{stat} && $job->{stat}{size} ) {
      $size= $job->{stat}{size};
    }
    elsif( $job->{origfile} && -f $job->{origfile} ) {
      $size= ( stat($job->{origfile}) )[7];
    }
    elsif( $job->{headers} && $job->{headers}{"Content-Length"} ) {
      $size= $job->{headers}{"Content-Length"}[0];
    }
    else {
      $size= "?";
    }
    push @columns, $size;
  }
  if( $opts->{print}{time} ) {
    my $time;
    if( $job->{stat} && $job->{stat}{time} ) {
      $time= localtime($job->{stat}{time});
    }
    elsif( $job->{headers} && $job->{headers}{"Last-Modified"} ) {
      $time= $job->{headers}{"Last-Modified"}[0];
    }
    else {
      $time= "?";
    }
    push @columns, $time;
  }
  print join("\t", @columns), "\n";
  if( $opts->{print}{trace} ) {
    my $props= 0;
    if( $opts->{print}{linkprops} ) {
      $props= $job->{linktrace} && @{$job->{linktrace}} ? 1 : -1;
    }
    if( $job->{trace} && @{$job->{trace}} ) {
      for my $ind (0..$#{$job->{trace}}) {
        print "  ", $job->{trace}[$ind], "\n";
        if( $props > 0 ) {
          my $p= $job->{linktrace}[$ind];
          print "    ", join(", ", @{$p->{indices}}), "\n" if @{$p->{indices}};
          print "    ", join("\n    ", @{$p->{linktexts}}), "\n";
        }
      }
      print "    (no link properties available)\n"
        if $props < 0;
    }
    else {
      print "  (no trace available)\n";
    }
  }
  elsif( $opts->{print}{linkprops} ) {
    if( $job->{linktrace} && $job->{linktrace} ) {
      my $p= $job->{linktrace}[0];
      print "  ", join(", ", @{$p->{indices}}), "\n" if @{$p->{indices}};
      print "  ", join("\n  ", @{$p->{linktexts}}), "\n";
    }
    else {
      print "  (no link properties available)\n";
    }
  }
}


# Download the URL of a job to the current directory, i.e. copy it from the
# cache directory.  This is the "download" action.
# -> Reference to job hash
#    Reference to option hash relevant for this download
sub downloadresult
{
  my ($job, $opts)= @_;

  if( $job->{type} eq "file" || $job->{download} eq "done" ) {
    my $source= $opts->{decompress}?
                      $job->{decfile} : $job->{origfile};
    my $dest= localname($job, $opts);
    if( defined($dest) ) {
      if( ! File::Copy::copy($source, $dest) ) {
        print STDERR "Error storing $job->{url} as $dest.  Continuing.\n";
      }
      elsif( $job->{headers} && $job->{headers}{"Last-Modified"} &&
                                                  $globals{dateparse} ) {
          my $time= Date::Parse::str2time($job->{headers}{"Last-Modified"}[0]);
          utime $time, $time, $dest if defined $time;
      }
    }
    else {
      print STDERR "Error trying to store $job->{url}: all similar file names taken.  Continuing.\n";
    }
  }
  elsif( $job->{download} eq "failed" ) {
    print STDERR "Error: Could not retrieve $job->{url}.\n";
  }
  else {
    print STDERR "Bug: Did not try to retrieve $job->{url} despite -download.\n";
  }
}


# Find a non-existing local filename matching a given URL.  Intended as a new
# file for storing downloaded files.  The file name is immediately created and
# closed again, but there still is a race conditions if two instances of wfind
# run in the same directory and simultaneously try to store files with the same
# names.
# -> Reference to URL job hash
#    Reference to options hash of download action (for -downxform and
#    -decompress)
# <- Non-existing file name similar to file name from URL, or undef if none
#    could be found.
sub localname
{
  my ($job, $opts)= @_;
  my $url= $job->{shortened} ? $job->{realurl} : $job->{url};
  my ($server, $name, $query);
  my $ext= "";

  if( $opts->{downxform} ) {
    $name= transformstring($url, $opts->{downxform}, $job);
    $ext= $1 if defined($name) && $name =~ s/(\.[^\.]*)$//;
  }
  if( !defined($name) || $name eq "" ) {
    if( $job->{headers} && ref($job->{headers}{"Content-Disposition"}) eq "ARRAY" &&
         ($job->{headers}{"Content-Disposition"}[0] // "") =~ /\bfilename=([^;]+)/i ) {
      $name= $1;
      $ext= $1 if $name =~ s/(\.[^\.]*)$//;
    }
    else {
      (undef, $server, $name, $query)= URI::Split::uri_split($url);
      $name =~ s/\/$//;
      $name =~ s/^.*\///;
      if( $name ) {
        $name =~ s/(\.(?:gz|bz2|z|uu))$//i;
        my $comext= $1 || "";
        $name =~ s/(\.[^\.]*)$//;
        $ext= $1 // "";
        $ext .= $comext unless $opts->{decompress};
      }
      else {
        $name= $server || "wfind_out";
      }
      if( $query ) {
        $query =~ tr|&/|_+|;
        $name .= "_$query";
      }
    }
  }
  unless( -e "$name$ext" ) {
    open TOUCH, ">$name$ext" && close TOUCH;
    return "$name$ext";
  }
  for my $ind (1..999) {
    my $out= $name."_$ind$ext";
    next if -e $out;
    open TOUCH, ">$out" && close TOUCH;
    return $out;
  }
  return undef;
}

}


# Print how many documents could not be retrieved.
sub printstats
{
  return if $globalopts{silent};
  if( $globals{nonex} ) {
    print STDERR $globals{nonex}, " documents (weight ",
        sprintf("%.2f", $globals{nonexwgt}/(1 - $globals{nowhilewgt})),
        ") could not be retrieved/decoded.\n";
    if( (!$globals{lwphttps} || (!$globals{lwpconnect} && $ENV{https_proxy})) && $globals{nohttps} ) {
      print STDERR "($globals{nohttps} of these were HTTPS URLs - install ",
          ($globals{lwphttps} ? "":"LWP::Protocol::https and possibly "),
          "LWP::Protocol::connect (for HTTPS via proxy).)\n";
    }
  }
  if( $globals{norobots} ) {
    print STDERR $globals{norobots}, " documents (weight ",
        sprintf("%.2f", $globals{norobwgt}/(1 - $globals{nowhilewgt})),
        ") were forbidden to robots.\n";
    printnorobotservers() if $globalopts{verbose};
  }
  if( $globals{blacklisted} ) {
    print STDERR $globals{blacklisted}, " documents (weight ",
        sprintf("%.2f", $globals{blacklistwgt}/(1 - $globals{nowhilewgt})),
        ") were blacklisted.\n";
  }
}


# Print wfind version and exit.
sub printversion
{
  print "wfind version  $wfversion  from  $wfverdate\n";
  exit;
}


# Print out capabilities and exit.
sub printstatus
{
  if( $globalopts{verbose} ) {
    printcapsverbose();
  }
  else {
    printcapsbrief();
  }
  exit;
}


# Print help message.
sub printhelp
{
  print $0 =~ /\bwfind$/i ? <<EOF1 : <<EOF2;
Usage: wfind <URLs, files, directories> [ "--" ] <keywords, tests, options>
Recursively follows hyperlinks and retrieves documents and outputs URLs of
documents matching specific criteria, in the simplest case containing given
words.  Frequently used options:
-status   Print supported file types and tests (no URL necessary)
-depth    Hyperlink recursion depth
-or       Require only one of several keywords / tests to match
-url <test>   Apply test / keyword match to URL instead of content
-wordnear <n> Proximity requirement in words
-while <test> Hyperlink recursion condition
See the manual page of wfind (1) for more information,  or go to:
http://www.volkerschatz.com/net/wfind.html
EOF1
Usage: scav [ <global options> ] <script> [ <URLs, files, directories> ] [ "--" ] [ <general arguments> ]
A script-driven web spider supporting advanced matching criteria.
Global options:
-status   Print supported file types and tests (no script necessary)
-echo     Print internal representation of script
Script commands:
recurse <condition>   Hyperlink recursion condition
follow                Follow hyperlinks without recursing
filter <condition>    Discard current URLs not matching condition
output                Print URLs
download              Save document
Conditions may simply be keywords or test options.  Scav is the scripting
interface of the wfind spider.  It is simply too complex to explain briefly,
so read the manual page of wfind (1) or scav (1),  or go to:
http://www.volkerschatz.com/net/wfind.html
EOF2
  exit;
}


# Echo interpreted command line arguments (URLs + options) and default options
# back to the user.
sub printcmdline
{
  printscavcmdline() if $globals{scav} || $globalopts{scavecho};

  print "wfind command line and default options summary\n";
  print "----------------------------------------------\n";
  print "Start URLs:\n";
  print "  (none)\n" unless @{$pipesecs[0]{starturls} || []};
  printurlargs($pipesecs[0]{starturls}, \@jobs, "  ");
  print "  Accepting further start URLs from STDIN\n" if $globals{stdin};
  print "Global options:\n";
  printopthash(\%globalopts, "  ", 22);
  while( my ($secnum, $sec)= each @pipesecs ) {
    my ($testexpr, $whileexpr, $linktext);
    for (@{$sec->{actions}}) {
      if( $_->{type} eq "filter" ) {
        $testexpr= $_->{testexpr} 
      }
      elsif( $_->{type} eq "recurse" ) {
        $whileexpr= $_->{testexpr} if $_->{testexpr}{type} ne "true";
        $linktext= $_->{linktext};
      }
    }
    print "Pipe section $secnum:\n";
    print "  Search expression:\n";
    printexprtree($testexpr, "    ");
    if( $whileexpr || $globalopts{verbose} ) {
      print "  While expression:\n";
      printexprtree($whileexpr, "    ");
    }
    if( $linktext || $globalopts{verbose} ) {
      print "  Linktext expression:\n";
      printexprtree($linktext, "    ");
    }
    print "  Pipe section options:\n";
    printopthash($globalopts{verbose} ? $sec->{options} : grepoptclass("PSA", $sec->{options}), "    ", 22);
  }
  exit;
}


# Echo data structures representing parsed script and command line back to the
# user.
sub printscavcmdline
{
  my $filtopts;

  print "scavenger script summary\n";
  print "------------------------\n\n";
  print "Global options:\n";
  printopthash(\%globalopts, "  ", 22);
  while( my ($secnum, $sec)= each @pipesecs ) {
    print "\nSection $secnum", ($sec->{name}? ", \"$sec->{name}\"" : ""), ":\n";
    my $stdinurls= shortgrep { $_ == $secnum; } @{$globals{stdinsecs}};
    if( $stdinurls || $sec->{starturls} && @{$sec->{starturls}} ) {
      my $further= "";
      print "  Start URLs:\n";
      if( $sec->{starturls} && shortgrep { ref; } @{$sec->{starturls}} ) {
        my @secjobs= grep $_->{section} == $secnum, @jobs;
        printurlargs($sec->{starturls}, \@secjobs, "    ");
        $further= " further";
      }
      print "    Accepting$further start URLs from STDIN\n"
            if $stdinurls;
    }
    $filtopts= $globalopts{verbose} ? $sec->{options} : grepoptclass("S", $sec->{options});
    if( keys %$filtopts ) {
      print "  Section options:\n";
      printopthash($filtopts, "    ", 22);
    }
    print @{$sec->{actions}} ? "  Actions:\n" : "  No actions\n";
    for my $act (@{$sec->{actions}}) {
      print "    ", ucfirst($act->{type}), " action",
           ($act->{type} =~ /^(?:follow|recurse|feed)$/?
                    ", target section $act->{target}" : ""), "\n";
      $filtopts= $globalopts{verbose} ? $act->{options} : grepoptclass("A", $act->{options});
      if( keys %$filtopts ) {
        print "      Action options:\n";
        printopthash($filtopts, "        ", 22);
      }
      print "      Test expression:\n";
      printexprtree($act->{testexpr}, "        ");
      if( $act->{linktext} ) {
        print "      Linktext test expression:\n";
        printexprtree($act->{linktext}, "        ");
      }
    }
  }
  exit;
}


# Print out the list of URL argument and URLs originating from them.  Depending
# on -verbose, all or only 3 URLs at most are printed.  The URLs are sorted in
# alphabetical order.
# -> Reference to array of references to URL arguments, possibly containing
#    ranges and braced lists
#    Reference to array of URL jobs
#    Indentation to prepend to each line
sub printurlargs
{
  my ($ualist, $joblist, $ind)= @_;

  for my $ua (@$ualist) {
    next unless ref $ua;
    my @urls= sort { $a->{url} cmp $b->{url} }
                grep $_->{urlarg} == $ua, @$joblist;
    @urls= map $_->{url}, @urls;
    if( $globalopts{verbose} ) {
      print "$ind$$ua :\n$ind  ", (@urls>0? join("\n$ind  ", @urls): "no matches found"), "\n";
    }
    elsif( @urls > 1 ) {
      print "$ind$$ua :\n$ind  ";
      print3 "\n$ind  ", @urls;
      print "\n";
    }
    elsif( @urls == 1 ) {
      print "$ind$urls[0]\n";
    }
    else {
      print "$ind$$ua :  no matches found\n";
    }
  }
}


my %testclassstr= ( utests => "URL", htests => "HTTP header", ctests => "document" );

# Print a string description of a tree of test expressions to stdout.
# -> Reference to test expression hash
#    Indentation (prefix) string
sub printexprtree
{
  my ($expr, $indent)= @_;

  unless( defined($expr) ) {
    print "$indent(none)\n";
    return;
  }
  if( $expr->{class} eq "op" ) {
    print $indent, ($expr->{options}{require}?
                        "REQUIRE" : uc($expr->{type})), "\n";
    if( $expr->{options} && keys(%{$expr->{options}}) ) {
      if( $globalopts{verbose} ) {
        print "$indent  Options:\n";
        printopthash($expr->{options}, "$indent    ", length($indent)+16);
      }
      elsif( defined  $expr->{options}{wordnear} || defined $expr->{options}{charnear} ) {
        print "$indent  Options:";
        print "  wordnear ", $expr->{options}{wordnear}
              if defined $expr->{options}{wordnear};
        print "  charnear ", $expr->{options}{charnear}
              if defined $expr->{options}{charnear};
        print "\n";
      }
    }
    print "$indent  Subexpressions:\n";
    for my $subexpr (@{$expr->{subexprs}}) {
      printexprtree($subexpr, "$indent    ");
    }
  }
  else {
    print $indent, ucfirst($expr->{type}), "  ";
    printmatchexpr($expr);
    print "  (", $testclassstr{$expr->{class}}, ")\n";
    if( $expr->{options} && keys(%{$expr->{options}}) ) {
      if( $globalopts{verbose} ) {
        print "$indent  Options:\n";
        printopthash($expr->{options}, "$indent    ", length($indent)+16);
      }
      elsif( $expr->{type} eq "linksto" && $expr->{options}{linktags} ) {
        my $lts= $expr->{options}{linktags};
        print "$indent  Option:  linktags    ", join(", ", 
          map { $a= $_; map "$a.$_", keys %{$lts->{$a}}; } sort keys %$lts), "\n";
      }
    }
  }
}



#sectionmain##################################################################
##############################################################################
####                    Main program
##############################################################################
##############################################################################

$globals{scav}= $0 =~ /\bscav$/;

makeuni2base();
preparseopts();
getwfinddir();
getcaps();

@pipesecs= ( dup($secdefault) );
if( $globals{scav} ) {
  parsescavcmdline( \@ARGV );
}
else {
  parsecmdline( \@ARGV );
}

if( $globalopts{help} ) { printhelp(); }
if( $globalopts{version} ) { printversion(); }
if( $globalopts{status} ) { printstatus(); }
if( $globalopts{echo} || $globalopts{scavecho} ) { printcmdline(); }

$globals{tmpdir} && mkdir $globals{tmpdir} or
  croak "Could not create cache directory.  Aborting.";

# print Dumper(@pipesecs);
# print Dumper(@jobs);

if( !$globals{fork} ) {
  crawler();
}
else {
  dispatcher();
}

printstats();

# vim: set ts=2 sw=2 et:
